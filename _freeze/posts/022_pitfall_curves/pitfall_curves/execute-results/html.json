{
  "hash": "a0901a030ecff97ce01f9692163255aa",
  "result": {
    "markdown": "---\ntitle: \"The Pitfall of Arbitrary Curve Deconvolution: A Cautionary Tale for Scientists and Students\"\nauthor: \"Giorgio Luciano & AI\"\ndate: \"2024-09-24\"\ncategories: \n  - Essential\n  - Tutorial\n---\n\n\nAs scientists and data analysts, we often encounter complex curves in our work. These could be spectroscopic data, chromatograms, or any other type of signal that appears as a single peak but might be composed of multiple underlying components. The process of breaking down such a curve into its constituent parts is known as curve deconvolution or peak fitting.\n\nIt's a common scenario: a researcher or student successfully decomposes a curve into two or three components, and they're thrilled with the result. It seems to fit well, and they're ready to draw conclusions based on these components. But there's a crucial point that's often overlooked: the number of components used in the deconvolution is often arbitrary.\n\nIn this post, we'll explore why this is problematic and demonstrate how we can fit a curve with a varying number of components using R. This exercise will highlight why we should be cautious about interpreting the results of curve deconvolution without careful consideration.\n\nLet's start with some R code that generates a simple curve and then fits it with a varying number of Gaussian components:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(minpack.lm)\nlibrary(ggplot2)\n\n# Function to generate a Gaussian curve\ngaussian <- function(x, amp, cen, wid) {\n  amp * exp(-((x - cen)^2) / (2 * wid^2))\n}\n\n# Generate sample data (a curve with a single peak)\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ny <- gaussian(x, 1, 5, 1) + rnorm(100, sd = 0.05)\ndata <- data.frame(x = x, y = y)\n\n# Function to create a model with n Gaussians\ncreate_n_gaussian_model <- function(n) {\n  components <- paste0(\"gaussian(x, amp\", 1:n, \", cen\", 1:n, \", wid\", 1:n, \")\", collapse = \" + \")\n  as.formula(paste(\"y ~\", components))\n}\n\n# Function to fit n Gaussians\nfit_n_gaussian <- function(data, n) {\n  model <- create_n_gaussian_model(n)\n  start_list <- as.list(rep(c(0.5, 5, 1), n))\n  names(start_list) <- c(rbind(paste0(\"amp\", 1:n), paste0(\"cen\", 1:n), paste0(\"wid\", 1:n)))\n  \n  fit <- nlsLM(model, data = data, start = start_list)\n  return(fit)\n}\n\n# Fit with varying number of Gaussians\nfits <- lapply(1:5, function(n) fit_n_gaussian(data, n))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n```\n:::\n\n```{.r .cell-code}\n# Calculate AIC for each fit\naics <- sapply(fits, AIC)\n\n# Create predictions and components for each fit\nfor (i in 1:length(fits)) {\n  data[paste0(\"fit\", i)] <- predict(fits[[i]], newdata = data)\n  params <- coef(fits[[i]])\n  for (j in 1:i) {\n    data[paste0(\"comp\", j, \"_\", i)] <- gaussian(data$x, params[paste0(\"amp\", j)], \n                                                params[paste0(\"cen\", j)], \n                                                params[paste0(\"wid\", j)])\n  }\n}\n\n# Plot the results\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit1, color = \"1 Gaussian\")) +\n  geom_line(aes(y = fit2, color = \"2 Gaussians\")) +\n  geom_line(aes(y = fit3, color = \"3 Gaussians\")) +\n  geom_line(aes(y = fit4, color = \"4 Gaussians\")) +\n  geom_line(aes(y = fit5, color = \"5 Gaussians\")) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\", \"orange\")) +\n  labs(title = \"Curve Deconvolution with Varying Number of Gaussians\",\n       subtitle = paste(\"AIC values:\", paste(round(aics, 2), collapse = \", \")),\n       x = \"X\", y = \"Y\", color = \"Fit\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](pitfall_curves_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot individual components for the 3-Gaussian fit\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit3, color = \"Total Fit\")) +\n  geom_line(aes(y = comp1_3, color = \"Component 1\")) +\n  geom_line(aes(y = comp2_3, color = \"Component 2\")) +\n  geom_line(aes(y = comp3_3, color = \"Component 3\")) +\n  scale_color_manual(values = c(\"black\", \"red\", \"blue\", \"green\")) +\n  labs(title = \"Deconvolution into 3 Gaussians\",\n       x = \"X\", y = \"Y\", color = \"Component\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](pitfall_curves_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n\nThis code generates a single peak and then fits it with 1 to 5 Gaussian components. It then plots the results and calculates the Akaike Information Criterion (AIC) for each fit.\n\nWhat we see from this exercise is striking:\n\n1.  All fits, from 1 to 5 components, appear to fit the data reasonably well visually.\n\n2.  The AIC values generally decrease as we add more components, suggesting that more complex models fit the data better.\n\nHowever, here's the crucial point: our original data was generated from a single Gaussian curve with added noise. Despite this, we can \"successfully\" decompose it into 2, 3, 4, or even 5 components, each of which might seem meaningful if we didn't know the true origin of the data.\n\nThis demonstrates a fundamental issue in curve deconvolution: without additional information or constraints, we can often fit a curve with an arbitrary number of components, and purely statistical measures might even suggest that more components provide a better fit.\n\nSo, what should we do? Here are some guidelines:\n\n1.  \n\n    Always consider the physical or chemical meaning behind the components. Does it make sense in your specific context to have 2, 3, or more components?\n\n2.  \n\n    Use multiple criteria for model selection, not just visual fit or a single statistical measure. AIC, BIC, cross-validation, and other techniques can provide different perspectives.\n\n3.  \n\n    Be cautious about over-interpretation. Just because you can fit a curve with multiple components doesn't necessarily mean those components represent real, distinct physical or chemical entities.\n\n4.  \n\n    When possible, use additional experimental techniques to validate your deconvolution. For example, in spectroscopy, you might use different types of spectroscopy or chemical separation techniques to confirm the presence of multiple components.\n\n5.  \n\n    Always report the uncertainty in your fits and be transparent about the assumptions made in your analysis.\n\nIn conclusion, while curve deconvolution can be a powerful tool, it's crucial to approach it with caution and skepticism. The ability to fit a curve with multiple components doesn't always reflect the underlying reality of the system you're studying. As scientists and data analysts, it's our responsibility to critically evaluate our methods and results, always keeping in mind the limitations and potential pitfalls of our analytical techniques.\n",
    "supporting": [
      "pitfall_curves_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}