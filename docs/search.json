[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Giorgio’s pages",
    "section": "",
    "text": "I got a Ph.D in materials science and I’m a researcher with a variety of interests. I currently works as a researcher at the Italian National Council of Research (CNR) on topics related to the characterization and development of new materials. My interests are also in 3D CG (lighting and rendering), and creating scientific illustration. Here you will find my code tutorials, blend files and other doodles\n\n\nhttps://giorgioluciano.github.io/listing.html\n\n\n\nhttps://github.com/giorgioluciano\n\n\n\nhttps://giorgioluciano.github.io/CrystalNodes/\n\n\n\nAll the images of my book in high resolution\nhttps://github.com/giorgioluciano/ESGT/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a Chemist that works as a researcher at the National Research Council of Italy. I work on characterization of materials and data analysis. I code in Python and R and I like Computer Graphics and Illustration\nAbout this blog\nin this blog I share the code that I write in my everyday work and also fun stuff that I find on other blogs and site about data analytics"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "docs/posts/R Packages/index.html",
    "href": "docs/posts/R Packages/index.html",
    "title": "Fun with Data",
    "section": "",
    "text": "Don’t trust boxplot\n\ncategories: [r,ggplot,recipes]\n\n\n# Author: Giorgio Luciano\n# Title: boxplot fun\n# Date: 16/2/2023\n# Status: checked working\n##---------------------------------------------------------------\n\n# Step 1: Load libraries\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(data.table)\nlibrary(RColorBrewer)\nlibrary(ggpubr)\nlibrary(rstatix, warn.conflicts = FALSE)\nlibrary(ggrepel)\nlibrary(ggpubr)\nlibrary(patchwork)\n\n# Step 2: Write a function for generating data with custom number of rows, means and sds\n\nsimpleDataset <- function(number_of_rows, means, sds)\n{\nl <- length( means )\nres <- lapply(seq(1:l),function(x) \n       eval(parse(text=paste(\"rnorm(\",number_of_rows,\",\",means[x],\",\",sds[x],\")\",sep = \"\")))) \ndat <- data.frame((sapply(res,c)))\nid <- rownames(dat)\ndat <-  cbind(id = id,dat)\ndt <- data.table(dat)\nreturn(dt)\n}\n\ndat1 <- simpleDataset(number_of_rows=100,\n                      means = runif(10,100,150),\n                      sds = runif(10,10,40))\n\noutliers <- simpleDataset(number_of_rows=5,\n                      means = runif(10,60,80),\n                      sds = runif(10,10,10))                  \n\ndato     <-rbind(dat1,outliers) \n\ndt.melt <- melt(dat1, id.vars=\"id\")\ncolnames(dt.melt) <- c(\"id\",\"category\",\"var1\")\ndt.melt$ncat <- as.numeric(dt.melt$category)\n\n#Step 3:    Jiitter plots + boxplot + brackets \n\n#setting up dimensions\noptions(repr.plot.width=8.9, repr.plot.height=8.9,units=\"cm\")\n\n#adding jiitter plot\n\np <-  ggplot(dt.melt,aes(x=factor(ncat),y=var1))        +\n      geom_jitter(position = position_jitter(0.15),alpha=0.5,size = 3) +\n      geom_boxplot(alpha = 0,lwd=0.2) \n##---------------------------------------------------------------\n\nSo for now everything on track. We created a dataset using a custom function. 10 variables with 100 points each and them we plot them using scatter plots. Before plotting a few more data we need to answer the question\n*How are boxplot constructed?* (*warning*: shameless self-promotion ahead) First of all you can check on my book/ebook [https://amzn.com/B08W8W5WSF](https://amzn.com/B08W8W5WSF)\nNow it starts the fun part we will recreate a plot on the anatomy of a boxplot (see [here](https://www.sharpsightlabs.com/blog/ggplot-boxplot/)) using ggplot.\n\n# we create a dataset and add a few outliers\n# \ny   <- c(60,63,105,155,rnorm(100,80,25))\nbox <-  ggplot()                                               +\n        theme_void()                                           +\n        geom_boxplot(aes(x=0,y=y),width=1,notch = FALSE,lwd=1) +  \n        theme(legend.position = \"none\")                        +\n        lims( x = c(-2,2) )\n      \n#how can we  get out data? using the function ggplot_build()\n#need to change it to a data.frame and rename cols\n\nbox_data <- (ggplot_build(box)$data)[[1]]\nbox_data\n\n      ymin    lower   middle    upper     ymax outliers notchupper notchlower x\n1 16.81564 58.19611 75.04227 96.32843 130.8743      155   80.95017   69.13436 0\n  flipped_aes PANEL group ymin_final ymax_final xmin xmax xid newx new_width\n1       FALSE     1    -1   16.81564        155 -0.5  0.5   1    0         1\n  weight colour  fill size alpha shape linetype\n1      1 grey20 white    1    NA    19    solid\n\nbdata <- data.frame(t(box_data[c(1,2,3,4,5,14)]))\ncolnames(bdata) <- c(\"y\")\n#we need to transpose the data and convert them to a data frame\n#now we extract the ourliers \noutl  <- data.frame(box_data$outliers)\ncolnames(outl)  <- c(\"outl\")\n\n#now that I got the data I plot everything with labels\np2 <- box + geom_text (data=bdata,aes(\n                      x=1.5,\n                      y=y,\n                      label = c(\"min\",\"Lower Q\",\"Median\",\"Upper Q\",\"max\",\"outliers\")), size = 2)+\n                      geom_segment(data = bdata, aes(x = 0.8, y = y, xend = 0.7, yend = y),lwd=1)  \n                        \n#since we have created the dataset WITH outliers we include labels also for them\n#if your dataset has no outliers you need to commet this part out\n    \np2 + geom_text_repel(data=outl,aes(x=0.1, y=outl,label=format(round(outl, 2), nsmall = 2)),size= 2) \n\n\n\n\n*notes on the code:* we create our variable `y` with `rnorm` and we add a few outliers by hand then we create the boxplot with an empty theme using `theme_void()`. The funny part start when we ask ggplot to show how the plot was built with the `ggplot_build`. We then need to rotate (`t`) the selected columns `c(1,2,3,4,5,14)` ,convert the results into a `data.frame`, rename the columns (`colnames`) and then use them (our `y`) to add labvels to our plot using `geom_text`\nAnother representation of boxplot can also include *notch*. the default is not to visualuize them but just adding `notch=true` to the previous plot we will do the trick\n\nboxnotch <-  ggplot()                                               +\n             theme_void()                                           +\n             geom_boxplot(aes(x=0,y=y),width=1,notch = TRUE,lwd=1)  +  \n             theme(legend.position = \"none\")                        +\n             lims(x=c(-2,4))\nnotchdata <- data.frame(t(box_data[c(7,8)]))\ncolnames(notchdata) <- c(\"y_notch\")\n#we need to transpose the data and convert them to a data frame\n\n#now that I got the data I plot everything with labels\np3 <- boxnotch +  geom_segment(data=notchdata, aes(x = 0.8, \n                                                   y = mean(y_notch),\n                                                   xend = 0.6, yend = y_notch\n                                                   ),lwd=1)\np4 <- p3 + annotate(geom=\"text\", x=2.5, y= mean(notchdata$y_notch),\n                    label=\"notch (95% confidence\\ninterval of median)\",size=4)\np4\n\n\n\n\nok but what if multiple dataset have same statistics? Like in the case of the datasaurus package(Matejka and Fitzmaurice 2017)\n\nlibrary(datasauRus)\n\nWarning: package 'datasauRus' was built under R version 4.2.2\n\nsummary(box_plots)\n\n      left              lines               normal          right       \n Min.   :-9.76964   Min.   :-9.769575   Min.   :-9.76   Min.   :-9.760  \n 1st Qu.:-2.68999   1st Qu.:-2.689993   1st Qu.:-2.68   1st Qu.:-2.680  \n Median :-0.00999   Median :-0.007132   Median : 0.00   Median : 0.000  \n Mean   :-1.17780   Mean   :-0.831733   Mean   : 0.00   Mean   : 1.174  \n 3rd Qu.: 2.67007   3rd Qu.: 2.670236   3rd Qu.: 2.68   3rd Qu.: 2.680  \n Max.   : 9.75025   Max.   : 9.756001   Max.   : 9.76   Max.   : 9.760  \n     split          \n Min.   :-9.769886  \n 1st Qu.:-2.689989  \n Median :-0.003099  \n Mean   :-0.003060  \n 3rd Qu.: 2.680000  \n Max.   : 9.760000  \n\np1 <-ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_jitter(alpha=0.05)                                 +\ntheme_void()  \n\np2 <- ggplot(stack(box_plots), aes(x = ind, y = values))+\ngeom_boxplot(lwd=0.05) +\ntheme_void()  \np1+p2\n\n\n\n\nWe can see that plotting the raw points even for hundreds of points works and represent well our data. In this case adding notch does not solve the problem. Other kind of plot get not fooled by our data as it can be seen in the following figure:\n\npnotch <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_boxplot(notch=TRUE)  +  ggtitle(\"(notch=TRUE)\")\n  \npjitter <-ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_jitter(alpha=0.05)  +  ggtitle(\"geom_jitter\") \n\npviolin <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_violin(lwd=1)  +  ggtitle(\"geom_violin\") \n\npnotch \n\n\n\npjitter \n\n\n\npviolin\n\n\n\n\nOther packages\n1. beeswarm plot ggbeeswarm [https://github.com/eclarke/ggbeeswarm] (and here the things start getting artistic too!) (note: not all representation for this dataset work due to the number of points)\n\nlibrary(ggbeeswarm)\n\nWarning: package 'ggbeeswarm' was built under R version 4.2.2\n\np_qrandom0 <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_quasirandom(alpha=0.05)  +  ggtitle(\"quasi_random\") \n\n#p_qrandom0\n\np_qrandom1 <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_quasirandom(alpha=0.05,method = \"tukey\")  +  ggtitle(\"Tukey\") \n\n#p_qrandom1\n\np_qrandom2 <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_quasirandom(alpha=0.05,method = \"tukeyDense\")   +  ggtitle(\"Tukey + density\") \n\n#p_qrandom2\n\np_qrandom3 <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_quasirandom(alpha=0.05,method = \"tukeyDense\")   +  ggtitle(\"Banded frowns\") \n\n#p_qrandom3\n\np_qrandom4 <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_quasirandom(alpha=0.05,method = \"frowney\")   +  ggtitle(\"Banded smiles\") \n\n#p_qrandom4\n\n#too many points\n#p_beeswarm <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\n#geom_beeswarm(alpha=0.05) +  ggtitle(\"beeswarm\") \n\np_qrandom0\n\n\n\np_qrandom1\n\n\n\np_qrandom2\n\n\n\n\nyou can halso mix plot a useful package for that is gghalves\n\nlibrary(gghalves)\n\nWarning: package 'gghalves' was built under R version 4.2.2\n\npoint_half <- ggplot(stack(box_plots), aes(x = ind, y = values)) +\ngeom_half_point(alpha=0.05) \n\ngeom_half_violin() \n\ngeom_half_violin: side = l, nudge = 0, draw_quantiles = NULL, na.rm = FALSE\nstat_half_ydensity: trim = TRUE, scale = area, na.rm = FALSE\nposition_dodge \n\npoint_half\n\n\n\n\nfinally a very useful package, also my favorite one for EDA ggstatplotthat you can find here that calculate also a lot of useful stats and combine different kind of plot in one plot\n\nlibrary(ggstatsplot)\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\n\n\nAttaching package: 'ggstatsplot'\n\n\nThe following object is masked from 'package:data.table':\n\n    :=\n\nstackbox <- stack(box_plots)\n\npstack  <- ggbetweenstats(\n  data = stackbox,\n  x = ind,\n  y = values,\n)\npstack \n\n\n\n\n\n\n\n\nReferences\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/3025453.3025912."
  },
  {
    "objectID": "posts/Tip01/index.html",
    "href": "posts/Tip01/index.html",
    "title": "Snippet #1: ggplot loops",
    "section": "",
    "text": "Create an empty list\nPopulate your list with objects (ggplots)\nCreate iteratively names for the objects\nRename the objects inside the list using the name list generated previously\nShow all plots using wrap_plots\n\nInstead of using boring plots we will use our private art collections and items.\nOne great package to create your art in R is aRtsy Let’s fire it up\n\nrequire(aRtsy)\n\nCaricamento del pacchetto richiesto: aRtsy\n\nrequire(patchwork)\n\nCaricamento del pacchetto richiesto: patchwork\n\n\n\n#before starting for having a look at the palette \n?colorPalette\n\navvio in corso del server httpd per la guida ... fatto\n\n\nCreate a Mondrian and save it\n\nset.seed(23)\nComposition_10 &lt;- canvas_squares(colors = colorPalette(\"boogy2\"))\nsaveCanvas(Composition_10 , filename = \"Mondrian.png\")\nComposition_10 \n\n\n\n\nand another one\n\nset.seed(1)\naspect_ratio &lt;- 1\nheight &lt;- 2\nComposition_1 = canvas_segments(colors = colorPalette(\"blackwhite\"))\nComposition_1 \n\n\n\n\nor if you want to create a lots of them, create names automatically and then take a look at just one of your artistic composition in your collection use the following code:\n\nn_items &lt;- 3\ncollection &lt;- list()\nname_of_Composition  &lt;- list()\nfor (i in 1:n_items) {\n  seed &lt;-  (sample(1:100000,1)) + 1\n  name_of_Composition[[i]] &lt;- paste0(\"Composition_\", i)\n  collection[[i]] &lt;- canvas_squares(colors = colorPalette(\"boogy2\"))\n  \n}\nnames(collection) &lt;- name_of_Composition\n\ncollection\n\n$Composition_1\n\n\n\n\n\n\n$Composition_2\n\n\n\n\n\n\n$Composition_3\n\n\n\n\n\n\n#as you can notice the setting for figure output in this chunk was changed in order to showplots with a rato of 3:1\nwrap_plots(collection)\n\n\n\n\n[Wickham (2016)](Derks 2022)(Pedersen 2022)\n\n\n\n\nReferences\n\nDerks, Koen. 2022. “aRtsy: Generative Art with ’Ggplot2’.” https://CRAN.R-project.org/package=aRtsy.\n\n\nPedersen, Thomas Lin. 2022. “Patchwork: The Composer of Plots.” https://CRAN.R-project.org/package=patchwork.\n\n\nWickham, Hadley. 2016. “Ggplot2: Elegant Graphics for Data Analysis.” https://ggplot2.tidyverse.org."
  },
  {
    "objectID": "posts/Tip02/index.html",
    "href": "posts/Tip02/index.html",
    "title": "Tip 2: Cleaning column names of an imported csv",
    "section": "",
    "text": "Summary\n\nImport data from a csv file\nUse the function clean_names from (Firke 2023)j R function\nWrite a function in base using gsub and regex to tackle specific issues\nYou’re done\n\nFirst of all we import the csv using the library (Müller 2020)here\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.2.2\n\n\nhere() starts at D:/giorgioluciano.github.io\n\nfile_in <- \"FakeData.csv\"\npath_in <- \"posts/Tip02/\"\ndata <- read.csv(here(path_in,file_in), head=T, check.names=F, encoding=\"latin1\")\n\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.2.2\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ndata_fixed <- clean_names(data)\n\nAnd now the function written by William Doane\n\nclinical_names <- function(.data, unique = FALSE) {\n  n <- if (is.data.frame(.data)) colnames(.data) else .data\n  n <- gsub(\"cvrisk\", \"CVrisk\", n , ignore.case=T)\n  n <- gsub(\"hbo\", \"HBO\", n , ignore.case=T)\n  n <- gsub(\"ft4\", \"fT4\", n , ignore.case=T)\n  n <- gsub(\"f_t4\", \"fT4\", n , ignore.case=T)\n  n <- gsub(\"ft3\", \"fT3\", n , ignore.case=T)\n  n <- gsub(\"f_t3\", \"fT3\", n , ignore.case=T)\n  n <- gsub(\"ldl\", \"LDL\", n , ignore.case=T)\n  n <- gsub(\"hdl\", \"HDL\", n , ignore.case=T)\n  n <- gsub(\"hba1c\", \"HbA1C\", n, ignore.case=T)\n  n <- gsub(\"hbac1\", \"HbA1C\", n, ignore.case=T)\n  n <- gsub(\"hb_ac1\", \"HbA1C\",n,ignore.case=T)\n  n <- gsub(\"\\\\igf\\\\b\", \"IGF\", n , ignore.case=T)\n  n <- gsub(\"tsh\", \"TSH\", n , ignore.case=T)\n  n <- gsub(\"acth\", \"ACTH\", n, ignore.case=T)\n  n <- gsub(\"\\\\Na\\\\b\", \"Sodio\", n)\n  n <- gsub(\"\\\\K\\\\b\",  \"Potassio\", n)\n  n <- gsub(\"\\\\P\\\\b\",  \"Fosforo\", n)\n  n <- gsub(\"\\\\pas\\\\b\", \"PAS\", n, ignore.case=T)\n  n <- gsub(\"\\\\pad\\\\b\", \"PAD\", n, ignore.case=T)\n  n <- gsub(\"\\\\pth\\\\b\", \"PTH\", n, ignore.case=T)\n  n <- gsub(\"\\\\clu\\\\b\", \"CLU\", n, ignore.case=T)\n  n <- gsub(\"\\\\tg\\\\b\", \"TG\", n, ignore.case=T)\n  n <- gsub(\"\\\\glic\\\\b\", \"glicemia\", n, ignore.case=T)\n  if (unique) n <- make.unique(n, sep = \"_\")\n  if (is.data.frame(.data)) {\n    colnames(.data) <- n\n    .data\n  } else {\n    n\n  }\n}\n\n\ndata_clean <- clinical_names(data_fixed)\n\ncomparison <- cbind(data.frame((colnames(data))),\n                        data.frame((colnames(data_fixed))),\n                        data.frame((colnames(data_clean))))\n\ncolnames(comparison) <- c(\"original\",\"fixed\",\"clean\") \n\ncomparison\n\n           original             fixed             clean\n1          paziente          paziente          paziente\n2               età               eta               eta\n3               SEX               sex               sex\n4          diagnosi          diagnosi          diagnosi\n5           terapia           terapia           terapia\n6             tempo             tempo             tempo\n7            Cvrisk            cvrisk            CVrisk\n8              peso              peso              peso\n9        delta Peso        delta_peso        delta_peso\n10              BMI               bmi               bmi\n11         deltaBMI         delta_bmi         delta_bmi\n12              PAS               pas               PAS\n13         deltaPas         delta_pas         delta_PAS\n14              pad               pad               PAD\n15         deltaPad         delta_pad         delta_PAD\n16              HBO               hbo               HBO\n17           neutro            neutro            neutro\n18            linfo             linfo             linfo\n19             glic              glic          glicemia\n20    deltaglicemia     deltaglicemia     deltaglicemia\n21            HBAC1             hbac1             HbA1C\n22       deltaHbAc1      delta_hb_ac1       delta_HbA1C\n23            sodio             sodio             sodio\n24         potassio          potassio          potassio\n25           calcio            calcio            calcio\n26          fosforo           fosforo           fosforo\n27      colesterolo       colesterolo       colesterolo\n28 deltaColesterolo delta_colesterolo delta_colesterolo\n29              HDL               hdl               HDL\n30         deltaHDL         delta_hdl         delta_HDL\n31              ldl               ldl               LDL\n32         deltaLDL         delta_ldl         delta_LDL\n33               TG                tg                tg\n34          deltaTG          delta_tg          delta_tg\n35             ACTH              acth              ACTH\n36        cortisolo         cortisolo         cortisolo\n37              CLU               clu               CLU\n38              IGF               igf               IGF\n39              TSH               tsh               TSH\n40              fT4              f_t4               fT4\n41              PTH               pth               PTH\n42       Vitamina D        vitamina_d        vitamina_d\n43          dose_CA           dose_ca           dose_ca\n44          dose_HC           dose_hc           dose_hc\n45          dose_PL           dose_pl           dose_pl\n46 dose equivalente  dose_equivalente  dose_equivalente\n\n\n\n\n\n\nReferences\n\nFirke, Sam. 2023. “Janitor: Simple Tools for Examining and Cleaning Dirty Data.” https://CRAN.R-project.org/package=janitor.\n\n\nMüller, Kirill. 2020. “Here: A Simpler Way to Find Your Files.” https://CRAN.R-project.org/package=here."
  },
  {
    "objectID": "posts/Tip03/index.html",
    "href": "posts/Tip03/index.html",
    "title": "Tip 3: Functions for simulating data",
    "section": "",
    "text": "Summary\n\nExample of creating variables using runif and rnorm\nWriting a function that wraps all\n\nFirst of all we use the runif and rnorm to have a look how they work.\n\nlibrary(data.table)\nx_min   <- 0\nx_max   <- 10   \nx_step  <- 0.01\n\ny_mean  <- 0.5\ny_sd    <- 0.25\ny_min   <- -1\ny_max   <- 1   \n\nx       <- seq(x_min,x_max,x_step)\nvar_random  <- runif(x,y_min,y_max)\nvar_norm    <- rnorm(x,y_mean,y_sd) \n\ndf  <- data.frame (x,var_random,var_norm)\ndt  <- data.table(df)\n\n\nsimpleDataset <- function(number_of_rows,means,sds)\n{\nl <- length(means)\nres <- lapply(seq(1:l),function(x) \n       eval(\n       parse(\n       text=paste(\"rnorm(\",number_of_rows,\",\",means[x],\",\",sds[x],\")\",sep=\"\"))\n       )\n       ) \ndat <- data.frame((sapply(res,c)))\nid <- rownames(dat)\ndat <-  cbind(id=id,dat)\ndt <- data.table(dat)\nreturn(dt)\n}\n\nExample: We simulate the values of the LDL cholesterol of 2 patients in 3 different times. The first one patient (X1) has an average value of 200 of LDL with a standard variation of 2 while the second (X2) has an average of 150 with a standard deviation of 10. Note: All values are expressed in mg/dL\n\ndataset1 <- simpleDataset(3,c(200,180),c(2,10))\ndataset1\n\n   id       X1       X2\n1:  1 200.6442 196.5358\n2:  2 200.6801 166.5065\n3:  3 200.4200 196.2089\n\n\nExample: this time we combine runif and simpleDataset. We simulate the values of the LDL cholesterol of 5 patients in 7 different times. The values for each patient are between a min = 100 and a max = 150 with a standard deviation between a min sd = 10 and max sd = 40. We also simulate two time that presents outliers values between a min = 180 and max = 200 and an min sd = 10 and max sd = 40 . We merge the values for each patient (7 times + 2 outliers times) and finally we use the function melt to reshape the dataset.\n\ndat1 <- simpleDataset(number_of_rows=7,\n                      means=runif(5,100,150),\n                      sds=runif(5,10,40))\noutliers <- simpleDataset(number_of_rows=2,\n                      means=runif(5,180,200),\n                      sds=runif(5,10,40))                 \n\ndat1\n\n   id        X1       X2        X3        X4       X5\n1:  1  97.92524 105.5036 110.66623  34.45252 108.1226\n2:  2 169.96962 136.5666  97.62086 -11.68481 135.7269\n3:  3  77.46899 135.7573  93.37948 133.03592 129.4088\n4:  4 197.75026 168.3904 132.91647 154.98579 164.1461\n5:  5 121.99923 140.2879 127.80267 165.29857 158.9153\n6:  6 176.63684 127.6318 107.84311 138.62779 157.1849\n7:  7 117.90619 176.1431  99.01017 166.91662 219.9138\n\noutliers\n\n   id       X1       X2       X3       X4       X5\n1:  1 202.0947 191.3382 138.8483 194.0729 207.0543\n2:  2 194.0590 196.3606 209.5827 248.9402 175.0254\n\ndato     <-rbind(dat1,outliers) \ndt.melt <- melt(dat1, id.vars=\"id\")\ncolnames(dt.melt) <- c(\"id\",\"category\",\"var1\")\ndt.melt$ncat <- as.numeric(dt.melt$category)\n\ndt.melt\n\n    id category      var1 ncat\n 1:  1       X1  97.92524    1\n 2:  2       X1 169.96962    1\n 3:  3       X1  77.46899    1\n 4:  4       X1 197.75026    1\n 5:  5       X1 121.99923    1\n 6:  6       X1 176.63684    1\n 7:  7       X1 117.90619    1\n 8:  1       X2 105.50360    2\n 9:  2       X2 136.56661    2\n10:  3       X2 135.75728    2\n11:  4       X2 168.39042    2\n12:  5       X2 140.28791    2\n13:  6       X2 127.63183    2\n14:  7       X2 176.14312    2\n15:  1       X3 110.66623    3\n16:  2       X3  97.62086    3\n17:  3       X3  93.37948    3\n18:  4       X3 132.91647    3\n19:  5       X3 127.80267    3\n20:  6       X3 107.84311    3\n21:  7       X3  99.01017    3\n22:  1       X4  34.45252    4\n23:  2       X4 -11.68481    4\n24:  3       X4 133.03592    4\n25:  4       X4 154.98579    4\n26:  5       X4 165.29857    4\n27:  6       X4 138.62779    4\n28:  7       X4 166.91662    4\n29:  1       X5 108.12259    5\n30:  2       X5 135.72688    5\n31:  3       X5 129.40880    5\n32:  4       X5 164.14611    5\n33:  5       X5 158.91531    5\n34:  6       X5 157.18490    5\n35:  7       X5 219.91383    5\n    id category      var1 ncat\n\nstr(dt.melt)\n\nClasses 'data.table' and 'data.frame':  35 obs. of  4 variables:\n $ id      : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ category: Factor w/ 5 levels \"X1\",\"X2\",\"X3\",..: 1 1 1 1 1 1 1 2 2 2 ...\n $ var1    : num  97.9 170 77.5 197.8 122 ...\n $ ncat    : num  1 1 1 1 1 1 1 2 2 2 ...\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "posts/Tip04/index.html",
    "href": "posts/Tip04/index.html",
    "title": "Snippet #4: boxplots and scatterplots: simple recipes",
    "section": "",
    "text": "Simulate data, check and assign data types\nCreate a scatterplot with ggplot\nCreate violin plot with ggstatsplot\n\nExample 1: We want to visualize the difference between two groups of patients that follow two different diets. Group A has an average of total cholesterol of 180 with a standard deviation of 20 while Group B and average of 200 with a standard deviation of 40\n\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(data.table)\n\n\nnpatientsA &lt;- 500\nnpatientsB &lt;- 520\ncholA &lt;- mvrnorm(n=npatientsA, mu=180, Sigma=20, empirical=T)\ncholB &lt;- mvrnorm(n=npatientsB, mu=200, Sigma=40, empirical=T)\n\ndataA &lt;- cbind(cholA,rep(\"A\",npatientsA))  \ndataB &lt;- cbind(cholB,rep(\"B\",npatientsB))  \n\ndata &lt;- data.frame(rbind(dataA,dataB))\ncolnames(data) &lt;- c(\"Cholesterol\",\"group\")\ndata$Cholesterol &lt;- as.numeric(data$Cholesterol)\n\np1 &lt;-ggplot(data, aes(x = group, y = Cholesterol)) + geom_jitter(alpha=0.05) \n\np1\n\n\n\n\nA few observations on the code. First of all, we need to input the data in a data.frame otherwise ggplot will give us an error. The second observation is that since we put chr labels on our groups we needed to define Cholesterol as.numeric in order to avoid unwanted resultsstrange results. Try to comment the line data$Cholesterol &lt;- as.numeric(data$Cholesterol) and you can see by yourself what will happen. (hint: a “labelstorm!”)\nJiiter plots is one of my favorite way to represent data. data and immediately understand the distribution of your data and also avoid the pitfall of boxplot (see (Matejka and Fitzmaurice 2017))\nIf you need inferential statistics on your data another resource is (Patil 2021). See the following example with our data. NOTE that we nee to transform the group label as.factor\n\nlibrary(ggstatsplot)\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\ndata$group &lt;- as.factor(data$group)\n\npstack  &lt;- ggbetweenstats(data,group,Cholesterol)\n                          \npstack    \n\n\n\n\n\n\n\n\nReferences\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/3025453.3025912.\n\n\nPatil, Indrajeet. 2021. “Visualizations with Statistical Details: The ’Ggstatsplot’ Approach” 6: 3167. https://doi.org/10.21105/joss.03167."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html",
    "href": "posts/Packages Review 2022/index.html",
    "title": "R Packages 2022 list",
    "section": "",
    "text": "List of packages I’ve found useful in my workflow during 2022 (in no particular order)"
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#plot",
    "href": "posts/Packages Review 2022/index.html#plot",
    "title": "R Packages 2022 list",
    "section": "Plot",
    "text": "Plot\n\nggvoronoi: Voronoi Diagrams and Heatmaps with ‘ggplot2’\ntags: #ggplot #tidyverse #voronoi\n[cran package link] https://CRAN.R-project.org/package=ggvoronoi\n\ndescription from the author/vignette\n\n\nEasy creation and manipulation of Voronoi diagrams using ‘deldir’ with visualization in ‘ggplot2’. Convenient functions are provided to create nearest neighbor diagrams and heatmaps. Diagrams are computed with ‘deldir’ and processed to work with the ‘sp’ framework. Results are provided in a convenient spatial data structure and displayed with ‘ggplot2’. An outline can be provided by the user to specify the spatial domain of interest.\n\n\n\nggh4x: Hacks for ‘ggplot2’\ntags: #ggplot #tidyverse\n[cran package link] https://CRAN.R-project.org/package=ggh4x\n\ndescription from the author/vignette\n\n\nA ‘ggplot2’ extension that does a variety of little helpful things. The package extends ‘ggplot2’ facets through customisation, by setting individual scales per panel, resizing panels and providing nested facets. Also allows multiple colour and fill scales per plot. Also hosts a smaller collection of stats, geoms and axis guides.\n\n\n\ngdiff: Graphical Difference Testing\ntags: #plot #ggplot #data analysis #comparison\n[cran package link] https://CRAN.R-project.org/package=gdiff\ndescription from the author/vignette\n\n\nFunctions for performing graphical difference testing. Differences are generated between raster images. Comparisons can be performed between different package versions and between different R versions.\n\n\n\naplot: Decorate a ‘ggplot’ with Associated Information\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=aplot\ndescription from the author/vignette \nFor many times, we are not just aligning plots as what ‘cowplot’ and ‘patchwork’ did. Users would like to align associated information that requires axes to be exactly matched in subplots, e.g. hierarchical clustering with a heatmap. This package provides utilities to aligns associated subplots to a main plot at different sides (left, right, top and bottom) with axes exactly matched.\n\n\ngghighlight: Highlight Lines and Points in ‘ggplot2’\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=gghighlight\ndescription from the author/vignette\n\nMake it easier to explore data with highlights.\n\n\n\npdp: Partial Dependence Plots\ntags: #plot #data analysis #ggplot [cran package link] https://CRAN.R-project.org/package=pdp\ndescription from the author/vignette\n\nA general framework for constructing partial dependence (i.e., marginal effect) plots from various types machine learning models in R.\n\n\n\nvcd: Visualizing Categorical Data\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=testDriveR\ndescription from the author/vignette\n\nVisualization techniques, data sets, summary and inference procedures aimed particularly at categorical data. Special emphasis is given to highly extensible grid graphics. The package was package was originally inspired by the book “Visualizing Categorical Data” by Michael Friendly and is now the main support package for a new book, “Discrete Data Analysis with R” by Michael Friendly and David Meyer (2015).\n\n\n\ngTestsMulti: New Graph-Based Multi-Sample Tests\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=gTestsMulti\ndescription from the author/vignette\n\nNew multi-sample tests for testing whether multiple samples are from the same distribution. They work well particularly for high-dimensional data. Song, H. and Chen, H. (2022) &lt;arXiv:2205.13787&gt;.\n\n\n\nspiralize: Visualize Data on Spirals\ntags: #statistic #visualization #plot\n[cran package link] https://CRAN.R-project.org/package=spiralize\ndescription from the author/vignette\n\nIt visualizes data along an Archimedean spiral https://en.wikipedia.org/wiki/Archimedean_spiral, makes so-called spiral graph or spiral chart. It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns.\n\n\n\nvaluemap: Making Choropleth Map\ntags: #plot #valuemap\n[cran package link] https://CRAN.R-project.org/package=valuemap\ndescription from the author/vignette\n\nYou can easily visualize your ‘sf’ polygons or data.frame with h3 address. While ‘leaflet’ package is too raw for data analysis, this package can save data analysts’ efforts & time with pre-set visualize options.\n\n\n\ntessellation: Delaunay and Voronoï Tessellations\ntags: #tesselation #voronoi #delaunay\n[cran package link] https://cran.r-project.org/package=tessellation\ndescription from the author/vignette\n\nDelaunay and Voronoï tessellations, with emphasis on the two-dimensional and the three-dimensional cases (the package provides functions to plot the tessellations for these cases). Delaunay tessellations are computed in C with the help of the ‘Qhull’ library http://www.qhull.org/.\n\n\n\nggdist: Visualizations of Distributions and Uncertainty\ntags: #ggplot #Distributions #Uncertainty\n[cran package link] https://cran.r-project.org/package=ggdist\ndescription from the author/vignette\n\nProvides primitives for visualizing distributions using ‘ggplot2’ that are particularly tuned for visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as bootstrap distributions or Bayesian posterior samples) are easily visualized. Visualization primitives include but are not limited to: points with multiple uncertainty intervals, eye plots (Spiegelhalter D., 1999) https://ideas.repec.org/a/bla/jorssa/v162y1999i1p45-58.html, density plots, gradient plots, dot plots (Wilkinson L., 1999) doi:10.1080/00031305.1999.10474474, quantile dot plots (Kay M., Kola T., Hullman J., Munson S., 2016) doi:10.1145/2858036.2858558, complementary cumulative distribution function barplots (Fernandes M., Walls L., Munson S., Hullman J., Kay M., 2018) doi:10.1145/3173574.3173718, and fit curves with multiple uncertainty ribbons.\n\n\n\ngrafify: Easy Graphs for Data Visualisation and Linear Models for ANOVA\ntags: #multivariate #Inference #tests #statistics\n[cran package link] https://cran.r-project.org//package=energy\ndescription from the author/vignette\n\nE-statistics (energy) tests and statistics for multivariate and univariate inference, including distance correlation, one-sample, two-sample, and multi-sample tests for comparing multivariate distributions, are implemented. Measuring and testing multivariate independence based on distance correlation, partial distance correlation, multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy distance, testing for multivariate normality, distance components (disco) for non-parametric analysis of structured data, and other energy statistics/methods are implemented.\n\n\n\nDiagrammeR: Graph/Network Visualization\ntags: #graph #networks\n[cran package link] https://cran.r-project.org/package=DiagrammeR\ndescription from the author/vignette\n\nBuild graph/network structures using functions for stepwise addition and deletion of nodes and edges. Work with data available in tables for bulk addition of nodes, edges, and associated metadata. Use graph selections and traversals to apply changes to specific nodes or edges. A wide selection of graph algorithms allow for the analysis of graphs. Visualize the graphs and take advantage of any aesthetic properties assigned to nodes and edges.\n\n\n\nnetplot: Beautiful Graph Drawing\ntags: #plot #graph\n[cran package link] https://cran.r-project.org/package=netplot\ndescription from the author/vignette\n\nA graph visualization engine that puts an emphasis on aesthetics at the same time of providing default parameters that yield out-of-the-box-nice visualizations. The package is built on top of ‘The Grid Graphics Package’ and seamlessly work with ‘igraph’ and ‘network’ objects.\n\n\n\nvivid: variable importance and variable interaction displays\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=vivid\ndescription from the author/vignette\n\nVariable importance, interaction measures and partial dependence plots are important summaries in the interpretation of statistical and machine learning models. In our R package vivid (variable importance and variable interaction displays) we create new visualisation techniques for exploring these model summaries. We construct heatmap and graph-based displays showing variable importance and interaction jointly, which are carefully designed to highlight important aspects of the fit. We also construct a new matrix-type layout showing all single and bivariate partial dependence plots, and an alternative layout based on graph Eulerians focusing on key subsets. Our new visualisations are model-agnostic and are applicable to regression and classification supervised learning settings. They enhance interpretation even in situations where the number of variables is large and the interaction structure complex.\n\n\n\ngridpattern: ‘grid’ Pattern Grobs\ntags: #database #relational #data\n[cran package link] https://cran.r-project.org/package=gridpattern\ndescription from the author/vignette\n\nProvides ‘grid’ grobs that fill in a user-defined area with various patterns. Includes enhanced versions of the geometric and image-based patterns originally contained in the ‘ggpattern’ package as well as original ‘pch’, ‘polygon_tiling’, ‘regular_polygon’, ‘rose’, ‘text’, ‘wave’, and ‘weave’ patterns plus support for custom user-defined patterns.\n\n\n\nPairViz: Visualization using Graph Traversal\ntags: #graphs #visualization [cran package link] https://cran.r-project.org/package=PairViz\ndescription from the author/vignette\n\nImproving graphics by ameliorating order effects, using Eulerian tours and Hamiltonian decompositions of graphs. References for the methods presented here are C.B. Hurley and R.W. Oldford (2010) doi:10.1198/jcgs.2010.09136 and C.B. Hurley and R.W. Oldford (2011) doi:10.1007/s00180-011-0229-5.\n\n\n\nDiagrammeR: Graph/Network Visualization\ntags: #graph #networks\n[cran package link] https://cran.r-project.org/package=DiagrammeR\ndescription from the author/vignette\n\nBuild graph/network structures using functions for stepwise addition and deletion of nodes and edges. Work with data available in tables for bulk addition of nodes, edges, and associated metadata. Use graph selections and traversals to apply changes to specific nodes or edges. A wide selection of graph algorithms allow for the analysis of graphs. Visualize the graphs and take advantage of any aesthetic properties assigned to nodes and edges.\n\n\n\nggimage: Use Image in ‘ggplot2’\ntags: #ggplot [cran package link] https://cran.r-project.org/package=ggimage\ndescription from the author/vignette\n\nSupports image files and graphic objects to be visualized in ‘ggplot2’ graphic system.\n\n\n\nsuperb: Summary Plots with Adjusted Error Bars\ntags: #ggplot #summary #plots\n[cran package link] https://cran.r-project.org//package=superb\ndescription from the author/vignette\n\nComputes standard error and confidence interval of various descriptive statistics under various designs and sampling schemes. The main function, superbPlot(), can either return a plot or a dataframe with the statistic and its precision interval so that other plotting package can be used. See Cousineau and colleagues (2021) doi:10.1177/25152459211035109 or Cousineau (2017) doi:10.5709/acp-0214-z for a review as well as Cousineau (2005) doi:10.20982/tqmp.01.1.p042, Morey (2008) doi:10.20982/tqmp.04.2.p061, Baguley (2012) doi:10.3758/s13428-011-0123-7, Cousineau & Laurencelle (2016) doi:10.1037/met0000055, Cousineau & O’Brien (2014) doi:10.3758/s13428-013-0441-z, Calderini & Harding doi:10.20982/tqmp.15.1.p001 for specific references.\n\n\n\nkhroma: Colour Schemes for Scientific Data Visualization\ntags: #plot #colors\n[cran package link] https://cran.r-project.org/web/package=khroma\n\nColour schemes ready for each type of data (qualitative, diverging or sequential), with colours that are distinct for all people, including colour-blind readers. This package provides an implementation of Paul Tol (2018) and Fabio Crameri (2018) doi:10.5194/gmd-11-2541-2018 colour schemes for use with ‘graphics’ or ‘ggplot2’. It provides tools to simulate colour-blindness and to test how well the colours of any palette are identifiable. Several scientific thematic schemes (geologic timescale, land cover, FAO soils, etc.) are also implemented\n\n\n\nggside: Side Grammar Graphics\ntags: #plot #ggplot\n[cran package link] https://cran.r-project.org//package=ggside\ndescription from the author/vignette\n\nThe grammar of graphics as shown in ‘ggplot2’ has provided an expressive API for users to build plots. ‘ggside’ extends ‘ggplot2’ by allowing users to add graphical information about one of the main panel’s axis using a familiar ‘ggplot2’ style API with tidy data. This package is particularly useful for visualizing metadata on a discrete axis, or summary graphics on a continuous axis such as a boxplot or a density distribution.\n\n\n\nggquiver: Quiver Plots for ‘ggplot2’\ntags: #plot #ggplot\n[cran package link] https://cran.r-project.org/package=ggquiver\ndescription from the author/vignette\n\nAn extension of ‘ggplot2’ to provide quiver plots to visualise vector fields. This functionality is implemented using a geom to produce a new graphical layer, which allows aesthetic options. This layer can be overlaid on a map to improve visualisation of mapped data.\n\n\n\ntimevis: Create Interactive Timeline Visualizations in R\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/package=timevis\ndescription from the author/vignette\n\nCreate rich and fully interactive timeline visualizations. Timelines can be included in Shiny apps and R markdown documents, or viewed from the R console and ‘RStudio’ Viewer. ‘timevis’ includes an extensive API to manipulate a timeline after creation, and supports getting data out of the visualization into R. Based on the ‘vis.js’ Timeline module and the ‘htmlwidgets’ R package.\n\n\n\nmisc3d: Miscellaneous 3D Plots\ntags: #plot #misc\n[cran package link] https://cran.r-project.org//package=misc3d\ndescription from the author/vignette\n\nA collection of miscellaneous 3d plots, including isosurfaces.."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#math",
    "href": "posts/Packages Review 2022/index.html#math",
    "title": "R Packages 2022 list",
    "section": "Math",
    "text": "Math\n\nlmtest: Testing Linear Regression Models\ntags: #linear regression #testing\n[cran package link] https://cran.r-project.org/package=lmtest\ndescription from the author/vignette\n\nA collection of tests, data sets, and examples for diagnostic checking in linear regression models. Furthermore, some generic tools for inference in parametric models are provided.\n\n\n\noptimx: Expanded Replacement and Extension of the ‘optim’ Function\ntags: #optim\n[cran package link] https://cran.r-project.org/packages=optimx\ndescription from the author/vignette\n\nProvides a replacement and extension of the optim() function to call to several function minimization codes in R in a single statement. These methods handle smooth, possibly box constrained functions of several or many parameters. Note that function ‘optimr()’ was prepared to simplify the incorporation of minimization codes going forward. Also implements some utility codes and some extra solvers, including safeguarded Newton methods. Many methods previously separate are now included here."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#statistics",
    "href": "posts/Packages Review 2022/index.html#statistics",
    "title": "R Packages 2022 list",
    "section": "Statistics",
    "text": "Statistics\n\ncorrr: Correlations in R\ntags: #statistics #data #correlation #calculus\n[cran package link] https://CRAN.R-project.org/package=corrr]\ndescription from the author/vignette\n\nA ‘ggplot2’ extension that does a variety of little helpful things. The package extends ‘ggplot2’ facets through customisation, by setting individual scales per panel, resizing panels and providing nested facets. Also allows multiple colour and fill scales per plot. Also hosts a smaller collection of stats, geoms and axis guides.\n\n\n\nFactoMineR: Multivariate Exploratory Data Analysis and Data Mining\ntags: #statistics #pca #clustering #multivariate #data\n[cran package link]https://CRAN.R-project.org/package=FactoMineR\ndescription from the author/vignette\n\nExploratory data analysis methods to summarize, visualize and describe datasets. The main principal component methods are available, those with the largest potential in terms of applications: principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) and multiple correspondence analysis (MCA) when variables are categorical, Multiple Factor Analysis when variables are structured in groups, etc. and hierarchical cluster analysis. F. Husson, S. Le and J. Pages (2017).\n\n\n\nVIM: Visualization and Imputation of Missing Values\ntags: #data #missing values\n[cran package link] https://CRAN.R-project.org/package=VIM\ndescription from the author/vignette\n\nNew tools for the visualization of missing and/or imputed values are introduced, which can be used for exploring the data and the structure of the missing and/or imputed values. Depending on this structure of the missing values, the corresponding methods may help to identify the mechanism generating the missing values and allows to explore the data including missing values. In addition, the quality of imputation can be visually explored using various univariate, bivariate, multiple and multivariate plot methods. A graphical user interface available in the separate package VIMGUI allows an easy handling of the implemented plot methods.\n\n\n\ncfda: Categorical Functional Data Analysis\ntags: #data #categorical\n[cran package link] https://CRAN.R-project.org/package=cfda\ndescription from the author/vignette\n\nPackage for the analysis of categorical functional data. The main purpose is to compute an encoding (real functional variable) for each state doi:10.3390/math9233074. It also provides functions to perform basic statistical analysis on categorical functional data.\n\n\n\nSHT: Statistical Hypothesis Testing Toolbox\ntags: #statistics #data analysis #comparison\n[cran package link] https://CRAN.R-project.org/package=SHT\ndescription from the author/vignette\n\nWe provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) doi:10.1007/0-387-27605-X.\n\n\n\ncontingencytables: Statistical Analysis of Contingency Tables\ntags: #plot #data analysis #ggplot\n[cran package link] &lt;https://contingencytables.com/\ndescription from the author/vignette\n\nProvides functions to perform statistical inference of data organized in contingency tables. This package is a companion to the “Statistical Analysis of Contingency Tables” book by Fagerland et al. &lt;ISBN 9781466588172&gt;.\n\n\n\nMorphoTools2: Multivariate Morphometric Analysis\ntags: #statistics #multivatiate [cran package link] https://CRAN.R-project.org/package=MorphoTools2\ndescription from the author/vignette\n\nTools for multivariate analyses of morphological data, wrapped in one package, to make the workflow convenient and fast. Statistical and graphical tools provide a comprehensive framework for checking and manipulating input data, statistical analyses, and visualization of results. Several methods are provided for the analysis of raw data, to make the dataset ready for downstream analyses. Integrated statistical methods include hierarchical classification, principal component analysis, principal coordinates analysis, non-metric multidimensional scaling, and multiple discriminant analyses: canonical, stepwise, and classificatory (linear, quadratic, and the non-parametric k nearest neighbours). The philosophy of the package will be described in Šlenker et al. (in prep).\n\n\n\nautostats: Auto Stats\ntags: #statistic #reports #exploration\n[cran package link]https://CRAN.R-project.org/package=autostats\ndescription from the author/vignette\n\nAutomatically do statistical exploration. Create formulas using ‘tidyselect’ syntax, and then determine cross-validated model accuracy and variable contributions using ‘glm’ and ‘xgboost’. Contains additional helper functions to create and modify formulas. Has a flagship function to quickly determine relationships between categorical and continuous variables in the data set.\n\n\n\nflexclust: Flexible Cluster Algorithms\ntags: #classificatrion #clusters #multivariate\n[cran package link] https://cran.r-project.org/package=flexclust\ndescription from the author/vignette\n\nThe main function kcca implements a general framework for k-centroids cluster analysis supporting arbitrary distance measures and centroid computation. Further cluster methods include hard competitive learning, neural gas, and QT clustering. There are numerous visualization methods for cluster results (neighborhood graphs, convex cluster hulls, barcharts of centroids, …), and bootstrap methods for the analysis of cluster stability.\n\n\n\nffmanova: Fifty-Fifty MANOVA\ntags: #MANOVA #MANCONVA\n[cran package link] https://cran.r-project.org/package=ffmanova\ndescription from the author/vignette\n\nGeneral linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) doi:10.1111/1467-9884.00320, which handles collinear responses. Rotation testing, described by Langsrud (2005) doi:10.1007/s11222-005-4789-5, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) doi:10.1128/AEM.71.4.2086-2094.2005. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) doi:10.1023/A:1023260610025. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) doi:10.1080/02664760701594246. This means that the method is invariant to scale changes and that common pitfalls are avoided.\n\n\n\ncompareGroups 4.0: Descriptives by groups\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=compareGroups\ndescription from the author/vignette\n\ncompareGroups is an R package available on CRAN which performs descriptive tables displaying means, standard deviation, quantiles or frequencies of several variables. Also, p-value to test equality between groups is computed using the appropiate test. With a very simple code, nice, compact and ready-to-publish descriptives table are displayed on R console. They can also be exported to different formats, such as Word, Excel, PDF or inserted in a R-Sweave or R-markdown d\n\n\n\nDescTools: Tools for Descriptive Statistics\ntags: #statistics\n[cran package link] https://cran.r-project.org//package=DescTools\ndescription from the author/vignette\n\nA collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author’s intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The ‘BigCamelCase’ style was consequently applied to functions borrowed from contributed R packages as well.\n\n\n\noutForest: Multivariate Outlier Detection and Replacement\ntags: #random forest #outliers\n[cran package link] https://cran.r-project.org/package=outForest\ndescription from the author/vignette\n\nProvides a random forest based implementation of the method described in Chapter 7.1.2 (Regression model based anomaly detection) of Chandola et al. (2009) doi:10.1145/1541880.1541882. It works as follows: Each numeric variable is regressed onto all other variables by a random forest. If the scaled absolute difference between observed value and out-of-bag prediction of the corresponding random forest is suspiciously large, then a value is considered an outlier. The package offers different options to replace such outliers, e.g. by realistic values found via predictive mean matching. Once the method is trained on a reference data, it can be applied to new data.\n\n\n\nmultid: Multivariate Difference Between Two Groups\ntags: #multivariate #test\n[cran package link] https://cran.r-project.org/package=multid\ndescription from the author/vignette\n\nEstimation of multivariate differences between two groups (e.g., multivariate sex differences) with regularized regression methods and predictive approach. See Lönnqvist & Ilmarinen (2021) doi:10.1007/s11109-021-09681-2 and Ilmarinen et al. (2021) doi:10.31234/osf.io/j59bs.\n\n\n\nsimpr: Flexible ‘Tidyverse’-Friendly Simulations\ntags: #simulation #tidyverse\n[cran package link] https://cran.r-project.org/package=simpr\ndescription from the author/vignette\n\nA general, ‘tidyverse’-friendly framework for simulation studies, design analysis, and power analysis. Specify data generation, define varying parameters, generate data, fit models, and tidy model results in a single pipeline, without needing loops or custom functions.\n\n\n\nggfortify: Data Visualization Tools for Statistical Analysis Results\ntags: #statistics #datavis\n[cran package link] https://cran.r-project.org/web/packages/ggfortify/index.html\ndescription from the author/vignette\n\nUnified plotting tools for statistics commonly used, such as GLM, time series, PCA families, clustering and survival analysis. The package offers a single plotting interface for these analysis results and plots in a unified style using ‘ggplot2’.\n\n\n\nMVTests: Multivariate Hypothesis Tests\ntags: #statistics #test #multivariate\n[cran package link] https://cran.r-project.org/package=MVTests\ndescription from the author/vignette\n\nMultivariate hypothesis tests and the confidence intervals. It can be used to test the hypothesizes about mean vector or vectors (one-sample, two independent samples, paired samples), covariance matrix (one or more matrices), and the correlation matrix. Moreover, it can be used for robust Hotelling T^2 test at one sample case in high dimensional data. For this package, we have benefited from the studies Rencher (2003), Nel and Merwe (1986) doi:10.1080/03610928608829342, Tatlidil (1996), Tsagris (2014), Villasenor Alva and Estrada (2009) doi:10.1080/03610920802474465.\n\n\n\nplsVarSel: Variable Selection in Partial Least Squares\ntags: #pls #chemometrics\n[cran package link] https://cran.r-project.org/packages=plsVarSel\ndescription from the author/vignette\n\nInterfaces and methods for variable selection in Partial Least Squares. The methods include filter methods, wrapper methods and embedded methods. Both regression and classification is supported.\n\n\n\nRcmdrPlugin.EZR: R Commander Plug-in for the EZR (Easy R) Package\ntags: #statistics #ROC\n[cran package link] https://cran.r-project.org/package=RcmdrPlugin.EZR/index.html\ndescription from the author/vignette\n\nEZR (Easy R) adds a variety of statistical functions, including survival analyses, ROC analyses, metaanalyses, sample size calculation, and so on, to the R commander. EZR enables point-and-click easy access to statistical functions, especially for medical statistics. EZR is platform-independent and runs on Windows, Mac OS X, and UNIX. Its complete manual is available only in Japanese (Chugai Igakusha, ISBN: 978-4-498-10918-6, Nankodo, ISBN: 978-4-524-26158-1, Ohmsha, ISBN: 978-4-274-22632-8), but an report that introduced the investigation of EZR was published in Bone Marrow Transplantation (Nature Publishing Group) as an Open article. This report can be used as a simple manual. It can be freely downloaded from the journal website as shown below. This report has been cited in more than 3,000 scientific articles.\n\n\n\nRcmdrPlugin.NMBU: R Commander Plug-in for University Level Applied Statistics\ntags: #PLS #LDA #QDA\n[cran package link] https://cran.r-project.org/package=RcmdrPlugin.NMBU\ndescription from the author/vignette\n\nAn R Commander “plug-in” extending functionality of linear models and providing an interface to Partial Least Squares Regression and Linear and Quadratic Discriminant analysis. Several statistical summaries are extended, predictions are offered for additional types of analyses, and extra plots, tests and mixed models are available.\n\n\n\nDataEditR: An Interactive Editor for Viewing, Entering, Filtering & Editing Data\ntags: #tables #editor\n[cran package link] https://cran.r-project.org/package=DataEditR\ndescription from the author/vignette\n\nAn interactive editor built on ‘rhandsontable’ to allow the interactive viewing, entering, filtering and editing of data in R https://dillonhammill.github.io/DataEditR/.\n\n\n\nfICA: Classical, Reloaded and Adaptive FastICA Algorithms\ntags: #ggplot #summary #plots\n[cran package link] https://cran.r-project.org/package=fICA\ndescription from the author/vignette\n\nAlgorithms for classical symmetric and deflation-based FastICA, reloaded deflation-based FastICA algorithm and an algorithm for adaptive deflation-based FastICA using multiple nonlinearities. For details, see Miettinen et al. (2014) doi:10.1109/TSP.2014.2356442 and Miettinen et al. (2017) doi:10.1016/j.sigpro.2016.08.028. The package is described in Miettinen, Nordhausen and Taskinen (2018) doi:10.32614/RJ-2018-046.\n\n\n\nJFE: Tools and GUI for Analyzing Time Series Data of Just Finance and Econometrics\ntags: #econometrics #finance\n[cran package link] https://cran.r-project.org/web/packages/JFE/index.html\ndescription from the author/vignette\n\nSupport the analysis of financial and econometric time series, including recursive forecasts for machine learning.\n\n\n\nanscombiser: Create Datasets with Identical Summary Statistics\ntags: #statistics #anscombe\n[cran package link] https://cran.r-project.org/package=anscombiser\ndescription from the author/vignette\n\nThe anscombiser package takes a simpler and quicker approach to the same problem, using Anscombe’s statistics. It uses shifting, scaling and rotating to transform the observations in an input dataset to achieve a target set of Anscombe’s statistics”\n\n\n\nrandtoolbox: Toolbox for Pseudo and Quasi Random Number Generation and Random Generator Tests\ntags: #distributions\n[cran package link] https://cran.r-project.org/package=randtoolbox\ndescription from the author/vignette\n\nProvides (1) pseudo random generators - general linear congruential generators, multiple recursive generators and generalized feedback shift register (SF-Mersenne Twister algorithm and WELL generators); (2) quasi random generators - the Torus algorithm, the Sobol sequence, the Halton sequence (including the Van der Corput sequence) and (3) some generator tests - the gap test, the serial test, the poker test. See e.g. Gentle (2003) doi:10.1007/b97336. The package can be provided without the rngWELL dependency on demand. Take a look at the Distribution task view of types and tests of random number generators. Version in Memoriam of Diethelm and Barbara Wu"
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#medicine",
    "href": "posts/Packages Review 2022/index.html#medicine",
    "title": "R Packages 2022 list",
    "section": "Medicine",
    "text": "Medicine\n\nvisR: Clinical Graphs and Tables Adhering to Graphical Principles\ntags: #data #clinical\n[cran package link] https://CRAN.R-project.org/package=cfda\ndescription from the author/vignette\n\nTo enable fit-for-purpose, reusable clinical and medical research focused visualizations and tables with sensible defaults and based on graphical principles as described in: “Vandemeulebroecke et al. (2018)” doi:10.1002/pst.1912, “Vandemeulebroecke et al. (2019)” doi:10.1002/psp4.12455, and “Morris et al. (2019)” doi:10.1136/bmjopen-2019-030215.\n\n\n\nvbp: Blood Pressure Analysis in R\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=bp\ndescription from the author/vignette\n\nCardiovascular disease (CVD) is the leading cause of death worldwide with Hypertension, specifically, affecting over 1.1 billion people annually. The goal of the package is to provide a comprehensive toolbox for analyzing blood pressure data using a variety of statistical metrics and visualizations to bring more clarity to CVD."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#teaching",
    "href": "posts/Packages Review 2022/index.html#teaching",
    "title": "R Packages 2022 list",
    "section": "Teaching",
    "text": "Teaching\n\ntestDriveR: Teaching Data for Statistics and Data Science\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=testDriveR\ndescription from the author/vignette\n\nProvides data sets for teaching statistics and data science courses. It includes a sample of data from John Edmund Kerrich’s famous coinflip experiment. These are data that I used for teaching SOC 4015 / SOC 5050 at Saint Louis University (SLU). The package also contains an R Markdown template with the required formatting for assignments in my courses SOC 4015, SOC 4650, SOC 5050, and SOC 5650 at SLU."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#chemistry",
    "href": "posts/Packages Review 2022/index.html#chemistry",
    "title": "R Packages 2022 list",
    "section": "Chemistry",
    "text": "Chemistry\n\nstoichcalc: R Functions for Solving Stoichiometric Equations\ntags: #chemistry #stoichiometry [cran package link] https://CRAN.R-project.org/package=stoichcalc\ndescription from the author/vignette\n\nGiven a list of substance compositions, a list of substances involved in a process, and a list of constraints in addition to mass conservation of elementary constituents, the package contains functions to build the substance composition matrix, to analyze the uniqueness of process stoichiometry, and to calculate stoichiometric coefficients if process stoichiometry is unique. (See Reichert, P. and Schuwirth, N., A generic framework for deriving process stoichiometry in enviromental models, Environmental Modelling and Software 25, 1241-1251, 2010 for more details.)\n\n\n\ninters: Flexible Tools for Estimating Interactions\ntags: #statistics #interactions [cran package link] https://CRAN.R-project.org/package=inters\ndescription from the author/vignette\n\nA set of functions to estimate interactions flexibly in the face of possibly many controls. Implements the procedures described in Blackwell and Olson (2022) doi:10.1093/restud/rdt044.\n\n\n\nwaves: Vis-NIR Spectral Analysis Wrapper\ntags: #spectroscopy #preprocessing #filtering #model training\n[cran package link] https://cran.r-project.org/package=waves\ndescription from the author/vignette\n\nOriginally designed application in the context of resource-limited plant research and breeding programs, ‘waves’ provides an open-source solution to spectral data processing and model development by bringing useful packages together into a streamlined pipeline. This package is wrapper for functions related to the analysis of point visible and near-infrared reflectance measurements. It includes visualization, filtering, aggregation, preprocessing, cross-validation set formation, model training, and prediction functions to enable open-source association of spectral and reference data. This package is documented in a peer-reviewed manuscript in the Plant Phenome Journal doi:10.1002/ppj2.20012. Specialized cross-validation schemes are described in detail in Jarquín et al. (2017) doi:10.3835/plantgenome2016.12.0130. Example data is from Ikeogu et al. (2017) doi:10.1371/journal.pone.0188918.\n\n\n\nMSclassifR: Automated Classification of Mass Spectra\ntags: #Classification #Mass-Spectra\n[cran package link] https://cran.r-project.org//package=MSclassifR\ndescription from the author/vignette\n\nFunctions to classify mass spectra in known categories, and to determine discriminant mass-over-charge values. It includes easy-to-use functions for pre-processing mass spectra, functions to determine discriminant mass-over-charge values (m/z) from a library of mass spectra corresponding to different categories, and functions to predict the category (species, phenotypes, etc.) associated to a mass spectrum from a list of selected mass-over-charge values. Two vignettes illustrating how to use the functions of this package from real data sets are also available online to help users: https://agodmer.github.io/MSclassifR_examples/Vignettes/Vignettemsclassifr_Ecrobia.html and https://agodmer.github.io/MSclassifR_examples/Vignettes/Vignettemsclassifr_Klebsiella.html.\n\n\n\nNGLVieweR: load a PDB in R in order to view it\ntags: #chemistrys #visualization #molecular\n[cran package link] https://cran.r-project.org/packages=NGLVieweR\ndescription from the author/vignette\n\nProvides an ‘htmlwidgets’ https://www.htmlwidgets.org/ interface to ‘NGL.js’ http://nglviewer.org/ngl/api/. ‘NGLvieweR’ can be used to visualize and interact with protein databank (‘PDB’) and structural files in R and Shiny applications. It includes a set of API functions to manipulate the viewer after creation in Shiny."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#writing-reports-and-articles",
    "href": "posts/Packages Review 2022/index.html#writing-reports-and-articles",
    "title": "R Packages 2022 list",
    "section": "Writing reports and articles",
    "text": "Writing reports and articles\n\nutile.tools: Summarize Data for Publication\ntags: #statistic #reports #exploration\n[cran package link] [https://CRAN.R-project.org/package=utile.tools]\ndescription from the author/vignette\n\nA set of tools for preparing and summarizing data for publication purposes. Includes functions for tabulating models, means to produce human-readable summary statistics from raw data, macros for calculating duration of time, and simplistic hypothesis testing tools.\n\n\n\nrrtable: Reproducible Research with a Table of R Codes\ntags: #tables #reproducible research\n[cran package link] https://cran.r-project.org/package=rrtable\ndescription from the author/vignette\n\nMakes documents containing plots and tables from a table of R codes. Can make “HTML”, “pdf(‘LaTex’)”, “docx(‘MS Word’)” and “pptx(‘MS Powerpoint’)” documents with or without R code. In the package, modularized ‘shiny’ app codes are provided. These modules are intended for reuse across applications.\n\n\n\nreporter: Creates Statistical Reports\ntags: #statistics #report\n[cran package link] https://CRAN.R-project.org/package=reporter\ndescription from the author/vignette\n\nContains functions to create regulatory-style statistical reports. Originally designed to create tables, listings, and figures for the pharmaceutical, biotechnology, and medical device industries, these reports are generalized enough that they could be used in any industry. Generates text, rich-text, PDF, HTML, and Microsoft Word file formats. The package specializes in printing wide and long tables with automatic page wrapping and splitting. Reports can be produced with a minimum of function calls, and without relying on other table packages. The package supports titles, footnotes, page header, page footers, spanning headers, page by variables, and automatic page numbering.\n\n\n\nPDE: Extract Tables and Sentences from PDFs with User Interface\ntags: #pdf #scraping\n[cran package link] https://cran.r-project.org/packages=PDE\ndescription from the author/vignette\n\nPDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\n\nTplyr: A Grammar of Clinical Data Summary\ntags: #clinical #medical\n[cran package link] https://cran.r-project.org/package=Tplyr\ndescription from the author/vignette\n\nA tool created to simplify the data manipulation necessary to create clinical reports."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#coding",
    "href": "posts/Packages Review 2022/index.html#coding",
    "title": "R Packages 2022 list",
    "section": "Coding",
    "text": "Coding\n\nmockr: Mocking in R\ntags: #testing\n[cran package link] https://cran.r-project.org/package=mockr\ndescription from the author/vignette\n\nProvides a means to mock a package function, i.e., temporarily substitute it for testing. Designed as a drop-in replacement for the now deprecated ‘testthat::with_mock()’ and ‘testthat::local_mock()’.\n\n\n\ncOde: Automated C Code Generation for ‘deSolve’, ‘bvpSolve’\ntags: #C #Jacobians\n[cran package link] https://cran.r-project.org/package=cOde\ndescription from the author/vignette\n\nGenerates all necessary C functions allowing the user to work with the compiled-code interface of ode() and bvptwp(). The implementation supports “forcings” and “events”. Also provides functions to symbolically compute Jacobians, sensitivity equations and adjoint sensitivities being the basis for sensitivity analysis.\n\n\n\nmatlab2r: Translation Layer from MATLAB to R\ntags: #R #Matlab\n[cran package link] https://cran.r-project.org/package=matlab2r\ndescription from the author/vignette\n\nAllows users familiar with MATLAB to use MATLAB-named functions in R. Several basic MATLAB functions are written in this package to mimic the behavior of their original counterparts, with more to come as this package grows.\n\n\n\nlessR: Less Code, More Results\ntags: #coding\n[cran package link] https://cran.r-project.org//package=lessR\ndescription from the author/vignette\n\nEach function accomplishes the work of several or more standard R functions. For example, two function calls, Read() and CountAll(), read the data and generate summary statistics for all variables in the data frame, plus histograms and bar charts as appropriate. Other functions provide for descriptive statistics, a comprehensive regression analysis, analysis of variance and t-test, plotting including the introduced here Violin/Box/Scatter plot for a numerical variable, bar chart, histogram, box plot, density curves, calibrated power curve, reading multiple data formats with the same function call, variable labels, color themes, Trellis graphics and a built-in help system. Also includes a confirmatory factor analysis of multiple indicator measurement models, pedagogical routines for data simulation such as for the Central Limit Theorem, and generation and rendering of R markdown instructions for interpretative output.\n\n\n\nGroundhog: Addressing The Threat That R Poses To Reproducible Research\ntags: #reproducibility\n[cran package link] https://cran.r-project.org/package=groundhog\ndescription from the author/vignette\n\nMake R scripts that rely on packages reproducible, by ensuring that every time a given script is run, the same version of the used packages are loaded (instead of whichever version the user running the script happens to have installed). This is achieved by using the new command groundhog.library() instead of the base command library(), and including a date in the call. The date is used to call on the same version of the package every time (the most recent version available on CRAN at that date)."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#graphics",
    "href": "posts/Packages Review 2022/index.html#graphics",
    "title": "R Packages 2022 list",
    "section": "Graphics",
    "text": "Graphics\n\nraymolecule: Parse and Render Molecular Structures in 3D\ntags: #chemistry #rendering\n[cran package link] https://cran.r-project.org/package=raymolecule\ndescription from the author/vignette\n\nDownloads and parses ‘SDF’ (Structural Description Format) and ‘PDB’ (Protein Database) files for 3D rendering."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#regression",
    "href": "posts/Packages Review 2022/index.html#regression",
    "title": "R Packages 2022 list",
    "section": "Regression",
    "text": "Regression\n\nspeedglm: Fitting Linear and Generalized Linear Models to Large Data Sets\ntags: #fitting #GLM\n[cran package link] https://cran.r-project.org//package=speedglm\ndescription from the author/vignette\n\nFitting linear models and generalized linear models to large data sets by updating algorithms.\n\n\n\nmodelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready\ntags: #models\n[cran package link] https://cran.r-project.org/package=modelsummary\ndescription from the author/vignette\n\nCreate beautiful and customizable tables to summarize several statistical models side-by-side. Draw coefficient plots, multi-level cross-tabs, dataset summaries, balance tables (a.k.a. “Table 1s”), and correlation matrices. This package supports dozens of statistical models, and it can produce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint, Excel, RTF, JPG, or PNG. Tables can easily be embedded in ‘Rmarkdown’ or ‘knitr’ dynamic documents."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#signal-processing",
    "href": "posts/Packages Review 2022/index.html#signal-processing",
    "title": "R Packages 2022 list",
    "section": "Signal Processing",
    "text": "Signal Processing\n\ngsignal: Signal Processing in R\ntags: #signal processing #filters #boxcar\n[cran package link] https://cran.r-project.org/packages=gsignal\ndescription from the author/vignette\n\nR implementation of the ‘Octave’ package ‘signal’, containing a variety of signal processing tools, such as signal generation and measurement, correlation and convolution, filtering, filter design, filter analysis and conversion, power spectrum analysis, system identification, decimation and sample rate change, and windowing."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#data-analysis",
    "href": "posts/Packages Review 2022/index.html#data-analysis",
    "title": "R Packages 2022 list",
    "section": "Data analysis",
    "text": "Data analysis\n\nsplitTools: Tools for Data Splitting\ntags: #data splitting\n[cran package link] https://cran.r-project.org/package=splitTools\ndescription from the author/vignette\n\nFast, lightweight toolkit for data splitting. Data sets can be partitioned into disjoint groups (e.g. into training, validation, and test) or into (repeated) k-folds for subsequent cross-validation. Besides basic splits, the package supports stratified, grouped as well as blocked splitting. Furthermore, cross-validation folds for time series data can be created. See e.g. Hastie et al. (2001) doi:10.1007/978-0-387-84858-7 for the basic background on data partitioning and cross-validation.\n\n\n\noptedr: Calculating Optimal and D-Augmented Designs\ntags: #DoE #Chemometrics #optimal-design\n[cran package link] https://cran.r-project.org//package=optedr\ndescription from the author/vignette\n\nCalculates D-, Ds-, A- and I-optimal designs for non-linear models, via an implementation of the cocktail algorithm (Yu, 2011, doi:10.1007/s11222-010-9183-2). Compares designs via their efficiency, and D-augments any design with a controlled efficiency. An efficient rounding function has been provided to transform approximate designs to exact designs. mynotes\n\n\n\nReDaMoR: Relational Data Modeler\ntags: #database #relational #data\n[cran package link] https://cran.r-project.org/package=ReDaMoR\ndescription from the author/vignette\n\nThe aim of this package is to manipulate relational data models in R. It provides functions to create, modify and export data models in json format. It also allows importing models created with ‘MySQL Workbench’ (https://www.mysql.com/products/workbench/). These functions are accessible through a graphical user interface made with ‘shiny’. Constraints such as types, keys, uniqueness and mandatory fields are automatically checked and corrected when editing a model. Finally, real data can be confronted to a model to check their compatibility.\n\n\n\nexplore: Simplifies Exploratory Data Analysis\ntags: #graphs #visualization\n[cran package link] https://cran.r-project.org/package=gridpattern\ndescription from the author/vignette\n\nInteractive data exploration with one line of code or use an easy to remember set of tidy functions for exploratory data analysis. Introduces three main verbs. explore() to graphically explore a variable or table, describe() to describe a variable or table and report() to create an automated report.\n\n\n\nesquisse: Explore and Visualize Your Data Interactivelly\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/package=esquisse\ndescription from the author/vignette\n\nA ‘shiny’ gadget to create ‘ggplot2’ figures interactively with drag-and-drop to map your variables to different aesthetics. You can quickly visualize your data accordingly to their type, export in various formats, and retrieve the code to reproduce the plot.\n\n\n\nplfMA: A GUI to View, Design and Export Various Graphs of Data\ntags: #visualization #GUI\n[cran package link] http://cran.stat.unipd.it/package=plfMA\ndescription from the author/vignette\n\nProvides a graphical user interface for viewing and designing various types of graphs of the data. The graphs can be saved in different formats of an image.\n\n\n\ndatasets.load: Interfaces for Loading Datasets\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/packages/datasets.load/index.html\ndescription from the author/vignette\n\nVisual interface for loading datasets in RStudio from all installed (including unloaded) packages, also includes command line interfaces.\n\n\n\nloon.shiny: Automatically Create a ‘Shiny’ App Based on Interactive ‘Loon’ Widgets\ntags: #data analysis\n[cran package link] https://cran.r-project.org/package=loon.shiny\ndescription from the author/vignette\n\nPackage ‘shiny’ provides interactive web applications in R. Package ‘loon’ is an interactive toolkit engaged in open-ended, creative and unscripted data exploration. The ‘loon.shiny’ package can take ‘loon’ widgets and display a selfsame ‘shiny’ app.\n\n\n\nloon: Interactive Statistical Data Visualization\ntags: #plot #data analysis\n[cran package link] https://cran.r-project.org//package=loon\ndescription from the author/vignette\n\nAn extendable toolkit for interactive data visualization and exploration.\n\n\n\nrio: A Swiss-Army Knife for Data I/O\ntags: #data input\n[cran package link] https://cran.r-project.org/package=rio\ndescription from the author/vignette\n\nStreamlined data import and export by making assumptions that the user is probably willing to make: ‘import()’ and ‘export()’ determine the data structure from the file extension, reasonable defaults are used for data import and export (e.g., ‘stringsAsFactors=FALSE’), web-based import is natively supported (including from SSL/HTTPS), compressed files can be read directly without explicit decompression, and fast import packages are used where appropriate. An additional convenience function, ‘convert()’, provides a simple method for converting between file types.\n\n\n\ntabxplor: User-Friendly Tables with Color Helpers for Data Exploration\ntags: #plots #tables\n[cran package link] https://cran.r-project.org/package=tabxplor\ndescription from the author/vignette\n\nMake it easy to deal with multiple cross-tables in data exploration, by creating them, manipulating them, and adding color helpers to highlight important informations. All functions are “tidy”, pipe-friendly, and render data frames which can be easily manipulated. Tables can be exported to Excel and in html with formats and colors.\n\n\n\ngroupdata2: Creating Groups from Data\ntags: #tables #data\n[cran package link] https://cran.r-project.org//package=groupdata2\ndescription from the author/vignette\n\nhods for dividing data into groups. Create balanced partitions and cross-validation folds. Perform time series windowing and general grouping and splitting of data. Balance existing groups with up- and downsampling or collapse them to fewer groups.\n\n\n\nconjurer: A Parametric Method for Generating Synthetic Data\ntags: #plot #colors\n[cran package link] https://cran.r-project.org//package=conjurer\ndescription from the author/vignette\n\nBuilds synthetic data applicable across multiple domains. This package also provides flexibility to control data distribution to make it relevant to many industry examples\n\n\n\nowidR: A Package for Importing Data from Our World in Data\ntags: #data #statistics\n[cran package link] https://cran.r-project.org/package=owidR\ndescription from the author/vignette\n\nScrapes data from the Our World in Data website to offer easy to use functions for searching for datasets and downloading them into R.\n\n\n\ntidycharts: Generate Tidy Charts Inspired by ‘IBCS’\ntags: #plots\n[cran package link] https://cran.r-project.org/package=tidycharts\ndescription from the author/vignette\n\nThere is a wide range of R packages created for data visualization, but still, there was no simple and easily accessible way to create clean and transparent charts - up to now. The ‘tidycharts’ package enables the user to generate charts compliant with International Business Communication Standards (‘IBCS’). It means unified bar widths, colors, chart sizes, etc. Creating homogeneous reports has never been that easy! Additionally, users can apply semantic notation to indicate different data scenarios (plan, budget, forecast). What’s more, it is possible to customize the charts by creating a personal color pallet with the possibility of switching to default options after the experiments. We wanted the package to be helpful in writing reports, so we also made joining charts in a one, clear image possible. All charts are generated in SVG format and can be shown in the ‘RStudio’ viewer pane or exported to HTML output of ‘knitr’/‘markdown’."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#extra",
    "href": "posts/Packages Review 2022/index.html#extra",
    "title": "R Packages 2022 list",
    "section": "EXTRA",
    "text": "EXTRA\n\ntidydice: simulates rolling a dice and flipping a coin\ntags: #teaching #fun\n[cran package link] https://cran.r-project.org//package=tidydice\ndescription from the author/vignette\n\nThis package simulates rolling a dice and flipping a coin. Each experiment generates a tibble. Dice rolls and coin flips are simulated using sample(). The properties of the dice can be changed, like the number of sides. A coin flip is simulated using a two sided dice. Experiments can be combined with the pipe-operator.\n\n\n\ntiling:Polygon Tiling Examples\ntags: #arts #fun\n[cran package link] https://cran.rstudio.com/web/package=gridpattern\ndescription from the author/vignette\n\nSeveral uniform regular polygon tiling patterns can be achieved by use of grid.pattern_regular_polygon() plus occasionally grid.polygon() to set a background color. This vignette highlights several such tiling patterns plus a couple notable non-uniform tiling patterns.\n\n\n\nlingtypology: Linguistic Typology and Mapping\ntags: #linguistic mapping\n[cran package link] https://cran.r-project.org/package=lingtypology\ndescription from the author/vignette\n\nProvides R with the Glottolog database https://glottolog.org/ and some more abilities for purposes of linguistic mapping. The Glottolog database contains the catalogue of languages of the world. This package helps researchers to make a linguistic maps, using philosophy of the Cross-Linguistic Linked Data project https://clld.org/, which allows for while at the same time facilitating uniform access to the data across publications. A tutorial for this package is available on GitHub pages https://docs.ropensci.org/lingtypology/ and package vignette. Maps created by this package can be used both for the investigation and linguistic teaching. In addition, package provides an ability to download data from typological databases such as WALS, AUTOTYP and some others and to create your own database website."
  },
  {
    "objectID": "posts/Tip05/index.html",
    "href": "posts/Tip05/index.html",
    "title": "Tip 4: boxplots and scatterplots: simple recipes",
    "section": "",
    "text": "Summary\n\nSimulate data, check and assign data types\nCreate a scatterplot with ggplot\nCreate violin plot with ggstatsplot\n\nExample 1: We want to visualize the difference between two groups of patients that follow two different diets. Group A has an average of total cholesterol of 180 with a standard deviation of 20 while Group B and average of 200 with a standard deviation of 40\n\nlibrary(MASS)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.2.3\n\nnpatientsA <- 500\nnpatientsB <- 520\ncholA <- mvrnorm(n=npatientsA, mu=180, Sigma=20, empirical=T)\ncholB <- mvrnorm(n=npatientsB, mu=200, Sigma=40, empirical=T)\n\ndataA <- cbind(cholA,rep(\"A\",npatientsA))  \ndataB <- cbind(cholB,rep(\"B\",npatientsB))  \n\ndata <- data.frame(rbind(dataA,dataB))\ncolnames(data) <- c(\"Cholesterol\",\"group\")\ndata$Cholesterol <- as.numeric(data$Cholesterol)\n\np1 <-ggplot(data, aes(x = group, y = Cholesterol)) + geom_jitter(alpha=0.05) \n\np1\n\n\n\n\nA few observations on the code. First of all, we need to input the data in a data.frame otherwise ggplot will give us an error. The second observation is that since we put chr labels on our groups we needed to define Cholesterol as.numeric in order to avoid unwanted resultsstrange results. Try to comment the line data$Cholesterol <- as.numeric(data$Cholesterol) and you can see by yourself what will happen. (hint: a “labelstorm!”)\nJiiter plots is one of my favorite way to represent data. data and immediately understand the distribution of your data and also avoid the pitfall of boxplot (see (Matejka and Fitzmaurice 2017))\nIf you need inferential statistics on your data another resource is (Patil 2021). See the following example with our data. NOTE that we nee to transform the group label as.factor\n\nlibrary(ggstatsplot)\n\nWarning: package 'ggstatsplot' was built under R version 4.2.3\n\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\ndata$group <- as.factor(data$group)\n\npstack  <- ggbetweenstats(data,group,Cholesterol)\n                          \npstack    \n\n\n\n\n\n\n\n\nReferences\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/3025453.3025912.\n\n\nPatil, Indrajeet. 2021. “Visualizations with Statistical Details: The ’Ggstatsplot’ Approach” 6: 3167. https://doi.org/10.21105/joss.03167."
  },
  {
    "objectID": "listing.html",
    "href": "listing.html",
    "title": "Giorgio Luciano",
    "section": "",
    "text": "A Mathematical Exploration of Pizza Sizes\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem with Dice Rolls\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Card Shuffling: A Visual Journey\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Coin Flip Sequences with Simulation in R\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Frequency of Number 0 in a Year of Roulette Spins: A Simulation in R\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonty Hall simulation\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNamespaces in shiny: Why you need them\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Packages 2022 list\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Packages 2024 list so far\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #1: ggplot loops\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #2: Cleaning column names of an imported csv\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #3: Functions for simulating data\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #4: boxplots and scatterplots: simple recipes\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Birthday Paradox: When Probability Plays Tricks\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Illusion of Luck: How the Number Zero Always Wins in the Casino\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling the DnD Dice Duel Riddle with Monte Carlo Simulation in R\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Art1/Visualization.html",
    "href": "posts/Art1/Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is the crucial first step in the data analysis process. Before applying complex statistical models or machine learning algorithms, it is essential to understand the structure, trends, and peculiarities of the data you are working with. In this introductory section, we will explore the fundamental concepts of EDA and its role in data analysis.\n\n\nEDA serves several purposes:\n\nUnderstanding Data: EDA helps us become familiar with the dataset, identify the available variables, and understand their nature (numeric, categorical, temporal, etc.).\nDetecting Patterns: EDA allows us to detect patterns, relationships, and potential outliers within the data. This is critical for making informed decisions during the analysis.\nData Cleaning: Through EDA, we can identify missing values, outliers, or data inconsistencies that require cleaning and preprocessing.\nFeature Engineering: EDA may suggest feature engineering opportunities, such as creating new variables or transforming existing ones to better represent the underlying data.\nHypothesis Generation: EDA often leads to the generation of hypotheses or research questions, guiding further investigation.\nCommunicating Insights: EDA produces visualizations and summaries that facilitate the communication of insights to stakeholders or team members.\n\nIn the following sections, we will delve into the practical aspects of EDA, starting with data simulation and visualization techniques.\n\n\n\nBefore diving into Exploratory Data Analysis (EDA) on real datasets, it’s helpful to begin with the generation of simulated data. This allows us to have full control over the data and create example scenarios to understand key EDA concepts. In this section, we will learn how to generate simulated datasets using R.\n\n\nTo start, let’s define some basic parameters that we’ll use to generate simulated data:\n\nx_min: The minimum value for the variable x.\nx_max: The maximum value for the variable x.\nx_step: The increment between successive x values.\ny_mean: The mean value for the dependent variable y.\ny_sd: The standard deviation for the dependent variable y.\ny_min: The minimum possible value for y.\ny_max: The maximum possible value for y.\n\nWe will use these parameters to generate sample data.\nNow, let’s proceed to generate sample data based on the defined parameters. In this example, we’ll create a simple dataset with the following variables:\n\nx: A sequence of values ranging from x_min to x_max with an increment of x_step.\nvar_random: A random variable with values uniformly distributed between y_min and y_max.\nvar_norm: A variable with values generated from a normal distribution with mean y_mean and standard deviation y_sd.\nvar_sin: A variable with values generated as the sine function of x.\n\nHere’s the R code to create the sample dataset:\n\nlibrary(data.table)\n\n# Parameters\nx_min   <- 0\nx_max   <- 10   \nx_step  <- 0.01\n\ny_mean  <- 0.5\ny_sd    <- 0.25\ny_min   <- -1\ny_max   <- 1     \n\nx       <- seq(x_min, x_max, x_step)\n\n# Variables\nvar_random  <- runif(length(x), y_min, y_max)\nvar_norm    <- rnorm(length(x), y_mean, y_sd) \nvar_sin     <- sin(x)\n\n# Data.frame \ndf  <- data.frame(x, var_random, var_norm, var_sin)\ndt  <- data.table(df)\n\n# Melt \ndtm <- melt(dt, id.vars=\"x\")\n\nThis code creates a dataset df and a data.table dt containing the generated variables. The melt function from the data.table library is used to reshape the data for visualization purposes.\nWith our simulated data ready, we can now move on to creating various plots and performing EDA.\nIn this section, we will explore various visualization techniques that play a crucial role in Exploratory Data Analysis (EDA). Visualizations help us gain insights into the data’s distribution, patterns, and relationships between variables. We will use the simulated dataset generated in the previous section to illustrate these techniques.\n\n\n\n\nThe choice of visualization depends on the nature of your data and the specific aspects you want to highlight. Generally, in EDA, we often need to:\n\nExamine Changes Over Time: Use time series plots when you want to assess changes in one or more variables over time.\nCheck for Data Distribution: Create distribution plots, such as histograms and density plots, to understand how data points are distributed.\nExplore Variable Relationships: Employ correlation plots and scatter plots to identify linear relationships between variables.\n\nLet’s start by examining these aspects one by one using our simulated dataset.\n\n\nTo explore changes over time, we’ll create a time series plot for the var_sin variable. This variable represents a sine wave and is well-suited for a time series representation. Here’s the R code to create a time series plot:\n\nlibrary(ggplot2)\n\np <- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\nIn this code, we use ggplot2 to create a line plot for the var_sin variable.\n\n\n\nTo check the data distribution, we’ll create histogram plots for each of the variables: var_random, var_norm, and var_sin. Histograms provide a visual representation of the frequency distribution of data values. Here’s the R code:\n\np3 <- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 <- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\np4\n\n\n\np5\n\n\n\n\nThese plots will help us understand the distribution characteristics of our variables.\n\n\n\nCorrelation plots allow us to examine relationships between variables. We’ll create scatter plots for pairs of variables to assess their linear correlation. Here’s an example for var_sin and var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  <- runif(x,y_min,y_max)\nvar_norm2    <- rnorm(x,y_mean,y_sd) \nvar_sin2     <- sin(x) + rnorm(x,0,0.01) \n\ndf2<- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 <- data.table(df2)\n\np10 <- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\nThese scatter plots help us identify whether variables exhibit linear correlation.\nIn the following sections, we’ll delve deeper into each of these plot types, interpret the results, and explore additional visualization techniques for EDA.\n\n\nBox plots, also known as box-and-whisker plots, provide a summary of the data’s distribution, including median, quartiles, and potential outliers. They are particularly useful for comparing the distributions of different variables or groups. Here’s an example of creating box plots for var_random, var_norm, and var_sin:\n\np6 <- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\nBox plots can reveal variations and central tendencies of the variables.\n\n\n\nPair plots, or scatterplot matrices, allow us to visualize pairwise relationships between multiple variables in a dataset. They are helpful for identifying correlations and patterns among variables simultaneously. Here’s how to create a pair plot for our dataset:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot <- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\n\n\n\nPair plots provide a comprehensive view of variable interactions.\n\n\n\nTime series data often contain underlying components such as trends and seasonality that can be crucial for understanding the data’s behavior. Time series decomposition is a technique used in Exploratory Data Analysis (EDA) to separate these components. In this section, we’ll demonstrate how to perform time series decomposition using our simulated var_sin data.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp <- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\nThe code above performs the following:\n\nDecomposes the var_sin time series using the decompose function. We specify a frequency of 365 since the data represents daily observations.\nPlots the decomposed components, including the original time series, trend, seasonal component, and residual.\n\nThe resulting plot will show the individual components of the time series, allowing us to gain insights into its underlying patterns.\n\n\n\nInteractive plots, created using libraries like plotly or shiny, allow users to explore data interactively. You can create interactive scatter plots, line plots, or heatmaps, enhancing the user’s ability to dig deeper into the data.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot <- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"<br>var_random:\", var_random, \"<br>var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %>%\n  add_markers() %>%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn this initial section, we’ve introduced the fundamental concepts of exploratory data analysis (EDA) and the importance of data visualization in gaining insights from complex datasets. We’ve explored various types of plots and their applications in EDA.\nNow, let’s dive deeper and enhance our understanding by demonstrating practical examples of EDA using real-world datasets. We’ll showcase how different types of plots and interactive visualizations can provide valuable insights and drive data-driven decisions.\nLet’s embark on this EDA journey and uncover the hidden stories within our data through hands-on examples.\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll dive into a meaningful example of time series decomposition to demonstrate its practical utility in Exploratory Data Analysis (EDA). Time series decomposition allows us to extract valuable insights from time-dependent data. We’ll use our simulated var_sin time series to illustrate its significance.\n\n\n\nImagine we have daily temperature data for a city over several years. We want to understand the underlying patterns in temperature variations, including trends and seasonality, to make informed decisions related to weather forecasts, climate monitoring, or energy management.\nLet’s create the enhanced dataset with temperature data for multiple cities. We’ll use the data.table library to manage the dataset efficiently:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities <- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date <- as.Date(\"2010-01-01\")\nend_date <- as.Date(\"2019-12-31\")\ndate_seq <- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data <- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature <- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp <- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\nWe’ve generated temperature data for each city over the span of ten years, resulting in a diverse and complex dataset.\n\n\n\nNow that we have our multi-city temperature dataset, let’s apply time series decomposition to analyze temperature trends and seasonality for one of the cities, such as New York (see plot)\n\n\n\nThe plot will display the components of the time series for New York, including the original time series, trend, seasonal component, and residual. Similar analyses can be performed for other cities to identify regional temperature patterns.\n\n\n\nWith our enhanced multi-city temperature dataset and time series decomposition, we can:\n\nRegional Analysis: Compare temperature patterns across different cities to identify regional variations.\nSeasonal Insights: Understand how temperature seasonality differs between cities and regions.\nLong-Term Trends: Analyze temperature trends for each city over the ten-year period.\n\nThis advanced analysis helps us make informed decisions related to climate monitoring, urban planning, and resource management.\n\n\n\n\n\n\nIn this section, we’ll illustrate the significance of distribution plots in Exploratory Data Analysis (EDA) by considering a practical scenario. Distribution plots help us understand how data points are distributed and can reveal insights about the underlying data characteristics. We’ll use our simulated dataset and focus on the var_random variable.\n\n\n\nImagine we have a dataset containing exam scores of students in a class. We want to gain insights into the distribution of exam scores to answer questions like:\n\nWhat is the typical exam score?\nAre the exam scores normally distributed?\nAre there any outliers or unusual patterns in the scores?\n\n\n\n\nLet’s create a histogram to visualize the distribution of exam scores using the var_random variable. This will help us answer the questions posed above.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\n\nThe resulting histogram will display the distribution of exam scores. Here’s what we can interpret:\n\nTypical Exam Score: The histogram will show where the majority of exam scores lie, indicating the typical or central value.\nDistribution Shape: We can assess whether the scores follow a normal distribution, are skewed, or have other unique characteristics.\nOutliers: Outliers, if present, will appear as data points far from the central part of the distribution.\n\n\n\n\nBy analyzing the distribution of exam scores, we can:\n\nIdentify Central Tendency: Determine the typical exam score, which can be useful for setting benchmarks or evaluating student performance.\nUnderstand Data Characteristics: Gain insights into the shape of the distribution, which informs us about the data’s characteristics.\nDetect Outliers: Identify outliers or unusual scores that may require further investigation.\n\n\n\n\n\nIn this section, we’ll explore advanced correlation analysis using more complex datasets. We’ll create two datasets: one representing students’ academic performance, and the other containing information about their study habits and extracurricular activities. We’ll investigate correlations between various factors to gain deeper insights.\n\n\nLet’s create the two complex datasets for our correlation analysis:\nAcademic Performance Dataset:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students <- 500\n\nacademic_data <- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data <- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\n\nNow that we have our complex datasets, let’s perform advanced correlation analysis to explore relationships between academic performance, study habits, and extracurricular activities. We’ll calculate correlations and visualize them using a heatmap:\n\n# Calculate correlations between variables\ncorrelation_matrix <- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\n\nThe resulting heatmap visually represents the correlations between academic performance and study-related factors. Here’s what we can interpret:\n\nColor Intensity: The color intensity indicates the strength and direction of the correlation. Positive correlations are shown in blue, while negative correlations are in red. The darker the color, the stronger the correlation.\nCorrelation Coefficients: The heatmap displays the actual correlation coefficients as labels in the lower triangle. These values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n\n\n\n\nBy conducting advanced correlation analysis, we can:\n\nUnderstand Complex Relationships: Explore intricate correlations between academic performance, study hours, extracurricular activities, and stress levels.\nIdentify Key Factors: Determine which factors have the most significant impact on academic performance.\nOptimize Student Support: Use insights to provide targeted support and interventions for students.\n\nAdvanced correlation analysis helps us uncover nuanced relationships within complex datasets.\n\n\n\n\nIn this section, we’ll explore the versatility of box plots by working with diverse and complex datasets. We’ll create two datasets: one representing the distribution of monthly sales for multiple product categories, and the other containing information about customer demographics. These datasets will allow us to visualize various types of distributions and identify outliers.\n\n\nLet’s create the two complex datasets for our box plot analysis:\nSales Dataset and Customer Demographics Dataset:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months <- 24\nproduct_categories <- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data <- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers <- 300\n\ndemographics_data <- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 <- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 <- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\n\nThese box plots help us gain insights into diverse distributions:\n\nSales Distribution: We can observe how sales are distributed across different product categories, identifying variations and potential outliers.\nCustomer Age Distribution: The box plot displays the spread of customer ages, highlighting the central tendency and any potential outliers.\n\n\n\n\nBy using box plots with complex datasets, we can:\n\nAnalyze Diverse Distributions: Visualize and compare distributions of sales for multiple product categories and customer age distributions.\nOutlier Detection: Identify potential outliers in both sales data and customer demographics.\nSegmentation Insights: Understand how sales vary across product categories and the age distribution of customers.\n\nBox plots are versatile tools for exploring various types of data distributions and making data-driven decisions.\n\n\n\n\n\n\nSuppose we have a dataset containing monthly stock prices for three companies: Company A, Company B, and Company C. We want to create an interactive time series plot that allows users to:\n\nSelect the company they want to visualize.\nZoom in and out to explore specific time periods.\nHover over data points to view detailed information.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months <- 24\n\ntime_series_data <- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot <- plot_ly(data = time_series_data, x = ~Date) %>%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %>%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\nInteractive data visualizations are a powerful tool for EDA and data presentation. In the next section, we’ll explore another advanced visualization technique: time series decomposition.\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics."
  },
  {
    "objectID": "posts/Art2/index.html",
    "href": "posts/Art2/index.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Exploring Anscombe’s Quartet with R, ggplot2, and Custom Functions\nAnscombe’s Quartet, known as the “Anscombe’s Test,” consists of four datasets with very similar descriptive statistics but visually distinct characteristics. These quartets serve as an enlightening example of the importance of visualizing data before drawing conclusions.\nIn this post, we will delve into how to calculate and visualize Anscombe’s Quartet using R and the powerful ggplot2 library. We’ll also use custom functions to generate these quartets and analyze them.\n\n\nIntroduction\nAnscombe’s Quartet was created by the statistician Francis Anscombe in 1973 to underscore the importance of data visualization before analysis. Despite having similar statistics, these datasets exhibit significantly different visual behaviors. Let’s see how R and ggplot2 help us explore them.\n\n\nSetting Up the Environment\nTo get started, we need to load some libraries:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\nUnderstanding Anscombe’s Quartet\nAnscombe’s Quartet comprises four datasets, each with 11 data points. Here’s a brief overview of the quartet:\n\nDataset 1: A straightforward linear relationship between X and Y.\nDataset 2: A linear relationship with an outlier.\nDataset 3: A linear relationship with one point substantially different from the others.\nDataset 4: A non-linear relationship.\n\n(see Rpubs page)\n\nlibrary(datasets)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nCaricamento pacchetto: 'dplyr'\n\n\nI seguenti oggetti sono mascherati da 'package:data.table':\n\n    between, first, last\n\n\nI seguenti oggetti sono mascherati da 'package:stats':\n\n    filter, lag\n\n\nI seguenti oggetti sono mascherati da 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\ndatasets::anscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nanscombe_tidy &lt;- anscombe %&gt;%\n    mutate(observation = seq_len(n())) %&gt;%\n    gather(key, value, -observation) %&gt;%\n    separate(key, c(\"variable\", \"set\"), 1, convert = TRUE) %&gt;%\n    mutate(set = c(\"I\", \"II\", \"III\", \"IV\")[set]) %&gt;%\n    spread(variable, value)\n\nhead(anscombe_tidy)\n\n  observation set  x    y\n1           1   I 10 8.04\n2           1  II 10 9.14\n3           1 III 10 7.46\n4           1  IV  8 6.58\n5           2   I  8 6.95\n6           2  II  8 8.14\n\nggplot(anscombe_tidy, aes(x, y)) +\n    geom_point() +\n    facet_wrap(~ set) +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nVisualizing the Quartet\nNow, let’s integrate your custom R code and examples to generate and visualize Anscombe’s Quartet:\n\nlibrary(vtable)\n\nCaricamento del pacchetto richiesto: kableExtra\n\n\n\nCaricamento pacchetto: 'kableExtra'\n\n\nIl seguente oggetto è mascherato da 'package:dplyr':\n\n    group_rows\n\nlibrary(kableExtra)\nlibrary(patchwork)\n\n\n# Note: Function to generate Anscombe's Quartet datasets for x2 we need a trick that can be also improved but for now as a brutal approx works\n\n\nplotreg &lt;- function(df) {\n  formula &lt;- y ~ x\n  \n  ggplot(df, aes(x = x, y = y)) +\n    geom_point(aes(size = 1), alpha = 0.3) +\n    geom_smooth(method = \"lm\", formula = formula, se = TRUE) +\n    coord_cartesian(xlim = c(4, 19), ylim = c(4, 14)) +  # Imposta i limiti di x e y\n    theme_light(base_size = 10) +\n    theme(legend.position = \"none\")\n}\n\ngenerate_noisy_points &lt;- function(x, y, noise_level = 0.1) {\n  n &lt;- length(x)\n  \n  # Generate random noise for x and y separately\n  noise_x &lt;- rnorm(n, mean = 0, sd = noise_level)\n  noise_y &lt;- rnorm(n, mean = 0, sd = noise_level)\n  \n  # Ensure that the sum of noise on x and y is approximately zero\n  noise_x &lt;- noise_x - mean(noise_x)\n  noise_y &lt;- noise_y - mean(noise_y)\n  \n  # Add noise to the original data\n  x_noisy &lt;- x + noise_x\n  y_noisy &lt;- y + noise_y\n  \n  return(data.frame(x = x_noisy, y = y_noisy))\n}\n\n# Function to generate approximated points with an option to add noise\ngenerate_approximated_points &lt;- function(n, x, y, noise_level = 0) {\n  # Create a new interpolation based on the original data\n  interpolated_values &lt;- approx(x, y, xout = seq(min(x), max(x), length.out = n))\n  \n  # Extract the interpolated points\n  x_interp &lt;- interpolated_values$x\n  y_interp &lt;- interpolated_values$y\n  \n  # Add noise if needed\n  if (noise_level &gt; 0) {\n    noise &lt;- rnorm(n, mean = 0, sd = noise_level)\n    x_interp &lt;- x_interp + noise\n    y_interp &lt;- y_interp + noise\n  }\n  \n  # Return the approximated points\n  return(data.frame(x = x_interp, y = y_interp))\n}\n\n\nmultians &lt;- function(npoints = 11, anscombe) {\n  x1 &lt;- anscombe$x1\n  x2 &lt;- anscombe$x2\n  x3 &lt;- anscombe$x3\n  x4 &lt;- anscombe$x4\n  y1 &lt;- anscombe$y1\n  y2 &lt;- anscombe$y2\n  y3 &lt;- anscombe$y3\n  y4 &lt;- anscombe$y4\n\n  ## Generate Quartet 1 ##\n  x_selected &lt;- c(x1[2], x1[4], x1[11])\n  y_selected &lt;- c(y1[2], y1[4], y1[11])\n\n  # Calculate the linear regression\n  linear_model &lt;- lm(y_selected ~ x_selected)\n\n  # Extract coefficients of the line\n  intercept &lt;- coef(linear_model)[1]\n  slope &lt;- coef(linear_model)[2]\n\n  # Create a sinusoidal curve above or below the line\n  x_sin &lt;- seq(min(x1), max(x1), length.out = npoints)  # x range for the sinusoid\n  amplitude &lt;- 1  # Amplitude of the sinusoid\n  frequency &lt;- 4  # Frequency of the sinusoid\n  phase &lt;- pi / 2  # Phase of the sinusoid (for rotation)\n  sinusoid &lt;- amplitude * sin(2 * pi * frequency * (x_sin - min(x1)) / (max(x1) - min(x1)) + phase)\n\n  # Generate points above or below the line\n  y_sin &lt;- slope * x_sin + intercept + sinusoid\n  df1 &lt;- data.frame(x = x_sin, y = y_sin)\n\n  ## Generate Quartet 2 ##\n  n_points_approximated &lt;- npoints\n  noise_level &lt;- 0.1\n\n  # Generate approximated points\n  approximated_points &lt;- generate_approximated_points(n_points_approximated, x2, y2, noise_level = 0.1)\n\n  # Add noise to the approximated points\n  noisy_approximated_points &lt;- generate_noisy_points(approximated_points$x, approximated_points$y, noise_level)\n\n  # Now, you have noisy approximated points in df2\n  df2 &lt;- data.frame(x = noisy_approximated_points$x, y = noisy_approximated_points$y)\n\n  ## Generate Quartet 3 ##\n  lm_model &lt;- lm(y3 ~ x3, subset = -c(3))\n  x_generated &lt;- seq(min(x3), max(x3), length.out = npoints)\n  y_generated &lt;- predict(lm_model, newdata = data.frame(x3 = x_generated))\n\n  x_outlier &lt;- 13\n  y_outlier &lt;- 12.74\n  x_generated &lt;- c(x_generated, x_outlier)\n  y_generated &lt;- c(y_generated, y_outlier)\n\n  df3 &lt;- data.frame(x = x_generated, y = y_generated)\n\n  ## Generate Quartet 4 ##\n  \n  y4[9]\n  x &lt;- c(rep(min(x4),npoints))\n  y &lt;- c(seq(min(y4[-8]), max(y4[-8]), length.out = (npoints)))\n  x_new = c(x,x4[8])\n  y_new = c(y,y4[8])\n  df4 &lt;- data.frame(x = x_new, y = y_new)\n\n  return(list(df1 = df1, df2 = df2, df3 = df3, df4 = df4))\n}\n\n\n\n# Generate and plot Quartet 1\nt1 &lt;- multians(33,anscombe)\n\np1 &lt;- plotreg(t1$df1)\np3 &lt;- plotreg(t1$df3)\np4 &lt;- plotreg(t1$df4)\np2 &lt;- plotreg(t1$df2)\n\n(p1 | p2) / (p3 | p4)\n\n\n\n# Example of eight summaries (replace them with your own)\nsummary1 &lt;- st(t1$df1)\nsummary5 &lt;- st(data.frame(anscombe$x1,anscombe$y1))\n\nsummary2 &lt;- st(t1$df2)\nsummary6 &lt;- st(data.frame(anscombe$x2,anscombe$y2))\n\nsummary3 &lt;- st(t1$df3)\nsummary7 &lt;- st(data.frame(anscombe$x3,anscombe$y3))\n\nsummary4 &lt;- st(t1$df4)\nsummary8 &lt;- st(data.frame(anscombe$x4,anscombe$y4))\n\n\nsummary1 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n33\n9\n3\n4\n6.5\n12\n14\n\n\ny\n33\n8.3\n2.2\n4.7\n6.5\n10\n13\n\n\n\n\n\n\nsummary5 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x1\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y1\n11\n7.5\n2\n4.3\n6.3\n8.6\n11\n\n\n\n\n\n\nsummary2 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n33\n9\n3\n3.9\n6.5\n12\n14\n\n\ny\n33\n7.6\n1.8\n3\n6.7\n9.1\n9.4\n\n\n\n\n\n\nsummary6 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x2\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y2\n11\n7.5\n2\n3.1\n6.7\n8.9\n9.3\n\n\n\n\n\n\nsummary3\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n34\n9.1\n3.1\n4\n6.6\n12\n14\n\n\ny\n34\n7.3\n1.4\n5.4\n6.3\n8.1\n13\n\n\n\n\n\n\nsummary7 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x3\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y3\n11\n7.5\n2\n5.4\n6.2\n8\n13\n\n\n\n\n\n\nsummary4 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n34\n8.3\n1.9\n8\n8\n8\n19\n\n\ny\n34\n7.2\n1.4\n5.2\n6.2\n8\n12\n\n\n\n\n\n\nsummary8\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x4\n11\n9\n3.3\n8\n8\n8\n19\n\n\nanscombe.y4\n11\n7.5\n2\n5.2\n6.2\n8.2\n12\n\n\n\n\n\n\n\n\n\nConclusion\nAnscombe’s Quartet is a powerful reminder that descriptive statistics alone may not reveal the complete story of your data. Visualization is a crucial tool in data analysis, helping you uncover patterns, outliers, and unexpected relationships that numbers alone might miss."
  },
  {
    "objectID": "posts/Tip05/Using Shiny Modules with Namespaces.html",
    "href": "posts/Tip05/Using Shiny Modules with Namespaces.html",
    "title": "Namespaces in shiny: Why you need them",
    "section": "",
    "text": "This is the first in a series of tips/tutorials that would have been incredibly helpful to me before writing certain R scripts. Thanks to CHATGPT and the minimal time it takes to organize and transform them into blog posts, I’ve decided to save and write them in this format to keep them on record\nIn Shiny, when you create modules, it’s essential to use namespaces (NS) to ensure that each module instance has unique IDs for its input and output elements. This prevents conflicts and unexpected behavior when working with multiple module instances."
  },
  {
    "objectID": "posts/Tip05/Using Shiny Modules with Namespaces.html#without-using-namespaces-not-recommended",
    "href": "posts/Tip05/Using Shiny Modules with Namespaces.html#without-using-namespaces-not-recommended",
    "title": "Namespaces in shiny: Why you need them",
    "section": "Without Using Namespaces (Not Recommended)",
    "text": "Without Using Namespaces (Not Recommended)\nLet’s start with an example where we create two instances of the same module without using namespaces:\n\nlibrary(shiny)\n\nmy_module <- function(id) {\n  tagList(\n    textInput(inputId = \"text_input\", label = \"Enter text:\"),\n    actionButton(inputId = \"action_button\", label = \"Click me\")\n  )\n}\n\nui <- fluidPage(\n  my_module(\"module_1\"),\n  my_module(\"module_2\")\n)\n\nIn this example, both module_1 and module_2 share the same IDs for input elements (text_input and action_button). If we interact with one module, it will affect the other module as well, leading to unexpected behavior."
  },
  {
    "objectID": "posts/Tip05/Using Shiny Modules with Namespaces.html#using-namespaces-recommended",
    "href": "posts/Tip05/Using Shiny Modules with Namespaces.html#using-namespaces-recommended",
    "title": "Namespaces in shiny: Why you need them",
    "section": "Using Namespaces (Recommended)",
    "text": "Using Namespaces (Recommended)\nNow, let’s use namespaces to create unique IDs for each module instance:\n\nlibrary(shiny)\n\nmy_module <- function(id) {\n  ns <- NS(id)\n  tagList(\n    textInput(inputId = ns(\"text_input\"), label = \"Enter text:\"),\n    actionButton(inputId = ns(\"action_button\"), label = \"Click me\")\n  )\n}\n\nui <- fluidPage(\n  my_module(\"module_1\"),\n  my_module(\"module_2\")\n)\n\nIn this example, we use NS to generate unique namespaces for each module instance (module_1 and module_2). As a result, the input element IDs are unique between instances, ensuring that interactions within one module do not affect the other module.\n\nConclusion\nWhen creating Shiny modules, it’s highly recommended to use namespaces (NS) to prevent ID conflicts between module instances. This practice ensures that each module operates independently and avoids unexpected behavior when working with multiple modules in your Shiny app."
  },
  {
    "objectID": "posts/montyHall/Using Shiny Modules with Namespaces.html",
    "href": "posts/montyHall/Using Shiny Modules with Namespaces.html",
    "title": "Monty Hall simulation",
    "section": "",
    "text": "Monty Hall Problem The Monty Hall problem is a famous probability puzzle. In this scenario, a contestant on a game show is presented with three doors. Behind one door, there is a car, while behind the other two, there are goats. The contestant chooses one door, and then the host, Monty Hall, who knows what is behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant is then faced with a choice: stick with their initial choice or switch to the other unopened door. What should the contestant do to maximize their chances of winning the car? ##Simulation in R\nLet’s use R to simulate this problem and analyze the results.\n\n# Set the number of simulations\nnum_simulations <- 10000\n\n# Initialize variables to keep track of wins when switching and staying\nswitch_wins <- 0\nstay_wins <- 0\n\n# Perform the simulations\nfor (i in 1:num_simulations) {\n  # Create three doors with one car and two goats\n  doors <- sample(c(\"car\", \"goat\", \"goat\"))\n\n  # Player's initial choice\n  player_choice <- sample(1:3, 1)\n\n  # Monty Hall reveals a goat behind one of the unchosen doors\n  monty_reveals <- which(doors[-player_choice] == \"goat\")\n  monty_reveals <- monty_reveals[1]  # Monty reveals the first goat he encounters\n\n  # Determine the other unchosen door\n  other_door <- setdiff(1:3, c(player_choice, monty_reveals))\n\n  # Simulate switching doors\n  switch_choice <- other_door[1]\n\n  # Check if the player wins when switching\n  if (doors[switch_choice] == \"car\") {\n    switch_wins <- switch_wins + 1\n  }\n\n  # Check if the player wins when staying\n  if (doors[player_choice] == \"car\") {\n    stay_wins <- stay_wins + 1\n  }\n}\n\n# Calculate win percentages\nswitch_win_percent <- (switch_wins / num_simulations) * 100\nstay_win_percent <- (stay_wins / num_simulations) * 100\n\n# Print results\ncat(\"When switching doors, the win percentage is:\", switch_win_percent, \"%\\n\")\ncat(\"When staying with the initial choice, the win percentage is:\", stay_win_percent, \"%\\n\")\n)"
  },
  {
    "objectID": "posts/montyHall/Monty Hall.html",
    "href": "posts/montyHall/Monty Hall.html",
    "title": "Monty Hall simulation",
    "section": "",
    "text": "The Monty Hall problem, a famous probability puzzle, is introduced.\nThe scenario involves a game show contestant choosing one door out of three, with one hiding a car and the others concealing goats.\nThe host, Monty Hall, who knows what’s behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant must decide whether to stick with their initial choice or switch to the unopened door to maximize their chances of winning the car.\nUsing R, a simulation with 10,000 scenarios is conducted, revealing that the win percentage is typically higher when switching doors in the Monty Hall problem.\n\nThe Monty Hall problem, a famous probability puzzle, is introduced. The scenario involves a game show contestant choosing one door out of three, with one hiding a car and the others concealing goats. The host, Monty Hall, who knows what’s behind each door, opens one of the remaining two doors to reveal a goat. The contestant must decide whether to stick with their initial choice or switch to the unopened door to maximize their chances of winning the car. Using R, a simulation with 10,000 scenarios is conducted, revealing that the win percentage is typically higher when switching doors in the Monty Hall problem.\nThe Monty Hall problem is a famous probability puzzle. In this scenario, a contestant on a game show is presented with three doors. Behind one door, there is a car, while behind the other two, there are goats. The contestant chooses one door, and then the host, Monty Hall, who knows what is behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant is then faced with a choice: stick with their initial choice or switch to the other unopened door. What should the contestant do to maximize their chances of winning the car?\nSimulation in R Let’s use R to simulate this problem and analyze the results.\n\n# Set the number of simulations\nnum_simulations &lt;- 10000\n\n# Initialize variables to keep track of wins when switching and staying\nswitch_wins &lt;- 0\nstay_wins &lt;- 0\n\n# Perform the simulations\nfor (i in 1:num_simulations) {\n  # Create three doors with one car and two goats\n  doors &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n\n  # Player's initial choice\n  player_choice &lt;- sample(1:3, 1)\n\n  # Monty Hall reveals a goat behind one of the unchosen doors\n  monty_reveals &lt;- which(doors[-player_choice] == \"goat\")\n  monty_reveals &lt;- monty_reveals[1]  # Monty reveals the first goat he encounters\n\n  # Determine the other unchosen door\n  other_door &lt;- setdiff(1:3, c(player_choice, monty_reveals))\n\n  # Simulate switching doors\n  switch_choice &lt;- other_door[1]\n\n  # Check if the player wins when switching\n  if (doors[switch_choice] == \"car\") {\n    switch_wins &lt;- switch_wins + 1\n  }\n\n  # Check if the player wins when staying\n  if (doors[player_choice] == \"car\") {\n    stay_wins &lt;- stay_wins + 1\n  }\n}\n\n# Calculate win percentages\nswitch_win_percent &lt;- (switch_wins / num_simulations) * 100\nstay_win_percent &lt;- (stay_wins / num_simulations) * 100\n\nNow, let’s create a more visually appealing plot to display the results:\n\nlibrary(ggplot2)\np &lt;- ggplot(data.frame(Strategy = c(\"Switch\", \"Stay\"), Win_Percentage = c(switch_win_percent, stay_win_percent)), aes(x = Strategy, y = Win_Percentage)) +\n  geom_bar(stat = \"identity\", fill = c(\"blue\", \"red\")) +\n  labs(title = \"Win Percentage in Monty Hall Problem\",\n       x = \"Strategy\",\n       y = \"Win Percentage (%)\") +\n  theme_minimal()\nprint(p)\n\n\n\n\nIn this simulation, we ran 10,000 scenarios of the Monty Hall problem and recorded the results. As the bar plot illustrates, the win percentage is typically higher when switching doors compared to staying with the initial choice.\n(Dickey et al. 1975)\n\n\n\n\nReferences\n\nDickey, James, N. T. Gridgeman, M. C. S. Kingsley, I. J. Good, James E. Carlson, Daniel Gianola, Michael H. Kutner, and Steve Selvin. 1975. “Letters to the Editor.” The American Statistician 29 (3): 131–34. http://www.jstor.org/stable/2683443."
  },
  {
    "objectID": "posts/montyHall - Copy/Dice rolls in shiny.html",
    "href": "posts/montyHall - Copy/Dice rolls in shiny.html",
    "title": "Monty Hall simulation",
    "section": "",
    "text": "Introduction\nIn this post, we’ll explore how to create a simple Shiny app that simulates dice rolls and displays the statistics of the results. Shiny is a powerful R package that allows you to build interactive web applications. We’ll use it to create a fun and interactive dice roll simulator.\n\n\nGetting Started\nTo get started, you’ll need to install the Shiny package if you haven’t already. You can do this with the following command:\n\ninstall.packages(\"shiny\")\nlibrary(shiny)\n\n# Define the user interface (UI)\nui <- fluidPage(\n  titlePanel(\"Dice Roll Simulator\"),\n  mainPanel(\n    actionButton(\"roll\", \"Roll Dice\"),\n    verbatimTextOutput(\"result\"),\n    plotOutput(\"dice_plot\")\n  )\n)\n\n#Creating the App Logic Now, let’s define the logic of our Shiny app. We’ll keep track of the results of the dice rolls and update the statistics. We’ll use reactive values to store the data.\n\nlibrary(shiny)\n\n# Define the user interface (UI)\nui <- fluidPage(\n  titlePanel(\"Dice Roll Simulator\"),\n  mainPanel(\n    actionButton(\"roll\", \"Roll Dice\"),\n    verbatimTextOutput(\"result\"),\n    plotOutput(\"dice_plot\")\n  )\n)\nserver <- function(input, output) {\n  dice_rolls <- reactiveVal(integer(0))\n  \n  observeEvent(input$roll, {\n    result1 <- sample(1:6, 1, replace = TRUE)\n    result2 <- sample(1:6, 1, replace = TRUE)\n    dice_rolls(c(dice_rolls(), result1 + result2))\n  })\n  \n  output$result <- renderText({\n    latest_roll <- tail(dice_rolls(), 1)\n    if (is.null(latest_roll)) {\n      \"Press 'Roll Dice' to start.\"\n    } else {\n      paste(\"Latest roll result: \", latest_roll)\n    }\n  })\n  \n  output$dice_plot <- renderPlot({\n    if (length(dice_rolls()) > 0) {\n      dice_counts <- table(dice_rolls())\n      barplot(dice_counts, main = \"Dice Roll Statistics\", xlab = \"Result\", ylab = \"Count\", col = \"blue\")\n    }\n  })\n}\n\nshinyApp(ui, server)\n\n\nListening on http://127.0.0.1:4231\n\n\n\n\n\n#Conclusion\nWith just a few lines of code, we’ve created an interactive dice roll simulator using Shiny. You can further customize the app, add more features, and change the appearance to suit your preferences. Shiny makes it easy to build web applications for data visualization and interactivity."
  },
  {
    "objectID": "posts/diceRolls/Dice rolls in shiny.html",
    "href": "posts/diceRolls/Dice rolls in shiny.html",
    "title": "Dice Rolls in shiny",
    "section": "",
    "text": "Introduction\nIn this post, we’ll explore how to create a simple Shiny app that simulates dice rolls and displays the statistics of the results. Shiny is a powerful R package that allows you to build interactive web applications. We’ll use it to create a fun and interactive dice roll simulator.\n\n\nGetting Started\nTo get started, you’ll need to install the Shiny package if you haven’t already. You can do this with the following command:\n\ninstall.packages(\"shiny\")\nlibrary(shiny)\n\n# Define the user interface (UI)\nui <- fluidPage(\n  titlePanel(\"Dice Roll Simulator\"),\n  mainPanel(\n    actionButton(\"roll\", \"Roll Dice\"),\n    verbatimTextOutput(\"result\"),\n    plotOutput(\"dice_plot\")\n  )\n)\n\n#Creating the App Logic Now, let’s define the logic of our Shiny app. We’ll keep track of the results of the dice rolls and update the statistics. We’ll use reactive values to store the data.\n\nlibrary(shiny)\n\n# Define the user interface (UI)\nui <- fluidPage(\n  titlePanel(\"Dice Roll Simulator\"),\n  mainPanel(\n    actionButton(\"roll\", \"Roll Dice\"),\n    verbatimTextOutput(\"result\"),\n    plotOutput(\"dice_plot\")\n  )\n)\nserver <- function(input, output) {\n  dice_rolls <- reactiveVal(integer(0))\n  \n  observeEvent(input$roll, {\n    result1 <- sample(1:6, 1, replace = TRUE)\n    result2 <- sample(1:6, 1, replace = TRUE)\n    dice_rolls(c(dice_rolls(), result1 + result2))\n  })\n  \n  output$result <- renderText({\n    latest_roll <- tail(dice_rolls(), 1)\n    if (is.null(latest_roll)) {\n      \"Press 'Roll Dice' to start.\"\n    } else {\n      paste(\"Latest roll result: \", latest_roll)\n    }\n  })\n  \n  output$dice_plot <- renderPlot({\n    if (length(dice_rolls()) > 0) {\n      dice_counts <- table(dice_rolls())\n      barplot(dice_counts, main = \"Dice Roll Statistics\", xlab = \"Result\", ylab = \"Count\", col = \"blue\")\n    }\n  })\n}\n\nshinyApp(ui, server)\n\n\nListening on http://127.0.0.1:7121\n\n\n\n\n\n\n\nConclusion\nWith just a few lines of code, we’ve created an interactive dice roll simulator using Shiny. You can further customize the app, add more features, and change the appearance to suit your preferences. Shiny makes it easy to build web applications for data visualization and interactivity."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "Giorgio’s pages",
    "section": "",
    "text": "I got a Ph.D in materials science and I’m a researcher with a variety of interests. I currently works as a researcher at the Italian National Council of Research (CNR) on topics related to the characterization and development of new materials.My interests also focus on lighting and rendering and creating scientific illustration. Here you will find my spaghetti code tutorials, blend files and other doodles\n\n\nhttps://giorgioluciano.github.io/listing.html\n\n\n\nhttps://github.com/giorgioluciano\n\n\n\nhttps://giorgioluciano.github.io/CrystalNodes/"
  },
  {
    "objectID": "posts/Tut1/Visualization.html",
    "href": "posts/Tut1/Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Introduction to EDA: Exploratory Data Analysis (EDA) is the initial step in data analysis, where the structure, trends, and characteristics of the data are understood before applying complex statistical models or machine learning algorithms.\nImportance of EDA: EDA serves various purposes, including understanding data, detecting patterns, data cleaning, feature engineering, generating hypotheses, and communicating insights to stakeholders.\nGenerating Simulated Data: Simulated data is essential for practicing EDA. Parameters and variables are defined to create sample data for further analysis in R.\nChoosing the Right Plot: The choice of visualization depends on the data’s nature and the analysis goals. Time series plots, distribution plots, and correlation plots are used to explore different aspects of data.\n5 .Interactive Data Visualization: Interactive visualizations, like those created using Plotly, enable users to explore data interactively by selecting elements, zooming, and getting detailed information, enhancing data exploration and presentation."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#importance-of-eda",
    "href": "posts/Tut1/Visualization.html#importance-of-eda",
    "title": "Data Visualization",
    "section": "Importance of EDA",
    "text": "Importance of EDA\nEDA serves several purposes:\n\nUnderstanding Data: EDA helps us become familiar with the dataset, identify the available variables, and understand their nature (numeric, categorical, temporal, etc.).\nDetecting Patterns: EDA allows us to detect patterns, relationships, and potential outliers within the data. This is critical for making informed decisions during the analysis.\nData Cleaning: Through EDA, we can identify missing values, outliers, or data inconsistencies that require cleaning and preprocessing.\nFeature Engineering: EDA may suggest feature engineering opportunities, such as creating new variables or transforming existing ones to better represent the underlying data.\nHypothesis Generation: EDA often leads to the generation of hypotheses or research questions, guiding further investigation.\nCommunicating Insights: EDA produces visualizations and summaries that facilitate the communication of insights to stakeholders or team members.\n\nIn the following sections, we will delve into the practical aspects of EDA, starting with data simulation and visualization techniques."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#generating-simulated-data",
    "href": "posts/Tut1/Visualization.html#generating-simulated-data",
    "title": "Data Visualization",
    "section": "Generating Simulated Data",
    "text": "Generating Simulated Data\nBefore diving into Exploratory Data Analysis (EDA) on real datasets, it’s helpful to begin with the generation of simulated data. This allows us to have full control over the data and create example scenarios to understand key EDA concepts. In this section, we will learn how to generate simulated datasets using R.\n\nParameters and Variables\nTo start, let’s define some basic parameters that we’ll use to generate simulated data:\n\nx_min: The minimum value for the variable x.\nx_max: The maximum value for the variable x.\nx_step: The increment between successive x values.\ny_mean: The mean value for the dependent variable y.\ny_sd: The standard deviation for the dependent variable y.\ny_min: The minimum possible value for y.\ny_max: The maximum possible value for y.\n\nWe will use these parameters to generate sample data.\nNow, let’s proceed to generate sample data based on the defined parameters. In this example, we’ll create a simple dataset with the following variables:\n\nx: A sequence of values ranging from x_min to x_max with an increment of x_step.\nvar_random: A random variable with values uniformly distributed between y_min and y_max.\nvar_norm: A variable with values generated from a normal distribution with mean y_mean and standard deviation y_sd.\nvar_sin: A variable with values generated as the sine function of x.\n\nHere’s the R code to create the sample dataset:\n\nlibrary(data.table)\n\n# Parameters\nx_min   &lt;- 0\nx_max   &lt;- 10   \nx_step  &lt;- 0.01\n\ny_mean  &lt;- 0.5\ny_sd    &lt;- 0.25\ny_min   &lt;- -1\ny_max   &lt;- 1     \n\nx       &lt;- seq(x_min, x_max, x_step)\n\n# Variables\nvar_random  &lt;- runif(length(x), y_min, y_max)\nvar_norm    &lt;- rnorm(length(x), y_mean, y_sd) \nvar_sin     &lt;- sin(x)\n\n# Data.frame \ndf  &lt;- data.frame(x, var_random, var_norm, var_sin)\ndt  &lt;- data.table(df)\n\n# Melt \ndtm &lt;- melt(dt, id.vars=\"x\")\n\nThis code creates a dataset df and a data.table dt containing the generated variables. The melt function from the data.table library is used to reshape the data for visualization purposes.\nWith our simulated data ready, we can now move on to creating various plots and performing EDA.\nIn this section, we will explore various visualization techniques that play a crucial role in Exploratory Data Analysis (EDA). Visualizations help us gain insights into the data’s distribution, patterns, and relationships between variables. We will use the simulated dataset generated in the previous section to illustrate these techniques."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#choosing-the-right-plot",
    "href": "posts/Tut1/Visualization.html#choosing-the-right-plot",
    "title": "Data Visualization",
    "section": "Choosing the Right Plot",
    "text": "Choosing the Right Plot\nThe choice of visualization depends on the nature of your data and the specific aspects you want to highlight. Generally, in EDA, we often need to:\n\nExamine Changes Over Time: Use time series plots when you want to assess changes in one or more variables over time.\nCheck for Data Distribution: Create distribution plots, such as histograms and density plots, to understand how data points are distributed.\nExplore Variable Relationships: Employ correlation plots and scatter plots to identify linear relationships between variables.\n\nLet’s start by examining these aspects one by one using our simulated dataset.\n\nTime Series Plots\nTo explore changes over time, we’ll create a time series plot for the var_sin variable. This variable represents a sine wave and is well-suited for a time series representation. Here’s the R code to create a time series plot:\n\nlibrary(ggplot2)\n\np &lt;- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\nIn this code, we use ggplot2 to create a line plot for the var_sin variable.\n\n\nDistribution Plots\nTo check the data distribution, we’ll create histogram plots for each of the variables: var_random, var_norm, and var_sin. Histograms provide a visual representation of the frequency distribution of data values. Here’s the R code:\n\np3 &lt;- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 &lt;- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\np4\n\n\n\np5\n\n\n\n\nThese plots will help us understand the distribution characteristics of our variables.\n\n\nCorrelation Plots\nCorrelation plots allow us to examine relationships between variables. We’ll create scatter plots for pairs of variables to assess their linear correlation. Here’s an example for var_sin and var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  &lt;- runif(x,y_min,y_max)\nvar_norm2    &lt;- rnorm(x,y_mean,y_sd) \nvar_sin2     &lt;- sin(x) + rnorm(x,0,0.01) \n\ndf2&lt;- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 &lt;- data.table(df2)\n\np10 &lt;- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\nThese scatter plots help us identify whether variables exhibit linear correlation.\nIn the following sections, we’ll delve deeper into each of these plot types, interpret the results, and explore additional visualization techniques for EDA.\n\nBox Plots\nBox plots, also known as box-and-whisker plots, provide a summary of the data’s distribution, including median, quartiles, and potential outliers. They are particularly useful for comparing the distributions of different variables or groups. Here’s an example of creating box plots for var_random, var_norm, and var_sin:\n\np6 &lt;- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\nBox plots can reveal variations and central tendencies of the variables.\n\n\nPair Plots\nPair plots, or scatterplot matrices, allow us to visualize pairwise relationships between multiple variables in a dataset. They are helpful for identifying correlations and patterns among variables simultaneously. Here’s how to create a pair plot for our dataset:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot &lt;- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\n\n\n\nPair plots provide a comprehensive view of variable interactions.\n\n\nTime Series Decomposition\nTime series data often contain underlying components such as trends and seasonality that can be crucial for understanding the data’s behavior. Time series decomposition is a technique used in Exploratory Data Analysis (EDA) to separate these components. In this section, we’ll demonstrate how to perform time series decomposition using our simulated var_sin data.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp &lt;- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\nThe code above performs the following:\n\nDecomposes the var_sin time series using the decompose function. We specify a frequency of 365 since the data represents daily observations.\nPlots the decomposed components, including the original time series, trend, seasonal component, and residual.\n\nThe resulting plot will show the individual components of the time series, allowing us to gain insights into its underlying patterns.\n\n\nInteractive Visualizations\nInteractive plots, created using libraries like plotly or shiny, allow users to explore data interactively. You can create interactive scatter plots, line plots, or heatmaps, enhancing the user’s ability to dig deeper into the data.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nCaricamento pacchetto: 'plotly'\n\n\nIl seguente oggetto è mascherato da 'package:ggplot2':\n\n    last_plot\n\n\nIl seguente oggetto è mascherato da 'package:stats':\n\n    filter\n\n\nIl seguente oggetto è mascherato da 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot &lt;- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"&lt;br&gt;var_random:\", var_random, \"&lt;br&gt;var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %&gt;%\n  add_markers() %&gt;%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn this initial section, we’ve introduced the fundamental concepts of exploratory data analysis (EDA) and the importance of data visualization in gaining insights from complex datasets. We’ve explored various types of plots and their applications in EDA.\nNow, let’s dive deeper and enhance our understanding by demonstrating practical examples of EDA using real-world datasets. We’ll showcase how different types of plots and interactive visualizations can provide valuable insights and drive data-driven decisions.\nLet’s embark on this EDA journey and uncover the hidden stories within our data through hands-on examples."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#examples",
    "href": "posts/Tut1/Visualization.html#examples",
    "title": "Data Visualization",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "posts/Tut1/Visualization.html#time-series-decomposition-for-insights",
    "href": "posts/Tut1/Visualization.html#time-series-decomposition-for-insights",
    "title": "Data Visualization",
    "section": "Time Series Decomposition for Insights",
    "text": "Time Series Decomposition for Insights\n\nIntroduction\nIn this section, we’ll dive into a meaningful example of time series decomposition to demonstrate its practical utility in Exploratory Data Analysis (EDA). Time series decomposition allows us to extract valuable insights from time-dependent data. We’ll use our simulated var_sin time series to illustrate its significance.\n\n\nScenario: Analyzing Daily Temperature Data\nImagine we have daily temperature data for a city over several years. We want to understand the underlying patterns in temperature variations, including trends and seasonality, to make informed decisions related to weather forecasts, climate monitoring, or energy management.\nLet’s create the enhanced dataset with temperature data for multiple cities. We’ll use the data.table library to manage the dataset efficiently:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities &lt;- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date &lt;- as.Date(\"2010-01-01\")\nend_date &lt;- as.Date(\"2019-12-31\")\ndate_seq &lt;- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data &lt;- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature &lt;- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp &lt;- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\nWe’ve generated temperature data for each city over the span of ten years, resulting in a diverse and complex dataset.\n\n\nPerforming Time Series Decomposition\nNow that we have our multi-city temperature dataset, let’s apply time series decomposition to analyze temperature trends and seasonality for one of the cities, such as New York (see plot)\n\n\nInterpretation\nThe plot will display the components of the time series for New York, including the original time series, trend, seasonal component, and residual. Similar analyses can be performed for other cities to identify regional temperature patterns.\n\n\nInsights and Applications\nWith our enhanced multi-city temperature dataset and time series decomposition, we can:\n\nRegional Analysis: Compare temperature patterns across different cities to identify regional variations.\nSeasonal Insights: Understand how temperature seasonality differs between cities and regions.\nLong-Term Trends: Analyze temperature trends for each city over the ten-year period.\n\nThis advanced analysis helps us make informed decisions related to climate monitoring, urban planning, and resource management."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#leveraging-distribution-plots-for-in-depth-analysis",
    "href": "posts/Tut1/Visualization.html#leveraging-distribution-plots-for-in-depth-analysis",
    "title": "Data Visualization",
    "section": "Leveraging Distribution Plots for In-Depth Analysis",
    "text": "Leveraging Distribution Plots for In-Depth Analysis\n\nIntroduction\nIn this section, we’ll illustrate the significance of distribution plots in Exploratory Data Analysis (EDA) by considering a practical scenario. Distribution plots help us understand how data points are distributed and can reveal insights about the underlying data characteristics. We’ll use our simulated dataset and focus on the var_random variable.\n\n\nScenario: Analyzing Exam Scores\nImagine we have a dataset containing exam scores of students in a class. We want to gain insights into the distribution of exam scores to answer questions like:\n\nWhat is the typical exam score?\nAre the exam scores normally distributed?\nAre there any outliers or unusual patterns in the scores?\n\n\n\nAnalyzing the Distribution of Exam Scores\nLet’s create a histogram to visualize the distribution of exam scores using the var_random variable. This will help us answer the questions posed above.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\nInterpretation\nThe resulting histogram will display the distribution of exam scores. Here’s what we can interpret:\n\nTypical Exam Score: The histogram will show where the majority of exam scores lie, indicating the typical or central value.\nDistribution Shape: We can assess whether the scores follow a normal distribution, are skewed, or have other unique characteristics.\nOutliers: Outliers, if present, will appear as data points far from the central part of the distribution.\n\n\n\nInsights and Applications\nBy analyzing the distribution of exam scores, we can:\n\nIdentify Central Tendency: Determine the typical exam score, which can be useful for setting benchmarks or evaluating student performance.\nUnderstand Data Characteristics: Gain insights into the shape of the distribution, which informs us about the data’s characteristics.\nDetect Outliers: Identify outliers or unusual scores that may require further investigation."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#correlation-analysis",
    "href": "posts/Tut1/Visualization.html#correlation-analysis",
    "title": "Data Visualization",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nIn this section, we’ll explore advanced correlation analysis using more complex datasets. We’ll create two datasets: one representing students’ academic performance, and the other containing information about their study habits and extracurricular activities. We’ll investigate correlations between various factors to gain deeper insights.\n\nCreating Complex Datasets\nLet’s create the two complex datasets for our correlation analysis:\nAcademic Performance Dataset:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students &lt;- 500\n\nacademic_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\nAdvanced Correlation Analysis\nNow that we have our complex datasets, let’s perform advanced correlation analysis to explore relationships between academic performance, study habits, and extracurricular activities. We’ll calculate correlations and visualize them using a heatmap:\n\n# Calculate correlations between variables\ncorrelation_matrix &lt;- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\nInterpretation\nThe resulting heatmap visually represents the correlations between academic performance and study-related factors. Here’s what we can interpret:\n\nColor Intensity: The color intensity indicates the strength and direction of the correlation. Positive correlations are shown in blue, while negative correlations are in red. The darker the color, the stronger the correlation.\nCorrelation Coefficients: The heatmap displays the actual correlation coefficients as labels in the lower triangle. These values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n\n\n\nInsights and Applications\nBy conducting advanced correlation analysis, we can:\n\nUnderstand Complex Relationships: Explore intricate correlations between academic performance, study hours, extracurricular activities, and stress levels.\nIdentify Key Factors: Determine which factors have the most significant impact on academic performance.\nOptimize Student Support: Use insights to provide targeted support and interventions for students.\n\nAdvanced correlation analysis helps us uncover nuanced relationships within complex datasets."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#exploring-diverse-distributions-with-box-plots",
    "href": "posts/Tut1/Visualization.html#exploring-diverse-distributions-with-box-plots",
    "title": "Data Visualization",
    "section": "Exploring Diverse Distributions with Box Plots",
    "text": "Exploring Diverse Distributions with Box Plots\nIn this section, we’ll explore the versatility of box plots by working with diverse and complex datasets. We’ll create two datasets: one representing the distribution of monthly sales for multiple product categories, and the other containing information about customer demographics. These datasets will allow us to visualize various types of distributions and identify outliers.\n\nCreating Complex Datasets\nLet’s create the two complex datasets for our box plot analysis:\nSales Dataset and Customer Demographics Dataset:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months &lt;- 24\nproduct_categories &lt;- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data &lt;- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers &lt;- 300\n\ndemographics_data &lt;- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 &lt;- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 &lt;- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\nInterpretation\nThese box plots help us gain insights into diverse distributions:\n\nSales Distribution: We can observe how sales are distributed across different product categories, identifying variations and potential outliers.\nCustomer Age Distribution: The box plot displays the spread of customer ages, highlighting the central tendency and any potential outliers.\n\n\n\nInsights and Applications\nBy using box plots with complex datasets, we can:\n\nAnalyze Diverse Distributions: Visualize and compare distributions of sales for multiple product categories and customer age distributions.\nOutlier Detection: Identify potential outliers in both sales data and customer demographics.\nSegmentation Insights: Understand how sales vary across product categories and the age distribution of customers.\n\nBox plots are versatile tools for exploring various types of data distributions and making data-driven decisions."
  },
  {
    "objectID": "posts/Tut1/Visualization.html#interactive-data-visualization-with-plotly",
    "href": "posts/Tut1/Visualization.html#interactive-data-visualization-with-plotly",
    "title": "Data Visualization",
    "section": "Interactive Data Visualization with Plotly",
    "text": "Interactive Data Visualization with Plotly\n\nCreating an Interactive Time Series Plot\nSuppose we have a dataset containing monthly stock prices for three companies: Company A, Company B, and Company C. We want to create an interactive time series plot that allows users to:\n\nSelect the company they want to visualize.\nZoom in and out to explore specific time periods.\nHover over data points to view detailed information.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months &lt;- 24\n\ntime_series_data &lt;- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot &lt;- plot_ly(data = time_series_data, x = ~Date) %&gt;%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %&gt;%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\nInteraction Features\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\nInsights and Applications\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\nInteractive data visualizations are a powerful tool for EDA and data presentation. In the next section, we’ll explore another advanced visualization technique: time series decomposition.\n\n\nInteraction Features\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\nInsights and Applications\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\n[Wickham (2016)](Schloerke et al. 2021)"
  },
  {
    "objectID": "posts/Fun01/Monty Hall.html",
    "href": "posts/Fun01/Monty Hall.html",
    "title": "Monty Hall simulation",
    "section": "",
    "text": "The Monty Hall problem, a famous probability puzzle, is introduced.\nThe scenario involves a game show contestant choosing one door out of three, with one hiding a car and the others concealing goats.\nThe host, Monty Hall, who knows what’s behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant must decide whether to stick with their initial choice or switch to the unopened door to maximize their chances of winning the car.\nUsing R, a simulation with 10,000 scenarios is conducted, revealing that the win percentage is typically higher when switching doors in the Monty Hall problem.\n\nThe Monty Hall problem is a famous probability puzzle. In this scenario, a contestant on a game show is presented with three doors. Behind one door, there is a car, while behind the other two, there are goats. The contestant chooses one door, and then the host, Monty Hall, who knows what is behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant is then faced with a choice: stick with their initial choice or switch to the other unopened door. What should the contestant do to maximize their chances of winning the car?\nSimulation in R Let’s use R to simulate this problem and analyze the results.\n\n# Set the number of simulations\nnum_simulations &lt;- 10000\n\n# Initialize variables to keep track of wins when switching and staying\nswitch_wins &lt;- 0\nstay_wins &lt;- 0\n\n# Perform the simulations\nfor (i in 1:num_simulations) {\n  # Create three doors with one car and two goats\n  doors &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n\n  # Player's initial choice\n  player_choice &lt;- sample(1:3, 1)\n\n  # Monty Hall reveals a goat behind one of the unchosen doors\n  monty_reveals &lt;- which(doors[-player_choice] == \"goat\")\n  monty_reveals &lt;- monty_reveals[1]  # Monty reveals the first goat he encounters\n\n  # Determine the other unchosen door\n  other_door &lt;- setdiff(1:3, c(player_choice, monty_reveals))\n\n  # Simulate switching doors\n  switch_choice &lt;- other_door[1]\n\n  # Check if the player wins when switching\n  if (doors[switch_choice] == \"car\") {\n    switch_wins &lt;- switch_wins + 1\n  }\n\n  # Check if the player wins when staying\n  if (doors[player_choice] == \"car\") {\n    stay_wins &lt;- stay_wins + 1\n  }\n}\n\n# Calculate win percentages\nswitch_win_percent &lt;- (switch_wins / num_simulations) * 100\nstay_win_percent &lt;- (stay_wins / num_simulations) * 100\n\nNow, let’s create a more visually appealing plot to display the results:\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.2\n\nlibrary(ggsci)\np &lt;- ggplot(data.frame(Strategy = c(\"Switch\", \"Stay\"), Win_Percentage = c(switch_win_percent, stay_win_percent)), aes(x = Strategy, y = Win_Percentage)) +\n  geom_bar(stat = \"identity\", fill = c(\"lightblue\", \"papayawhip\")) +\n  labs(title = \"Win Percentage in Monty Hall Problem\",\n       x = \"Strategy\",\n       y = \"Win Percentage (%)\") +\n  theme_minimal()\nprint(p)\n\n\n\n\nIn this simulation, we ran 10,000 scenarios of the Monty Hall problem and recorded the results. As the bar plot illustrates, the win percentage is typically higher when switching doors compared to staying with the initial choice.\n(Dickey et al. 1975)\n\n\n\n\nReferences\n\nDickey, James, N. T. Gridgeman, M. C. S. Kingsley, I. J. Good, James E. Carlson, Daniel Gianola, Michael H. Kutner, and Steve Selvin. 1975. “Letters to the Editor.” The American Statistician 29 (3): 131–34. http://www.jstor.org/stable/2683443."
  },
  {
    "objectID": "posts/Fun02/Dice rolls in shiny.html",
    "href": "posts/Fun02/Dice rolls in shiny.html",
    "title": "Central Limit Theorem with Dice Rolls",
    "section": "",
    "text": "Title: Unveiling the Central Limit Theorem with Dice Rolls\n\nDice rolls offer a fascinating insight into probability, especially when it comes to the sum of multiple dice rolls.\nThe Central Limit Theorem states that as the number of dice rolls increases, the distribution of sums approaches the Gaussian distribution.\nIn this blog post, we explore this intriguing concept, conduct real-world dice rolls, and use R to visualize the transformation from dice sums to Gaussian distribution.\n\nIntroduction:\nDice rolling is a pastime enjoyed by many, whether in board games or games of chance. But have you ever wondered how the sum of multiple dice rolls behaves when you roll them repeatedly? In this blog post, we embark on a journey into the world of dice rolls and the fascinating Central Limit Theorem. We’ll uncover how the sum of dice rolls can transform into a Gaussian distribution as the number of rolls increases.\nUnderstanding Dice Rolls:\nTo fully grasp how the sum of dice rolls approaches a Gaussian distribution, let’s start with an overview of dice rolls. Imagine rolling a standard six-sided die, the kind commonly found in board games. Each roll yields a number between 1 and 6, with each outcome equally likely at 1/6.\nNow, what happens when we roll two dice and sum the results? In this scenario, the range of possible sums expands. You could obtain a sum of 2 (when both dice show 1) or a sum of 12 (when both dice display 6), with all possible combinations in between. For instance, you could achieve a sum of 7 when one die shows 3 and the other shows 4, or a sum of 4 when one die shows 2 and the other shows 2.\nVisualizing the sum of two dice rolls can be done using a bar chart. Notably, there are many more possible combinations resulting in a sum of 7 compared to combinations yielding sums of 2 or 12. This creates a triangular distribution, with a central peak at the most likely sum (in the case of two dice, that’s 7) and tails tapering off as you move away from the peak.\nHowever, this triangular distribution will change as we increase the number of dice rolled and the total number of rolls. By continuing with our simulation, we will witness how this distribution transforms into a Gaussian distribution as we increase the value of n, as predicted by the Central Limit Theorem.\nStay with us as we explore this phenomenon through a practical simulation and visualize the results using R.\nSimulation and Visualization:\nLet’s see the theory in action. We’ll use R to simulate multiple dice rolls and visualize how the distribution changes with an increasing value of n.\n\n# Load the necessary library\nlibrary(ggplot2)\n\n# Number of dice rolls and number of dice\nn_rolls <- 10000\nn_dice <- c(2, 3, 4, 5, 6, 7,8,9, 10, 20, 50, 100)  # You can add more values\n\n# Function to simulate dice rolls and calculate the sum\nsimulate_dice_rolls <- function(n_rolls, n_dice) {\n  results <- replicate(n_rolls, sum(sample(1:6, n_dice, replace = TRUE)))\n  return(results)\n}\n\n# Simulate dice rolls for various values of n_dice\nresults_list <- lapply(n_dice, function(n) simulate_dice_rolls(n_rolls, n))\n\n# Create a data frame to store the results\nresults_df <- data.frame(N_Dice = rep(n_dice, each = n_rolls), Sum = unlist(results_list))\n\n# Create a histogram to visualize the distributions\np <- ggplot(results_df, aes(x = Sum)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  facet_wrap(~N_Dice, scales = \"free\") +\n  labs(title = \"Sum of Dice Rolls vs. Number of Dice\",\n       x = \"Sum of Dice Rolls\",\n       y = \"Frequency\")\n\nprint(p)\n\n\n\n\nConclusion:\nIn conclusion, the Central Limit Theorem offers an intriguing insight into the behavior of dice rolls. As we roll dice more and more times and sum the results, the distribution of the sums converges towards a Gaussian distribution, defying the original distribution of the dice rolls. This exemplifies the power and predictability of probability theory.\nDice, which often symbolize chance and unpredictability, paradoxically adhere to mathematical order described by the Central Limit Theorem. It’s yet another testament to how mathematics unveils hidden patterns in our everyday experiences.\nBy understanding this phenomenon, we can better predict the outcomes of random events, whether in games of chance or in statistical analysis. The world of dice, in its apparent chaos, unveils the beauty of order through the Central Limit Theorem."
  },
  {
    "objectID": "posts/Fun03/index.html",
    "href": "posts/Fun03/index.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Anscombe’s Quartet comprises four datasets that share nearly identical descriptive statistics but exhibit visually distinct behaviors, emphasizing the significance of data visualization.\nThis blog post delves into the calculations and visualization of Anscombe’s Quartet using R and the powerful ggplot2 library. Custom functions are also employed to generate and analyze these quartets.\nThe environment setup includes the necessary libraries such as ggplot2 for data visualization.\nUnderstanding Anscombe’s Quartet: A brief overview of the quartet’s four datasets, each with its unique characteristics, is provided, underlining the importance of visual representation in data analysis.\nCustom R code and examples are presented to generate and visualize Anscombe’s Quartet, shedding light on the importance of data visualization in revealing patterns and relationships that statistics alone may overlook.\n\n\nExploring Anscombe’s Quartet with R, ggplot2, and Custom Functions\nAnscombe’s Quartet, known as the “Anscombe’s Test,” consists of four datasets with very similar descriptive statistics but visually distinct characteristics. These quartets serve as an enlightening example of the importance of visualizing data before drawing conclusions.\nIn this post, we will delve into how to calculate and visualize Anscombe’s Quartet using R and the powerful ggplot2 library. We’ll also use custom functions to generate these quartets and analyze them.\n\n\nIntroduction\nAnscombe’s Quartet was created by the statistician Francis Anscombe in 1973 to underscore the importance of data visualization before analysis. Despite having similar statistics, these datasets exhibit significantly different visual behaviors. Let’s see how R and ggplot2 help us explore them.\n\n\nSetting Up the Environment\nTo get started, we need to load some libraries:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n\n\nUnderstanding Anscombe’s Quartet\nAnscombe’s Quartet comprises four datasets, each with 11 data points. Here’s a brief overview of the quartet:\n\nDataset 1: A straightforward linear relationship between X and Y.\nDataset 2: A linear relationship with an outlier.\nDataset 3: A linear relationship with one point substantially different from the others.\nDataset 4: A non-linear relationship.\n\n(see Rpubs page)\n\nlibrary(datasets)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nCaricamento pacchetto: 'dplyr'\n\n\nI seguenti oggetti sono mascherati da 'package:data.table':\n\n    between, first, last\n\n\nI seguenti oggetti sono mascherati da 'package:stats':\n\n    filter, lag\n\n\nI seguenti oggetti sono mascherati da 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\ndatasets::anscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nanscombe_tidy &lt;- anscombe %&gt;%\n    mutate(observation = seq_len(n())) %&gt;%\n    gather(key, value, -observation) %&gt;%\n    separate(key, c(\"variable\", \"set\"), 1, convert = TRUE) %&gt;%\n    mutate(set = c(\"I\", \"II\", \"III\", \"IV\")[set]) %&gt;%\n    spread(variable, value)\n\nhead(anscombe_tidy)\n\n  observation set  x    y\n1           1   I 10 8.04\n2           1  II 10 9.14\n3           1 III 10 7.46\n4           1  IV  8 6.58\n5           2   I  8 6.95\n6           2  II  8 8.14\n\nggplot(anscombe_tidy, aes(x, y)) +\n    geom_point() +\n    facet_wrap(~ set) +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nVisualizing the Quartet\nNow, let’s integrate your custom R code and examples to generate and visualize Anscombe’s Quartet:\n\nlibrary(vtable)\n\nCaricamento del pacchetto richiesto: kableExtra\n\n\n\nCaricamento pacchetto: 'kableExtra'\n\n\nIl seguente oggetto è mascherato da 'package:dplyr':\n\n    group_rows\n\nlibrary(kableExtra)\nlibrary(patchwork)\n\n\n# Note: Function to generate Anscombe's Quartet datasets for x2 we need a trick that can be also improved but for now as a brutal approx works\n\n\nplotreg &lt;- function(df) {\n  formula &lt;- y ~ x\n  \n  ggplot(df, aes(x = x, y = y)) +\n    geom_point(aes(size = 1), alpha = 0.3) +\n    geom_smooth(method = \"lm\", formula = formula, se = TRUE) +\n    coord_cartesian(xlim = c(4, 19), ylim = c(4, 14)) +  # Imposta i limiti di x e y\n    theme_light(base_size = 10) +\n    theme(legend.position = \"none\")\n}\n\ngenerate_noisy_points &lt;- function(x, y, noise_level = 0.1) {\n  n &lt;- length(x)\n  \n  # Generate random noise for x and y separately\n  noise_x &lt;- rnorm(n, mean = 0, sd = noise_level)\n  noise_y &lt;- rnorm(n, mean = 0, sd = noise_level)\n  \n  # Ensure that the sum of noise on x and y is approximately zero\n  noise_x &lt;- noise_x - mean(noise_x)\n  noise_y &lt;- noise_y - mean(noise_y)\n  \n  # Add noise to the original data\n  x_noisy &lt;- x + noise_x\n  y_noisy &lt;- y + noise_y\n  \n  return(data.frame(x = x_noisy, y = y_noisy))\n}\n\n# Function to generate approximated points with an option to add noise\ngenerate_approximated_points &lt;- function(n, x, y, noise_level = 0) {\n  # Create a new interpolation based on the original data\n  interpolated_values &lt;- approx(x, y, xout = seq(min(x), max(x), length.out = n))\n  \n  # Extract the interpolated points\n  x_interp &lt;- interpolated_values$x\n  y_interp &lt;- interpolated_values$y\n  \n  # Add noise if needed\n  if (noise_level &gt; 0) {\n    noise &lt;- rnorm(n, mean = 0, sd = noise_level)\n    x_interp &lt;- x_interp + noise\n    y_interp &lt;- y_interp + noise\n  }\n  \n  # Return the approximated points\n  return(data.frame(x = x_interp, y = y_interp))\n}\n\n\nmultians &lt;- function(npoints = 11, anscombe) {\n  x1 &lt;- anscombe$x1\n  x2 &lt;- anscombe$x2\n  x3 &lt;- anscombe$x3\n  x4 &lt;- anscombe$x4\n  y1 &lt;- anscombe$y1\n  y2 &lt;- anscombe$y2\n  y3 &lt;- anscombe$y3\n  y4 &lt;- anscombe$y4\n\n  ## Generate Quartet 1 ##\n  x_selected &lt;- c(x1[2], x1[4], x1[11])\n  y_selected &lt;- c(y1[2], y1[4], y1[11])\n\n  # Calculate the linear regression\n  linear_model &lt;- lm(y_selected ~ x_selected)\n\n  # Extract coefficients of the line\n  intercept &lt;- coef(linear_model)[1]\n  slope &lt;- coef(linear_model)[2]\n\n  # Create a sinusoidal curve above or below the line\n  x_sin &lt;- seq(min(x1), max(x1), length.out = npoints)  # x range for the sinusoid\n  amplitude &lt;- 1  # Amplitude of the sinusoid\n  frequency &lt;- 4  # Frequency of the sinusoid\n  phase &lt;- pi / 2  # Phase of the sinusoid (for rotation)\n  sinusoid &lt;- amplitude * sin(2 * pi * frequency * (x_sin - min(x1)) / (max(x1) - min(x1)) + phase)\n\n  # Generate points above or below the line\n  y_sin &lt;- slope * x_sin + intercept + sinusoid\n  df1 &lt;- data.frame(x = x_sin, y = y_sin)\n\n  ## Generate Quartet 2 ##\n  n_points_approximated &lt;- npoints\n  noise_level &lt;- 0.1\n\n  # Generate approximated points\n  approximated_points &lt;- generate_approximated_points(n_points_approximated, x2, y2, noise_level = 0.1)\n\n  # Add noise to the approximated points\n  noisy_approximated_points &lt;- generate_noisy_points(approximated_points$x, approximated_points$y, noise_level)\n\n  # Now, you have noisy approximated points in df2\n  df2 &lt;- data.frame(x = noisy_approximated_points$x, y = noisy_approximated_points$y)\n\n  ## Generate Quartet 3 ##\n  lm_model &lt;- lm(y3 ~ x3, subset = -c(3))\n  x_generated &lt;- seq(min(x3), max(x3), length.out = npoints)\n  y_generated &lt;- predict(lm_model, newdata = data.frame(x3 = x_generated))\n\n  x_outlier &lt;- 13\n  y_outlier &lt;- 12.74\n  x_generated &lt;- c(x_generated, x_outlier)\n  y_generated &lt;- c(y_generated, y_outlier)\n\n  df3 &lt;- data.frame(x = x_generated, y = y_generated)\n\n  ## Generate Quartet 4 ##\n  \n  y4[9]\n  x &lt;- c(rep(min(x4),npoints))\n  y &lt;- c(seq(min(y4[-8]), max(y4[-8]), length.out = (npoints)))\n  x_new = c(x,x4[8])\n  y_new = c(y,y4[8])\n  df4 &lt;- data.frame(x = x_new, y = y_new)\n\n  return(list(df1 = df1, df2 = df2, df3 = df3, df4 = df4))\n}\n\n\n\n# Generate and plot Quartet 1\nt1 &lt;- multians(33,anscombe)\n\np1 &lt;- plotreg(t1$df1)\np3 &lt;- plotreg(t1$df3)\np4 &lt;- plotreg(t1$df4)\np2 &lt;- plotreg(t1$df2)\n\n(p1 | p2) / (p3 | p4)\n\n\n\n# Example of eight summaries (replace them with your own)\nsummary1 &lt;- st(t1$df1)\nsummary5 &lt;- st(data.frame(anscombe$x1,anscombe$y1))\n\nsummary2 &lt;- st(t1$df2)\nsummary6 &lt;- st(data.frame(anscombe$x2,anscombe$y2))\n\nsummary3 &lt;- st(t1$df3)\nsummary7 &lt;- st(data.frame(anscombe$x3,anscombe$y3))\n\nsummary4 &lt;- st(t1$df4)\nsummary8 &lt;- st(data.frame(anscombe$x4,anscombe$y4))\n\n\nsummary1 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n33\n9\n3\n4\n6.5\n12\n14\n\n\ny\n33\n8.3\n2.2\n4.7\n6.5\n10\n13\n\n\n\n\n\n\nsummary5 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x1\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y1\n11\n7.5\n2\n4.3\n6.3\n8.6\n11\n\n\n\n\n\n\nsummary2 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n33\n9\n3\n4.1\n6.4\n12\n14\n\n\ny\n33\n7.6\n1.8\n3.1\n6.6\n9\n9.4\n\n\n\n\n\n\nsummary6 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x2\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y2\n11\n7.5\n2\n3.1\n6.7\n8.9\n9.3\n\n\n\n\n\n\nsummary3\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n34\n9.1\n3.1\n4\n6.6\n12\n14\n\n\ny\n34\n7.3\n1.4\n5.4\n6.3\n8.1\n13\n\n\n\n\n\n\nsummary7 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x3\n11\n9\n3.3\n4\n6.5\n12\n14\n\n\nanscombe.y3\n11\n7.5\n2\n5.4\n6.2\n8\n13\n\n\n\n\n\n\nsummary4 \n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nx\n34\n8.3\n1.9\n8\n8\n8\n19\n\n\ny\n34\n7.2\n1.4\n5.2\n6.2\n8\n12\n\n\n\n\n\n\nsummary8\n\n\nSummary Statistics\n\n\nVariable\nN\nMean\nStd. Dev.\nMin\nPctl. 25\nPctl. 75\nMax\n\n\n\n\nanscombe.x4\n11\n9\n3.3\n8\n8\n8\n19\n\n\nanscombe.y4\n11\n7.5\n2\n5.2\n6.2\n8.2\n12\n\n\n\n\n\n\n\n\n\nConclusion\nAnscombe’s Quartet is a powerful reminder that descriptive statistics alone may not reveal the complete story of your data. Visualization is a crucial tool in data analysis, helping you uncover patterns, outliers, and unexpected relationships that numbers alone might miss.\n(Szafir 2018; Anscombe 1973)\n\n\n\n\n\nReferences\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21. http://www.jstor.org/stable/2682899.\n\n\nSzafir, Danielle Albers. 2018. “The Good, the Bad, and the Biased: Five Ways Visualizations Can Mislead (and How to Fix Them).” Interactions 25 (4): 26–33. https://doi.org/10.1145/3231772."
  },
  {
    "objectID": "posts/Snip01/index.html",
    "href": "posts/Snip01/index.html",
    "title": "Snippet #1: ggplot loops",
    "section": "",
    "text": "Create an empty list\nPopulate your list with objects (ggplots)\nCreate iteratively names for the objects\nRename the objects inside the list using the name list generated previously\nShow all plots using wrap_plots\n\nInstead of using boring plots we will use our private art collections and items.\nOne great package to create your art in R is aRtsy Let’s fire it up\n\nrequire(aRtsy)\n\nCaricamento del pacchetto richiesto: aRtsy\n\nrequire(patchwork)\n\nCaricamento del pacchetto richiesto: patchwork\n\n\n\n#before starting for having a look at the palette \n?colorPalette\n\navvio in corso del server httpd per la guida ... fatto\n\n\nCreate a Mondrian and save it\n\nset.seed(34)\nComposition_10 &lt;- canvas_squares(colors = colorPalette(\"boogy2\"))\nsaveCanvas(Composition_10 , filename = \"Mondrian.png\")\nComposition_10 \n\n\n\n\nand another one\n\nset.seed(1)\naspect_ratio &lt;- 1\nheight &lt;- 2\nComposition_1 = canvas_segments(colors = colorPalette(\"blackwhite\"))\nComposition_1 \n\n\n\n\nor if you want to create a lots of them, create names automatically and then take a look at just one of your artistic composition in your collection use the following code:\n\nn_items &lt;- 3\ncollection &lt;- list()\nname_of_Composition  &lt;- list()\nfor (i in 1:n_items) {\n  seed &lt;-  (sample(1:100000,1)) + 1\n  name_of_Composition[[i]] &lt;- paste0(\"Composition_\", i)\n  collection[[i]] &lt;- canvas_squares(colors = colorPalette(\"boogy2\"))\n  \n}\nnames(collection) &lt;- name_of_Composition\n\ncollection\n\n$Composition_1\n\n\n\n\n\n\n$Composition_2\n\n\n\n\n\n\n$Composition_3\n\n\n\n\n\n\n#as you can notice the setting for figure output in this chunk was changed in order to showplots with a rato of 3:1\nwrap_plots(collection)\n\n\n\n\n[Wickham (2016)](Derks 2022)(Pedersen 2022)\n\n\n\n\nReferences\n\nDerks, Koen. 2022. “aRtsy: Generative Art with ’Ggplot2’.” https://CRAN.R-project.org/package=aRtsy.\n\n\nPedersen, Thomas Lin. 2022. “Patchwork: The Composer of Plots.” https://CRAN.R-project.org/package=patchwork.\n\n\nWickham, Hadley. 2016. “Ggplot2: Elegant Graphics for Data Analysis.” https://ggplot2.tidyverse.org."
  },
  {
    "objectID": "posts/Snip02/index.html",
    "href": "posts/Snip02/index.html",
    "title": "Snippet #2: Cleaning column names of an imported csv",
    "section": "",
    "text": "Import data from a csv file\nUse the function clean_names from (Firke 2023)j R function\nWrite a function in base using gsub and regex to tackle specific issues\nYou’re done\n\nFirst of all we import the csv using the library (Müller 2020)here\n\nlibrary(here)\n\nhere() starts at C:/Users/giorg/Documents/GitHub/giorgioluciano.github.io\n\nfile_in &lt;- \"FakeData.csv\"\npath_in &lt;- \"posts/Snip02/\"\ndata &lt;- read.csv(here(path_in,file_in), head=T, check.names=F, encoding=\"latin1\")\n\n\nlibrary(janitor)\n\n\nCaricamento pacchetto: 'janitor'\n\n\nI seguenti oggetti sono mascherati da 'package:stats':\n\n    chisq.test, fisher.test\n\ndata_fixed &lt;- clean_names(data)\n\nAnd now the function written by William Doane\n\nclinical_names &lt;- function(.data, unique = FALSE) {\n  n &lt;- if (is.data.frame(.data)) colnames(.data) else .data\n  n &lt;- gsub(\"cvrisk\", \"CVrisk\", n , ignore.case=T)\n  n &lt;- gsub(\"hbo\", \"HBO\", n , ignore.case=T)\n  n &lt;- gsub(\"ft4\", \"fT4\", n , ignore.case=T)\n  n &lt;- gsub(\"f_t4\", \"fT4\", n , ignore.case=T)\n  n &lt;- gsub(\"ft3\", \"fT3\", n , ignore.case=T)\n  n &lt;- gsub(\"f_t3\", \"fT3\", n , ignore.case=T)\n  n &lt;- gsub(\"ldl\", \"LDL\", n , ignore.case=T)\n  n &lt;- gsub(\"hdl\", \"HDL\", n , ignore.case=T)\n  n &lt;- gsub(\"hba1c\", \"HbA1C\", n, ignore.case=T)\n  n &lt;- gsub(\"hbac1\", \"HbA1C\", n, ignore.case=T)\n  n &lt;- gsub(\"hb_ac1\", \"HbA1C\",n,ignore.case=T)\n  n &lt;- gsub(\"\\\\igf\\\\b\", \"IGF\", n , ignore.case=T)\n  n &lt;- gsub(\"tsh\", \"TSH\", n , ignore.case=T)\n  n &lt;- gsub(\"acth\", \"ACTH\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\Na\\\\b\", \"Sodio\", n)\n  n &lt;- gsub(\"\\\\K\\\\b\",  \"Potassio\", n)\n  n &lt;- gsub(\"\\\\P\\\\b\",  \"Fosforo\", n)\n  n &lt;- gsub(\"\\\\pas\\\\b\", \"PAS\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\pad\\\\b\", \"PAD\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\pth\\\\b\", \"PTH\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\clu\\\\b\", \"CLU\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\tg\\\\b\", \"TG\", n, ignore.case=T)\n  n &lt;- gsub(\"\\\\glic\\\\b\", \"glicemia\", n, ignore.case=T)\n  if (unique) n &lt;- make.unique(n, sep = \"_\")\n  if (is.data.frame(.data)) {\n    colnames(.data) &lt;- n\n    .data\n  } else {\n    n\n  }\n}\n\n\ndata_clean &lt;- clinical_names(data_fixed)\n\ncomparison &lt;- cbind(data.frame((colnames(data))),\n                        data.frame((colnames(data_fixed))),\n                        data.frame((colnames(data_clean))))\n\ncolnames(comparison) &lt;- c(\"original\",\"fixed\",\"clean\") \n\ncomparison\n\n           original             fixed             clean\n1          paziente          paziente          paziente\n2               età               eta               eta\n3               SEX               sex               sex\n4          diagnosi          diagnosi          diagnosi\n5           terapia           terapia           terapia\n6             tempo             tempo             tempo\n7            Cvrisk            cvrisk            CVrisk\n8              peso              peso              peso\n9        delta Peso        delta_peso        delta_peso\n10              BMI               bmi               bmi\n11         deltaBMI         delta_bmi         delta_bmi\n12              PAS               pas               PAS\n13         deltaPas         delta_pas         delta_PAS\n14              pad               pad               PAD\n15         deltaPad         delta_pad         delta_PAD\n16              HBO               hbo               HBO\n17           neutro            neutro            neutro\n18            linfo             linfo             linfo\n19             glic              glic          glicemia\n20    deltaglicemia     deltaglicemia     deltaglicemia\n21            HBAC1             hbac1             HbA1C\n22       deltaHbAc1      delta_hb_ac1       delta_HbA1C\n23            sodio             sodio             sodio\n24         potassio          potassio          potassio\n25           calcio            calcio            calcio\n26          fosforo           fosforo           fosforo\n27      colesterolo       colesterolo       colesterolo\n28 deltaColesterolo delta_colesterolo delta_colesterolo\n29              HDL               hdl               HDL\n30         deltaHDL         delta_hdl         delta_HDL\n31              ldl               ldl               LDL\n32         deltaLDL         delta_ldl         delta_LDL\n33               TG                tg                tg\n34          deltaTG          delta_tg          delta_tg\n35             ACTH              acth              ACTH\n36        cortisolo         cortisolo         cortisolo\n37              CLU               clu               CLU\n38              IGF               igf               IGF\n39              TSH               tsh               TSH\n40              fT4              f_t4               fT4\n41              PTH               pth               PTH\n42       Vitamina D        vitamina_d        vitamina_d\n43          dose_CA           dose_ca           dose_ca\n44          dose_HC           dose_hc           dose_hc\n45          dose_PL           dose_pl           dose_pl\n46 dose equivalente  dose_equivalente  dose_equivalente\n\n\n\n\n\n\nReferences\n\nFirke, Sam. 2023. “Janitor: Simple Tools for Examining and Cleaning Dirty Data.” https://CRAN.R-project.org/package=janitor.\n\n\nMüller, Kirill. 2020. “Here: A Simpler Way to Find Your Files.” https://CRAN.R-project.org/package=here."
  },
  {
    "objectID": "posts/Snip03/index.html",
    "href": "posts/Snip03/index.html",
    "title": "Snippet #3: Functions for simulating data",
    "section": "",
    "text": "Example of creating variables using runif and rnorm\nWriting a function that wraps all\n\nFirst of all we use the runif and rnorm to have a look how they work.\n\nlibrary(data.table)\nx_min   &lt;- 0\nx_max   &lt;- 10   \nx_step  &lt;- 0.01\n\ny_mean  &lt;- 0.5\ny_sd    &lt;- 0.25\ny_min   &lt;- -1\ny_max   &lt;- 1   \n\nx       &lt;- seq(x_min,x_max,x_step)\nvar_random  &lt;- runif(x,y_min,y_max)\nvar_norm    &lt;- rnorm(x,y_mean,y_sd) \n\ndf  &lt;- data.frame (x,var_random,var_norm)\ndt  &lt;- data.table(df)\n\n\nsimpleDataset &lt;- function(number_of_rows,means,sds)\n{\nl &lt;- length(means)\nres &lt;- lapply(seq(1:l),function(x) \n       eval(\n       parse(\n       text=paste(\"rnorm(\",number_of_rows,\",\",means[x],\",\",sds[x],\")\",sep=\"\"))\n       )\n       ) \ndat &lt;- data.frame((sapply(res,c)))\nid &lt;- rownames(dat)\ndat &lt;-  cbind(id=id,dat)\ndt &lt;- data.table(dat)\nreturn(dt)\n}\n\nExample 1: We simulate the values of the LDL cholesterol of 2 patients in 3 different times. The first one patient (X1) has an average value of 200 of LDL with a standard variation of 2 while the second (X2) has an average of 150 with a standard deviation of 10. Note: All values are expressed in mg/dL\n\ndataset1 &lt;- simpleDataset(3,c(200,180),c(2,10))\ndataset1\n\n   id       X1       X2\n1:  1 203.6588 171.4630\n2:  2 204.2282 183.8984\n3:  3 196.4866 166.2477\n\n\nExample 2: this time we combine runif and simpleDataset. We simulate the values of the LDL cholesterol of 5 patients in 7 different times. The values for each patient are between a min = 100 and a max = 150 with a standard deviation between a min sd = 10 and max sd = 40. We also simulate two time that presents outliers values between a min = 180 and max = 200 and an min sd = 10 and max sd = 40 . We merge the values for each patient (7 times + 2 outliers times) and finally we use the function melt to reshape the dataset.\n\ndat1 &lt;- simpleDataset(number_of_rows=7,\n                      means=runif(5,100,150),\n                      sds=runif(5,10,40))\noutliers &lt;- simpleDataset(number_of_rows=2,\n                      means=runif(5,180,200),\n                      sds=runif(5,10,40))                 \n\ndat1\n\n   id        X1        X2       X3       X4        X5\n1:  1  71.84936  98.36598 141.2255 113.2426  85.93507\n2:  2  89.98873 107.55437 113.9083 128.7742 118.49871\n3:  3 134.42350 118.33060 123.1944 131.7786 113.95603\n4:  4 143.12526  90.45479 133.3862 119.5926  82.27963\n5:  5 119.89442  86.61020 133.3819 127.7598  97.79946\n6:  6 166.94995 150.24227 144.0762 140.2474 105.02831\n7:  7 108.50989  71.80047 135.3942 151.2846 107.16426\n\noutliers\n\n   id       X1       X2       X3       X4       X5\n1:  1 186.5589 223.7572 189.4940 215.1073 191.5749\n2:  2 184.9729 214.7796 201.4495 158.0832 177.9497\n\ndato     &lt;-rbind(dat1,outliers) \ndt.melt &lt;- melt(dat1, id.vars=\"id\")\ncolnames(dt.melt) &lt;- c(\"id\",\"category\",\"var1\")\ndt.melt$ncat &lt;- as.numeric(dt.melt$category)\n\ndt.melt\n\n    id category      var1 ncat\n 1:  1       X1  71.84936    1\n 2:  2       X1  89.98873    1\n 3:  3       X1 134.42350    1\n 4:  4       X1 143.12526    1\n 5:  5       X1 119.89442    1\n 6:  6       X1 166.94995    1\n 7:  7       X1 108.50989    1\n 8:  1       X2  98.36598    2\n 9:  2       X2 107.55437    2\n10:  3       X2 118.33060    2\n11:  4       X2  90.45479    2\n12:  5       X2  86.61020    2\n13:  6       X2 150.24227    2\n14:  7       X2  71.80047    2\n15:  1       X3 141.22552    3\n16:  2       X3 113.90832    3\n17:  3       X3 123.19443    3\n18:  4       X3 133.38618    3\n19:  5       X3 133.38188    3\n20:  6       X3 144.07624    3\n21:  7       X3 135.39417    3\n22:  1       X4 113.24255    4\n23:  2       X4 128.77417    4\n24:  3       X4 131.77862    4\n25:  4       X4 119.59255    4\n26:  5       X4 127.75984    4\n27:  6       X4 140.24743    4\n28:  7       X4 151.28465    4\n29:  1       X5  85.93507    5\n30:  2       X5 118.49871    5\n31:  3       X5 113.95603    5\n32:  4       X5  82.27963    5\n33:  5       X5  97.79946    5\n34:  6       X5 105.02831    5\n35:  7       X5 107.16426    5\n    id category      var1 ncat\n\nstr(dt.melt)\n\nClasses 'data.table' and 'data.frame':  35 obs. of  4 variables:\n $ id      : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ category: Factor w/ 5 levels \"X1\",\"X2\",\"X3\",..: 1 1 1 1 1 1 1 2 2 2 ...\n $ var1    : num  71.8 90 134.4 143.1 119.9 ...\n $ ncat    : num  1 1 1 1 1 1 1 2 2 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "posts/Snip04/index.html",
    "href": "posts/Snip04/index.html",
    "title": "Snippet #4: boxplots and scatterplots: simple recipes",
    "section": "",
    "text": "Simulate data, check and assign data types\nCreate a scatterplot with ggplot\nCreate violin plot with ggstatsplot\n\nExample 1: We want to visualize the difference between two groups of patients that follow two different diets. Group A has an average of total cholesterol of 180 with a standard deviation of 20 while Group B and average of 200 with a standard deviation of 40\n\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(data.table)\n\n\nnpatientsA &lt;- 500\nnpatientsB &lt;- 520\ncholA &lt;- mvrnorm(n=npatientsA, mu=180, Sigma=20, empirical=T)\ncholB &lt;- mvrnorm(n=npatientsB, mu=200, Sigma=40, empirical=T)\n\ndataA &lt;- cbind(cholA,rep(\"A\",npatientsA))  \ndataB &lt;- cbind(cholB,rep(\"B\",npatientsB))  \n\ndata &lt;- data.frame(rbind(dataA,dataB))\ncolnames(data) &lt;- c(\"Cholesterol\",\"group\")\ndata$Cholesterol &lt;- as.numeric(data$Cholesterol)\n\np1 &lt;-ggplot(data, aes(x = group, y = Cholesterol)) + geom_jitter(alpha=0.05) \n\np1\n\n\n\n\nA few observations on the code. First of all, we need to input the data in a data.frame otherwise ggplot will give us an error. The second observation is that since we put chr labels on our groups we needed to define Cholesterol as.numeric in order to avoid unwanted resultsstrange results. Try to comment the line data$Cholesterol &lt;- as.numeric(data$Cholesterol) and you can see by yourself what will happen. (hint: a “labelstorm!”)\nJiiter plots is one of my favorite way to represent data. data and immediately understand the distribution of your data and also avoid the pitfall of boxplot (see (Matejka and Fitzmaurice 2017))\nIf you need inferential statistics on your data another resource is (Patil 2021). See the following example with our data. NOTE that we nee to transform the group label as.factor\n\nlibrary(ggstatsplot)\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\ndata$group &lt;- as.factor(data$group)\n\npstack  &lt;- ggbetweenstats(data,group,Cholesterol)\n                          \npstack    \n\n\n\n\n\n\n\n\nReferences\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/3025453.3025912.\n\n\nPatil, Indrajeet. 2021. “Visualizations with Statistical Details: The ’Ggstatsplot’ Approach” 6: 3167. https://doi.org/10.21105/joss.03167."
  },
  {
    "objectID": "posts/Snip05/Using Shiny Modules with Namespaces.html",
    "href": "posts/Snip05/Using Shiny Modules with Namespaces.html",
    "title": "Namespaces in shiny: Why you need them",
    "section": "",
    "text": "This is the first in a series of tips/tutorials that would have been incredibly helpful to me before writing certain R scripts. Thanks to CHATGPT and the minimal time it takes to organize and transform them into blog posts, I’ve decided to save and write them in this format to keep them on record\nIn Shiny, when you create modules, it’s essential to use namespaces (NS) to ensure that each module instance has unique IDs for its input and output elements. This prevents conflicts and unexpected behavior when working with multiple module instances."
  },
  {
    "objectID": "posts/Snip05/Using Shiny Modules with Namespaces.html#without-using-namespaces-not-recommended",
    "href": "posts/Snip05/Using Shiny Modules with Namespaces.html#without-using-namespaces-not-recommended",
    "title": "Namespaces in shiny: Why you need them",
    "section": "Without Using Namespaces (Not Recommended)",
    "text": "Without Using Namespaces (Not Recommended)\nLet’s start with an example where we create two instances of the same module without using namespaces:\n\nlibrary(shiny)\n\nmy_module &lt;- function(id) {\n  tagList(\n    textInput(inputId = \"text_input\", label = \"Enter text:\"),\n    actionButton(inputId = \"action_button\", label = \"Click me\")\n  )\n}\n\nui &lt;- fluidPage(\n  my_module(\"module_1\"),\n  my_module(\"module_2\")\n)\n\nIn this example, both module_1 and module_2 share the same IDs for input elements (text_input and action_button). If we interact with one module, it will affect the other module as well, leading to unexpected behavior."
  },
  {
    "objectID": "posts/Snip05/Using Shiny Modules with Namespaces.html#using-namespaces-recommended",
    "href": "posts/Snip05/Using Shiny Modules with Namespaces.html#using-namespaces-recommended",
    "title": "Namespaces in shiny: Why you need them",
    "section": "Using Namespaces (Recommended)",
    "text": "Using Namespaces (Recommended)\nNow, let’s use namespaces to create unique IDs for each module instance:\n\nlibrary(shiny)\n\nmy_module &lt;- function(id) {\n  ns &lt;- NS(id)\n  tagList(\n    textInput(inputId = ns(\"text_input\"), label = \"Enter text:\"),\n    actionButton(inputId = ns(\"action_button\"), label = \"Click me\")\n  )\n}\n\nui &lt;- fluidPage(\n  my_module(\"module_1\"),\n  my_module(\"module_2\")\n)\n\nIn this example, we use NS to generate unique namespaces for each module instance (module_1 and module_2). As a result, the input element IDs are unique between instances, ensuring that interactions within one module do not affect the other module.\n\nConclusion\nWhen creating Shiny modules, it’s highly recommended to use namespaces (NS) to prevent ID conflicts between module instances. This practice ensures that each module operates independently and avoids unexpected behavior when working with multiple modules in your Shiny app."
  },
  {
    "objectID": "posts/Fun04/index.html",
    "href": "posts/Fun04/index.html",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "",
    "text": "Dive into the world of coin flip sequences using R simulations. Objective: Understand the probabilities of consecutive sequences of heads or tails. Simulating Coin Flips\nUtilize R to simulate coin flips with varying numbers (100, 1000, 10000, 100000) for experimentation. simulate_tosses function generates sequences of “Heads” and “Tails.” Analyzing Consecutive Sequences\nDefine count_consecutive_sequences function to analyze lengths of consecutive sequences. Obtain counts for different scenarios: 100 tosses, 1000 tosses, 10000 tosses, and 100000 tosses. Visualizing the Results\nUse ggplot2 to create line plots (create_plot function) illustrating consecutive sequence counts. Arrange plots with gridExtra to compare scenarios side by side. Results and Insights\nLonger consecutive sequences are more likely with a larger number of tosses. Simulation provides valuable insights into the probability distribution of sequences. Conclusion\nProbability exploration through simulation is a powerful tool. Whether a statistician, data scientist, or curious individual, simulations offer insights into chance. Encouragement to experiment with parameters and observe changes in results."
  },
  {
    "objectID": "posts/Fun04/index.html#creating-a-color-matrix",
    "href": "posts/Fun04/index.html#creating-a-color-matrix",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Creating a Color Matrix",
    "text": "Creating a Color Matrix\n\n# Image aspect ratio\naspect_ratio <- 1  # You can customize the aspect ratio here\nn_col <- round(sqrt(num_flips) * aspect_ratio)\nn_row <- ceiling(num_flips / n_col)\n\n# Create a color matrix to represent coin flips\ncolors <- ifelse(flips == \"Heads\", \"red\", \"blue\")"
  },
  {
    "objectID": "posts/Fun04/index.html#matrix-manipulation",
    "href": "posts/Fun04/index.html#matrix-manipulation",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Matrix Manipulation",
    "text": "Matrix Manipulation\n\n# Create matrices for Heads and Tails\nheads_matrix <- matrix(0, nrow = n_row, ncol = n_col)\ntails_matrix <- matrix(0, nrow = n_row, ncol = n_col)\n\nfor (i in 1:num_flips) {\n  if (flips[i] == \"Heads\") {\n    heads_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  } else {\n    tails_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  }\n}\n\nWe create matrices heads_matrix and tails_matrix to represent the sequences of heads and tails. These matrices help us analyze the coin flip sequences."
  },
  {
    "objectID": "posts/Fun04/index.html#analyzing-sequences",
    "href": "posts/Fun04/index.html#analyzing-sequences",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Analyzing Sequences",
    "text": "Analyzing Sequences\n\n# Function to calculate the number of consecutive sequences\ncalculate_sequences &lt;- function(matrix) {\n  sequences &lt;- matrix(0, nrow = nrow(matrix), ncol = ncol(matrix))\n  for (i in 1:nrow(matrix)) {\n    count &lt;- 0\n    for (j in 1:ncol(matrix)) {\n      if (matrix[i, j] == 1) {\n        count &lt;- count + 1\n        sequences[i, j] &lt;- count\n      } else {\n        count &lt;- 0\n      }\n    }\n  }\n  return(sequences)\n}\n\nVisualizing Sequences\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads &lt;- calculate_sequences(heads_matrix)\nsequences_tails &lt;- calculate_sequences(tails_matrix)\n\n# Find the longest sequence for Heads and Tails\nlongest_sequence_heads &lt;- max(sequences_heads)\nlongest_sequence_tails &lt;- max(sequences_tails)\n\nWe define a function calculate_sequences that calculates the number of consecutive sequences in a matrix. This function is used to analyze the sequences of heads and tails.\n\n# Create images with sequences and titles\npar(mfrow = c(1, 2))  # Display the two images side by side\nimage(t(sequences_heads), col = viridis(100), main = paste(\"Heads Sequences (Max:\", longest_sequence_heads, \")\"), xaxt = \"n\", yaxt = \"n\")\nimage(t(sequences_tails), col = inferno(100), main = paste(\"Tails Sequences (Max:\", longest_sequence_tails, \")\"), xaxt = \"n\", yaxt = \"n\")\n\n\n\n\nTable Generation\n\nlibrary(knitr)\n\n# Calculate sequence lengths for Heads and Tails\nsequence_lengths_heads &lt;- table(sequences_heads)\nsequence_lengths_tails &lt;- table(sequences_tails)\n\n# Calculate the percentage of sequence lengths\npercentages_heads &lt;- prop.table(sequence_lengths_heads) * 100\npercentages_tails &lt;- prop.table(sequence_lengths_tails) * 100\n\n# Create data frames with lengths, absolute numbers, and percentages\ndataframe_heads &lt;- data.frame(\n  Length = names(sequence_lengths_heads),\n  Absolute_Numbers = as.numeric(sequence_lengths_heads),\n  Percentage = percentages_heads\n)\ndataframe_tails &lt;- data.frame(\n  Length = names(sequence_lengths_tails),\n  Absolute_Numbers = as.numeric(sequence_lengths_tails),\n  Percentage = percentages_tails\n)\n\n# Create formatted tables\nkable(dataframe_heads, caption = \"Table of Heads Sequence Lengths\")\n\n\nTable of Heads Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_heads\nPercentage.Freq\n\n\n\n\n0\n25107\n0\n50.0378667\n\n\n1\n12544\n1\n25.0000000\n\n\n2\n6269\n2\n12.4940210\n\n\n3\n3137\n3\n6.2519930\n\n\n4\n1570\n4\n3.1289860\n\n\n5\n789\n5\n1.5724649\n\n\n6\n368\n6\n0.7334184\n\n\n7\n191\n7\n0.3806601\n\n\n8\n95\n8\n0.1893335\n\n\n9\n55\n9\n0.1096142\n\n\n10\n27\n10\n0.0538106\n\n\n11\n12\n11\n0.0239158\n\n\n12\n6\n12\n0.0119579\n\n\n13\n2\n13\n0.0039860\n\n\n14\n2\n14\n0.0039860\n\n\n15\n1\n15\n0.0019930\n\n\n16\n1\n16\n0.0019930\n\n\n\n\nkable(dataframe_tails, caption = \"Table of Tails Sequence Lengths\")\n\n\nTable of Tails Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_tails\nPercentage.Freq\n\n\n\n\n0\n25245\n0\n50.3128986\n\n\n1\n12535\n1\n24.9820631\n\n\n2\n6143\n2\n12.2429050\n\n\n3\n3130\n3\n6.2380421\n\n\n4\n1570\n4\n3.1289860\n\n\n5\n779\n5\n1.5525351\n\n\n6\n376\n6\n0.7493622\n\n\n7\n188\n7\n0.3746811\n\n\n8\n98\n8\n0.1953125\n\n\n9\n53\n9\n0.1056282\n\n\n10\n28\n10\n0.0558036\n\n\n11\n13\n11\n0.0259088\n\n\n12\n7\n12\n0.0139509\n\n\n13\n4\n13\n0.0079719\n\n\n14\n3\n14\n0.0059790\n\n\n15\n2\n15\n0.0039860\n\n\n16\n2\n16\n0.0039860\n\n\n\n\n\nWe use the knitr library to generate tables that display the lengths, absolute numbers, and percentages of the sequences for both heads and tails."
  },
  {
    "objectID": "posts/Fun04/index.html#visualizing-sequences",
    "href": "posts/Fun04/index.html#visualizing-sequences",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Visualizing Sequences",
    "text": "Visualizing Sequences\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Find the longest sequence for Heads and Tails\nlongest_sequence_heads <- max(sequences_heads)\nlongest_sequence_tails <- max(sequences_tails)\n\nWe define a function calculate_sequences that calculates the number of consecutive sequences in a matrix. This function is used to analyze the sequences of heads and tails."
  },
  {
    "objectID": "posts/Fun04/index.html#creating-images",
    "href": "posts/Fun04/index.html#creating-images",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Creating Images",
    "text": "Creating Images\n\n# Create images with sequences and titles\npar(mfrow = c(1, 2))  # Display the two images side by side\nimage(t(sequences_heads), col = viridis(100), main = paste(\"Heads Sequences (Max:\", longest_sequence_heads, \")\"), xaxt = \"n\", yaxt = \"n\")\nimage(t(sequences_tails), col = inferno(100), main = paste(\"Tails Sequences (Max:\", longest_sequence_tails, \")\"), xaxt = \"n\", yaxt = \"n\")"
  },
  {
    "objectID": "posts/Fun04/index.html#table-generation",
    "href": "posts/Fun04/index.html#table-generation",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Table Generation",
    "text": "Table Generation\n\nlibrary(knitr)\n\n# Calculate sequence lengths for Heads and Tails\nsequence_lengths_heads <- table(sequences_heads)\nsequence_lengths_tails <- table(sequences_tails)\n\n# Calculate the percentage of sequence lengths\npercentages_heads <- prop.table(sequence_lengths_heads) * 100\npercentages_tails <- prop.table(sequence_lengths_tails) * 100\n\n# Create data frames with lengths, absolute numbers, and percentages\ndataframe_heads <- data.frame(\n  Length = names(sequence_lengths_heads),\n  Absolute_Numbers = as.numeric(sequence_lengths_heads),\n  Percentage = percentages_heads\n)\ndataframe_tails <- data.frame(\n  Length = names(sequence_lengths_tails),\n  Absolute_Numbers = as.numeric(sequence_lengths_tails),\n  Percentage = percentages_tails\n)\n\n# Create formatted tables\nkable(dataframe_heads, caption = \"Table of Heads Sequence Lengths\")\n\n\nTable of Heads Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_heads\nPercentage.Freq\n\n\n\n\n0\n25107\n0\n50.0378667\n\n\n1\n12544\n1\n25.0000000\n\n\n2\n6269\n2\n12.4940210\n\n\n3\n3137\n3\n6.2519930\n\n\n4\n1570\n4\n3.1289860\n\n\n5\n789\n5\n1.5724649\n\n\n6\n368\n6\n0.7334184\n\n\n7\n191\n7\n0.3806601\n\n\n8\n95\n8\n0.1893335\n\n\n9\n55\n9\n0.1096142\n\n\n10\n27\n10\n0.0538106\n\n\n11\n12\n11\n0.0239158\n\n\n12\n6\n12\n0.0119579\n\n\n13\n2\n13\n0.0039860\n\n\n14\n2\n14\n0.0039860\n\n\n15\n1\n15\n0.0019930\n\n\n16\n1\n16\n0.0019930\n\n\n\n\nkable(dataframe_tails, caption = \"Table of Tails Sequence Lengths\")\n\n\nTable of Tails Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_tails\nPercentage.Freq\n\n\n\n\n0\n25245\n0\n50.3128986\n\n\n1\n12535\n1\n24.9820631\n\n\n2\n6143\n2\n12.2429050\n\n\n3\n3130\n3\n6.2380421\n\n\n4\n1570\n4\n3.1289860\n\n\n5\n779\n5\n1.5525351\n\n\n6\n376\n6\n0.7493622\n\n\n7\n188\n7\n0.3746811\n\n\n8\n98\n8\n0.1953125\n\n\n9\n53\n9\n0.1056282\n\n\n10\n28\n10\n0.0558036\n\n\n11\n13\n11\n0.0259088\n\n\n12\n7\n12\n0.0139509\n\n\n13\n4\n13\n0.0079719\n\n\n14\n3\n14\n0.0059790\n\n\n15\n2\n15\n0.0039860\n\n\n16\n2\n16\n0.0039860\n\n\n\n\n\nWe use the knitr library to generate tables that display the lengths, absolute numbers, and percentages of the sequences for both heads and tails."
  },
  {
    "objectID": "posts/Fun04/index.html#conclusion",
    "href": "posts/Fun04/index.html#conclusion",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored a fascinating R code snippet that simulates coin flips and analyzes the resulting sequences. We visualized the sequences and created tables to gain insights into the distribution of sequence lengths. This code is a great starting point for exploring and visualizing sequential data in R.\nFeel free to try this code on your own and customize it to suit your needs. Happy coding!"
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html",
    "href": "posts/Tut1-Ita/Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Introduction to EDA: Exploratory Data Analysis (EDA) is the initial step in data analysis, where the structure, trends, and characteristics of the data are understood before applying complex statistical models or machine learning algorithms.\nImportance of EDA: EDA serves various purposes, including understanding data, detecting patterns, data cleaning, feature engineering, generating hypotheses, and communicating insights to stakeholders.\nGenerating Simulated Data: Simulated data is essential for practicing EDA. Parameters and variables are defined to create sample data for further analysis in R.\nChoosing the Right Plot: The choice of visualization depends on the data’s nature and the analysis goals. Time series plots, distribution plots, and correlation plots are used to explore different aspects of data.\n5 .Interactive Data Visualization: Interactive visualizations, like those created using Plotly, enable users to explore data interactively by selecting elements, zooming, and getting detailed information, enhancing data exploration and presentation."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#importanza-delleda",
    "href": "posts/Tut1-Ita/Visualization.html#importanza-delleda",
    "title": "Data Visualization",
    "section": "Importanza dell’EDA",
    "text": "Importanza dell’EDA\nL’EDA ha diversi scopi:\n\nComprendere i Dati: L’EDA ci aiuta a familiarizzare con il dataset, identificare le variabili disponibili e comprenderne la natura (numerica, categorica, temporale, ecc.).\nRilevare Pattern: L’EDA ci consente di individuare pattern, relazioni e potenziali valori anomali nei dati. Questo è fondamentale per prendere decisioni informate durante l’analisi.\nPulizia dei Dati: Attraverso l’EDA, possiamo individuare valori mancanti, valori anomali o incongruenze nei dati che richiedono pulizia e preelaborazione.\nIngegneria delle Feature: L’EDA può suggerire opportunità di ingegneria delle feature, come la creazione di nuove variabili o la trasformazione di quelle esistenti per rappresentare meglio i dati sottostanti.\nGenerazione di Ipotesi: Spesso, l’EDA porta alla generazione di ipotesi o domande di ricerca che guidano ulteriori indagini.\nComunicazione delle Intuizioni: L’EDA produce visualizzazioni e riassunti che agevolano la comunicazione delle intuizioni agli interessati o ai membri del team.\n\nNelle sezioni seguenti, approfondiremo gli aspetti pratici dell’EDA, iniziando con la simulazione dei dati e le tecniche di visualizzazione"
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#scelta-del-grafico-giusto",
    "href": "posts/Tut1-Ita/Visualization.html#scelta-del-grafico-giusto",
    "title": "Data Visualization",
    "section": "Scelta del Grafico Giusto",
    "text": "Scelta del Grafico Giusto\nLa scelta della visualizzazione dipende dalla natura dei tuoi dati e dagli aspetti specifici che desideri evidenziare. In generale, nell’EDA spesso dobbiamo:\n\nEsaminare le Variazioni nel Tempo: Utilizzare i grafici delle serie temporali quando si desidera valutare le variazioni nel tempo di una o più variabili.\nVerificare la Distribuzione dei Dati: Creare grafici di distribuzione, come istogrammi e grafici di densità, per comprendere come sono distribuiti i punti dati.\nEsplorare le Relazioni tra le Variabili: Utilizzare grafici di correlazione e grafici a dispersione per identificare relazioni lineari tra le variabili.\n\nIniziamo esaminando questi aspetti uno per uno utilizzando il nostro dataset simulato.\n\nGrafici delle Serie Temporali\nPer esplorare le variazioni nel tempo, creeremo un grafico delle serie temporali per la variabile var_sin. Questa variabile rappresenta una sinusoide ed è adatta per una rappresentazione a serie temporali. Ecco il codice R per creare un grafico delle serie temporali:\n\nlibrary(ggplot2)\n\np <- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\nIn questo codice, utilizziamo ggplot2 per creare un grafico a linea per la variabile var_sin.\n\nGrafici di Distribuzione\nPer verificare la distribuzione dei dati, creeremo grafici a istogramma per ciascuna delle variabili: var_random, var_norm, e var_sin. Gli istogrammi forniscono una rappresentazione visuale della distribuzione della frequenza dei valori dei dati. Ecco il codice R:\n\np3 <- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 <- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\np4\n\n\n\np5\n\n\n\n\nQuesti grafici ci aiuteranno a comprendere le caratteristiche di distribuzione delle nostre variabili.\n\n\nGrafici di Correlazione\nI grafici di correlazione ci consentono di esaminare le relazioni tra le variabili. Creeremo grafici a dispersione per coppie di variabili al fine di valutare la loro correlazione lineare. Ecco un esempio per var_sin e var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  <- runif(x,y_min,y_max)\nvar_norm2    <- rnorm(x,y_mean,y_sd) \nvar_sin2     <- sin(x) + rnorm(x,0,0.01) \n\ndf2<- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 <- data.table(df2)\n\np10 <- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\nQuesti grafici a dispersione ci aiutano a identificare se le variabili mostrano una correlazione lineare.\nNelle sezioni seguenti, approfondiremo ulteriormente ciascun tipo di grafico, interpreteremo i risultati e esploreremo ulteriori tecniche di visualizzazione per l’EDA.\n\n\nGrafici a Barre (Box Plots)\nI grafici a barre, noti anche come box plot, forniscono un riepilogo della distribuzione dei dati, inclusa la mediana, i quartili e i potenziali valori anomali. Sono particolarmente utili per confrontare le distribuzioni di diverse variabili o gruppi. Ecco un esempio di creazione di grafici a barre per var_random, var_norm, e var_sin:\n\np6 <- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\nI box plot possono rivelare le variazioni e le tendenze centrali delle variabili.\n\n\nPair Plots\nI pair plot, o matrici di scatterplot, ci permettono di visualizzare le relazioni tra coppie di variabili in un dataset. Sono utili per identificare correlazioni e modelli tra variabili contemporaneamente. Ecco come creare un pair plot per il nostro dataset::\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot <- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\n\n\n\nI plot di coppia forniscono una visione completa delle interazioni tra le variabili.\nScomposizione delle Serie Temporali\nI dati delle serie temporali spesso contengono componenti sottostanti come trend e stagionalità che possono essere cruciali per comprendere il comportamento dei dati. La scomposizione delle serie temporali è una tecnica utilizzata nell’Analisi Esplorativa dei Dati (EDA) per separare queste componenti. In questa sezione, dimostreremo come eseguire la scomposizione delle serie temporali utilizzando i nostri dati simulati di var_sin.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp <- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\nIl codice sopra svolge le seguenti operazioni:\n\nScompone la serie temporale var_sin utilizzando la funzione decompose. Specifica una frequenza di 365 poiché i dati rappresentano osservazioni giornaliere.\nPlotta le componenti scomposte, compresa la serie temporale originale, il trend, la componente stagionale e il residuo.\n\nIl grafico risultante mostrerà le singole componenti della serie temporale, consentendoci di ottenere informazioni sulle sue tendenze sottostanti.\n\n\nVisualizzazioni Interattive\nI grafici interattivi, creati utilizzando librerie come plotly o shiny, consentono agli utenti di esplorare i dati in modo interattivo. È possibile creare grafici a dispersione interattivi, grafici a linee o mappe di calore, migliorando la capacità dell’utente di approfondire l’analisi dei dati.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot <- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"<br>var_random:\", var_random, \"<br>var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %>%\n  add_markers() %>%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn questa sezione iniziale, abbiamo introdotto i concetti fondamentali dell’analisi esplorativa dei dati (EDA) e l’importanza della visualizzazione dei dati nel comprendere complessi insiemi di dati. Abbiamo esaminato vari tipi di grafici e le loro applicazioni nell’EDA.\nOra, approfondiamo ulteriormente e miglioriamo la nostra comprensione dimostrando esempi pratici di EDA utilizzando set di dati del mondo reale. Mostreremo come diversi tipi di grafici e visualizzazioni interattive possano fornire preziose intuizioni e guidare decisioni basate sui dati.\nIniziamo questo viaggio nell’EDA e scopriamo le storie nascoste nei nostri dati attraverso esempi pratici."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#esempi",
    "href": "posts/Tut1-Ita/Visualization.html#esempi",
    "title": "Data Visualization",
    "section": "Esempi",
    "text": "Esempi"
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#decomposizione-delle-serie-temporali-per-ottenere-intuizioni",
    "href": "posts/Tut1-Ita/Visualization.html#decomposizione-delle-serie-temporali-per-ottenere-intuizioni",
    "title": "Data Visualization",
    "section": "Decomposizione delle serie temporali per ottenere intuizioni",
    "text": "Decomposizione delle serie temporali per ottenere intuizioni\n\nIntroduzione\nIn questa sezione, approfondiremo un significativo esempio di decomposizione delle serie temporali per dimostrare la sua utilità pratica nell’Analisi Esplorativa dei Dati (EDA). La decomposizione delle serie temporali ci consente di estrarre intuizioni preziose da dati dipendenti dal tempo. Utilizzeremo la nostra serie temporale simulata var_sin per illustrarne l’importanza.\n\n\nScenario: Analisi dei dati giornalieri sulle temperature\nImmagina di avere dati giornalieri sulle temperature di una città per diversi anni. Vogliamo comprendere i modelli sottostanti nelle variazioni di temperatura, inclusi trend e stagionalità, al fine di prendere decisioni informate legate alle previsioni del tempo, al monitoraggio del clima o alla gestione dell’energia.\nCreiamo il dataset arricchito con dati sulle temperature di diverse città. Utilizzeremo la libreria data.table per gestire il dataset in modo efficiente:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities <- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date <- as.Date(\"2010-01-01\")\nend_date <- as.Date(\"2019-12-31\")\ndate_seq <- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data <- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature <- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp <- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\nAbbiamo generato dati sulle temperature per ciascuna città nel corso di dieci anni, ottenendo un dataset diversificato e complesso.\n\n\nEseguire la Decomposizione delle Serie Temporali\nOra che abbiamo il nostro dataset sulle temperature di diverse città, eseguiamo la decomposizione delle serie temporali per analizzare i trend e la stagionalità delle temperature per una delle città, come New York (vedi il grafico).\n\n\nInterpretazione\nIl grafico mostrerà le componenti della serie temporale per New York, inclusa la serie temporale originale, il trend, la componente stagionale e il residuo. Analisi simili possono essere eseguite per altre città al fine di identificare modelli di temperatura regionali.\n\n\nInsights e Applicazioni\nCon il nostro dataset avanzato sulle temperature di diverse città e la decomposizione delle serie temporali, possiamo:\n\nAnalisi Regionale: Confrontare i modelli di temperatura tra diverse città per identificare variazioni regionali.\nIntuizioni Stagionali: Comprendere come la stagionalità delle temperature differisce tra città e regioni.\nTendenze a Lungo Termine: Analizzare le tendenze delle temperature per ciascuna città nel corso dei dieci anni.\n\nQuesta analisi avanzata ci aiuta a prendere decisioni informate legate al monitoraggio del clima, alla pianificazione urbana e alla gestione delle risorse."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#sfruttare-i-grafici-di-distribuzione-per-unanalisi-approfondita",
    "href": "posts/Tut1-Ita/Visualization.html#sfruttare-i-grafici-di-distribuzione-per-unanalisi-approfondita",
    "title": "Data Visualization",
    "section": "Sfruttare i Grafici di Distribuzione per un’Analisi Approfondita",
    "text": "Sfruttare i Grafici di Distribuzione per un’Analisi Approfondita\n\nIntroduzione\nIn questa sezione, illustreremo l’importanza dei grafici di distribuzione nell’Analisi Esplorativa dei Dati (EDA) prendendo in considerazione uno scenario pratico. I grafici di distribuzione ci aiutano a capire come sono distribuiti i punti dati e possono rivelare informazioni sulle caratteristiche sottostanti dei dati. Utilizzeremo il nostro dataset simulato e ci concentreremo sulla variabile var_random.\n\n\nScenario: Analisi dei Risultati degli Esami\nImmagina di avere un dataset contenente i voti degli esami degli studenti di una classe. Vogliamo ottenere intuizioni sulla distribuzione dei voti degli esami per rispondere a domande come:\n\nQual è il voto tipico dell’esame?\nI voti degli esami sono distribuiti normalmente?\nCi sono valori anomali o modelli insoliti nei voti?\n\n\n\nAnalisi della Distribuzione dei Voti degli Esami\nCreiamo un istogramma per visualizzare la distribuzione dei voti degli esami utilizzando la variabile var_random. Ciò ci aiuterà a rispondere alle domande poste in precedenza.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\nInterpretazione\nL’istogramma risultante mostrerà la distribuzione dei voti degli esami. Ecco cosa possiamo interpretare:\n\nVoto Tipico dell’Esame: L’istogramma mostrerà dove si trova la maggior parte dei voti degli esami, indicando il valore tipico o centrale.\nForma della Distribuzione: Possiamo valutare se i voti seguono una distribuzione normale, sono asimmetrici o presentano altre caratteristiche uniche.\nValori Anomali: Gli outliers, se presenti, appariranno come punti dati lontani dalla parte centrale della distribuzione.\n\n\n\nIntuizioni e Applicazioni\nAnalizzando la distribuzione dei voti degli esami, possiamo:\n\nIdentificare la Tendenza Centrale: Determinare il voto tipico dell’esame, che può essere utile per stabilire dei riferimenti o valutare le prestazioni degli studenti.\nComprendere le Caratteristiche dei Dati: Ottenere intuizioni sulla forma della distribuzione, che fornisce informazioni sulle caratteristiche dei dati.\nIndividuare gli Outliers: Identificare valori anomali o voti insoliti che potrebbero richiedere ulteriori approfondimenti."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#analisi-della-correlazione",
    "href": "posts/Tut1-Ita/Visualization.html#analisi-della-correlazione",
    "title": "Data Visualization",
    "section": "Analisi della Correlazione",
    "text": "Analisi della Correlazione\nIn questa sezione, esploreremo un’analisi della correlazione avanzata utilizzando dataset più complessi. Creeremo due dataset: uno che rappresenta le prestazioni accademiche degli studenti e l’altro contenente informazioni sulle loro abitudini di studio e attività extrascolastiche. Investigheremo le correlazioni tra vari fattori per ottenere intuizioni più approfondite.\n\nCreazione di Dataset Complessi\nCreiamo i due dataset complessi per la nostra analisi della correlazione:\nDataset delle Prestazioni Accademiche:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students <- 500\n\nacademic_data <- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data <- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\nAnalisi Avanzata della Correlazione\nOra che abbiamo i nostri dataset complessi, eseguiamo un’analisi avanzata della correlazione per esplorare le relazioni tra le prestazioni accademiche, le abitudini di studio e le attività extrascolastiche. Calcoleremo le correlazioni e le visualizzeremo utilizzando una mappa di calore (heatmap):\n\n# Calculate correlations between variables\ncorrelation_matrix <- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\nInterpretazione\nIl grafico a calore risultante rappresenta visivamente le correlazioni tra le prestazioni accademiche e i fattori legati allo studio. Ecco come possiamo interpretare:\n\nIntensità del Colore: L’intensità del colore indica la forza e la direzione della correlazione. Le correlazioni positive sono mostrate in blu, mentre le correlazioni negative sono in rosso. Più scuro è il colore, più forte è la correlazione.\nCoefficienti di Correlazione: Il grafico a calore mostra i coefficienti di correlazione effettivi come etichette nel triangolo inferiore. Questi valori vanno da -1 (correlazione negativa perfetta) a 1 (correlazione positiva perfetta), con 0 che indica l’assenza di correlazione.\n\n\n\nOsservazioni e Applicazioni\nConducento un’analisi avanzata delle correlazioni, possiamo:\n\nComprendere Relazioni Complesse: Esplorare correlazioni intricate tra le prestazioni accademiche, le ore di studio, le attività extracurriculari e i livelli di stress.\nIdentificare Fattori Chiave: Determinare quali fattori hanno il maggior impatto sulle prestazioni accademiche.\nOttimizzare il Supporto agli Studenti: Utilizzare le informazioni per fornire supporto mirato e interventi agli studenti.\n\nL’analisi avanzata delle correlazioni ci aiuta a scoprire relazioni sfumate all’interno di dataset complessi."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#esplorazione-di-diverse-distribuzioni-con-i-box-plot",
    "href": "posts/Tut1-Ita/Visualization.html#esplorazione-di-diverse-distribuzioni-con-i-box-plot",
    "title": "Data Visualization",
    "section": "Esplorazione di Diverse Distribuzioni con i Box Plot",
    "text": "Esplorazione di Diverse Distribuzioni con i Box Plot\nIn questa sezione, esploreremo la versatilità dei box plot lavorando con dataset diversificati e complessi. Creeremo due dataset: uno rappresenta la distribuzione delle vendite mensili per varie categorie di prodotti, e l’altro contiene informazioni sulle caratteristiche demografiche dei clienti. Questi dataset ci permetteranno di visualizzare vari tipi di distribuzioni e identificare i valori anomali.\n\nCreazione di Dataset Complessi\nCreiamo i due dataset complessi per la nostra analisi con i box plot:\nDataset delle Vendite e Dataset delle Caratteristiche Demografiche dei Clienti:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months <- 24\nproduct_categories <- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data <- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers <- 300\n\ndemographics_data <- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 <- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 <- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\nInterpretazione\nQuesti box plot ci aiutano a ottenere informazioni sulle diverse distribuzioni:\n\nDistribuzione delle Vendite: Possiamo osservare come le vendite siano distribuite tra diverse categorie di prodotti, identificando variazioni e potenziali valori anomali.\nDistribuzione dell’Età dei Clienti: Il box plot mostra la diffusione delle età dei clienti, mettendo in evidenza la tendenza centrale e potenziali valori anomali.\n\n\n\nOsservazioni e Applicazioni\nUtilizzando i box plot con dataset complessi, possiamo:\n\nAnalizzare Distribuzioni Diverse: Visualizzare e confrontare le distribuzioni delle vendite per varie categorie di prodotti e le distribuzioni dell’età dei clienti.\nIndividuazione di Valori Anomali: Identificare potenziali valori anomali sia nei dati sulle vendite che nelle informazioni demografiche dei clienti.\nOsservazioni sulla Segmentazione: Comprendere come le vendite variano tra le categorie di prodotti e la distribuzione per età dei clienti.\n\nI box plot sono strumenti versatili per esplorare vari tipi di distribuzioni dei dati e prendere decisioni basate sui dati."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#visualizzazione-interattiva-dei-dati-con-plotly",
    "href": "posts/Tut1-Ita/Visualization.html#visualizzazione-interattiva-dei-dati-con-plotly",
    "title": "Data Visualization",
    "section": "Visualizzazione Interattiva dei Dati con Plotly",
    "text": "Visualizzazione Interattiva dei Dati con Plotly\n\nCreazione di un Grafico Interattivo delle Serie Storiche\nSupponiamo di avere un dataset contenente i prezzi mensili delle azioni di tre aziende: Azienda A, Azienda B e Azienda C. Vogliamo creare un grafico interattivo delle serie storiche che permetta agli utenti di:\n\nScegliere l’azienda che desiderano visualizzare.\nIngrandire e ridurre per esplorare periodi temporali specifici.\nPassare il mouse sui punti dati per visualizzare informazioni dettagliate.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months <- 24\n\ntime_series_data <- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot <- plot_ly(data = time_series_data, x = ~Date) %>%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %>%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\nFunzionalità di Interazione\nIl grafico interattivo delle serie storiche creato con Plotly offre le seguenti funzionalità di interazione:\n\nSelezione: Gli utenti possono fare clic sulla legenda per selezionare/deselezionare aziende specifiche da visualizzare.\nZoom: Gli utenti possono fare clic e trascinare per ingrandire un periodo di tempo specifico.\nInformazioni al Passaggio del Mouse: Posizionando il puntatore del mouse sui punti dati, vengono visualizzate informazioni dettagliate sul punto dati selezionato.\n\n\n\nOsservazioni e Applicazioni\nLe visualizzazioni interattive con Plotly sono preziose per:\n\nEsplorazione: Gli utenti possono esplorare interattivamente dataset complessi e concentrarsi su aspetti specifici dei dati.\nComunicazione dei Dati: La presentazione dei dati in un formato interattivo migliora la comunicazione e l’interazione.\nSupporto alle Decisioni: I grafici interattivi possono essere utilizzati nei processi decisionali in cui gli utenti devono esplorare la dinamica dei dati.\n\nLe visualizzazioni interattive dei dati sono uno strumento potente per l’EDA e la presentazione dei dati. Nella sezione successiva, esploreremo un’altra tecnica di visualizzazione avanzata: la decomposizione delle serie storiche.\n[Wickham (2016)](Schloerke et al. 2021)"
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#importance-of-eda",
    "href": "posts/Tut1-Ita/Visualization.html#importance-of-eda",
    "title": "Data Visualization",
    "section": "Importance of EDA",
    "text": "Importance of EDA\nEDA serves several purposes:\n\nUnderstanding Data: EDA helps us become familiar with the dataset, identify the available variables, and understand their nature (numeric, categorical, temporal, etc.).\nDetecting Patterns: EDA allows us to detect patterns, relationships, and potential outliers within the data. This is critical for making informed decisions during the analysis.\nData Cleaning: Through EDA, we can identify missing values, outliers, or data inconsistencies that require cleaning and preprocessing.\nFeature Engineering: EDA may suggest feature engineering opportunities, such as creating new variables or transforming existing ones to better represent the underlying data.\nHypothesis Generation: EDA often leads to the generation of hypotheses or research questions, guiding further investigation.\nCommunicating Insights: EDA produces visualizations and summaries that facilitate the communication of insights to stakeholders or team members.\n\nIn the following sections, we will delve into the practical aspects of EDA, starting with data simulation and visualization techniques."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#generating-simulated-data",
    "href": "posts/Tut1-Ita/Visualization.html#generating-simulated-data",
    "title": "Data Visualization",
    "section": "Generating Simulated Data",
    "text": "Generating Simulated Data\nBefore diving into Exploratory Data Analysis (EDA) on real datasets, it’s helpful to begin with the generation of simulated data. This allows us to have full control over the data and create example scenarios to understand key EDA concepts. In this section, we will learn how to generate simulated datasets using R.\n\nParameters and Variables\nTo start, let’s define some basic parameters that we’ll use to generate simulated data:\n\nx_min: The minimum value for the variable x.\nx_max: The maximum value for the variable x.\nx_step: The increment between successive x values.\ny_mean: The mean value for the dependent variable y.\ny_sd: The standard deviation for the dependent variable y.\ny_min: The minimum possible value for y.\ny_max: The maximum possible value for y.\n\nWe will use these parameters to generate sample data.\nNow, let’s proceed to generate sample data based on the defined parameters. In this example, we’ll create a simple dataset with the following variables:\n\nx: A sequence of values ranging from x_min to x_max with an increment of x_step.\nvar_random: A random variable with values uniformly distributed between y_min and y_max.\nvar_norm: A variable with values generated from a normal distribution with mean y_mean and standard deviation y_sd.\nvar_sin: A variable with values generated as the sine function of x.\n\nHere’s the R code to create the sample dataset:\n\nlibrary(data.table)\n\n# Parameters\nx_min   <- 0\nx_max   <- 10   \nx_step  <- 0.01\n\ny_mean  <- 0.5\ny_sd    <- 0.25\ny_min   <- -1\ny_max   <- 1     \n\nx       <- seq(x_min, x_max, x_step)\n\n# Variables\nvar_random  <- runif(length(x), y_min, y_max)\nvar_norm    <- rnorm(length(x), y_mean, y_sd) \nvar_sin     <- sin(x)\n\n# Data.frame \ndf  <- data.frame(x, var_random, var_norm, var_sin)\ndt  <- data.table(df)\n\n# Melt \ndtm <- melt(dt, id.vars=\"x\")\n\nThis code creates a dataset df and a data.table dt containing the generated variables. The melt function from the data.table library is used to reshape the data for visualization purposes.\nWith our simulated data ready, we can now move on to creating various plots and performing EDA.\nIn this section, we will explore various visualization techniques that play a crucial role in Exploratory Data Analysis (EDA). Visualizations help us gain insights into the data’s distribution, patterns, and relationships between variables. We will use the simulated dataset generated in the previous section to illustrate these techniques."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#choosing-the-right-plot",
    "href": "posts/Tut1-Ita/Visualization.html#choosing-the-right-plot",
    "title": "Data Visualization",
    "section": "Choosing the Right Plot",
    "text": "Choosing the Right Plot\nThe choice of visualization depends on the nature of your data and the specific aspects you want to highlight. Generally, in EDA, we often need to:\n\nExamine Changes Over Time: Use time series plots when you want to assess changes in one or more variables over time.\nCheck for Data Distribution: Create distribution plots, such as histograms and density plots, to understand how data points are distributed.\nExplore Variable Relationships: Employ correlation plots and scatter plots to identify linear relationships between variables.\n\nLet’s start by examining these aspects one by one using our simulated dataset.\n\nTime Series Plots\nTo explore changes over time, we’ll create a time series plot for the var_sin variable. This variable represents a sine wave and is well-suited for a time series representation. Here’s the R code to create a time series plot:\n\nlibrary(ggplot2)\n\np <- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\nIn this code, we use ggplot2 to create a line plot for the var_sin variable.\n\n\nDistribution Plots\nTo check the data distribution, we’ll create histogram plots for each of the variables: var_random, var_norm, and var_sin. Histograms provide a visual representation of the frequency distribution of data values. Here’s the R code:\n\np3 <- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 <- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\np4\n\n\n\np5\n\n\n\n\nThese plots will help us understand the distribution characteristics of our variables.\n\n\nCorrelation Plots\nCorrelation plots allow us to examine relationships between variables. We’ll create scatter plots for pairs of variables to assess their linear correlation. Here’s an example for var_sin and var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  <- runif(x,y_min,y_max)\nvar_norm2    <- rnorm(x,y_mean,y_sd) \nvar_sin2     <- sin(x) + rnorm(x,0,0.01) \n\ndf2<- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 <- data.table(df2)\n\np10 <- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\nThese scatter plots help us identify whether variables exhibit linear correlation.\nIn the following sections, we’ll delve deeper into each of these plot types, interpret the results, and explore additional visualization techniques for EDA.\n\nBox Plots\nBox plots, also known as box-and-whisker plots, provide a summary of the data’s distribution, including median, quartiles, and potential outliers. They are particularly useful for comparing the distributions of different variables or groups. Here’s an example of creating box plots for var_random, var_norm, and var_sin:\n\np6 <- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\nBox plots can reveal variations and central tendencies of the variables.\n\n\nPair Plots\nPair plots, or scatterplot matrices, allow us to visualize pairwise relationships between multiple variables in a dataset. They are helpful for identifying correlations and patterns among variables simultaneously. Here’s how to create a pair plot for our dataset:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot <- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\n\n\n\nPair plots provide a comprehensive view of variable interactions.\n\n\nTime Series Decomposition\nTime series data often contain underlying components such as trends and seasonality that can be crucial for understanding the data’s behavior. Time series decomposition is a technique used in Exploratory Data Analysis (EDA) to separate these components. In this section, we’ll demonstrate how to perform time series decomposition using our simulated var_sin data.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp <- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\nThe code above performs the following:\n\nDecomposes the var_sin time series using the decompose function. We specify a frequency of 365 since the data represents daily observations.\nPlots the decomposed components, including the original time series, trend, seasonal component, and residual.\n\nThe resulting plot will show the individual components of the time series, allowing us to gain insights into its underlying patterns.\n\n\nInteractive Visualizations\nInteractive plots, created using libraries like plotly or shiny, allow users to explore data interactively. You can create interactive scatter plots, line plots, or heatmaps, enhancing the user’s ability to dig deeper into the data.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot <- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"<br>var_random:\", var_random, \"<br>var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %>%\n  add_markers() %>%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn this initial section, we’ve introduced the fundamental concepts of exploratory data analysis (EDA) and the importance of data visualization in gaining insights from complex datasets. We’ve explored various types of plots and their applications in EDA.\nNow, let’s dive deeper and enhance our understanding by demonstrating practical examples of EDA using real-world datasets. We’ll showcase how different types of plots and interactive visualizations can provide valuable insights and drive data-driven decisions.\nLet’s embark on this EDA journey and uncover the hidden stories within our data through hands-on examples."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#examples",
    "href": "posts/Tut1-Ita/Visualization.html#examples",
    "title": "Data Visualization",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#time-series-decomposition-for-insights",
    "href": "posts/Tut1-Ita/Visualization.html#time-series-decomposition-for-insights",
    "title": "Data Visualization",
    "section": "Time Series Decomposition for Insights",
    "text": "Time Series Decomposition for Insights\n\nIntroduction\nIn this section, we’ll dive into a meaningful example of time series decomposition to demonstrate its practical utility in Exploratory Data Analysis (EDA). Time series decomposition allows us to extract valuable insights from time-dependent data. We’ll use our simulated var_sin time series to illustrate its significance.\n\n\nScenario: Analyzing Daily Temperature Data\nImagine we have daily temperature data for a city over several years. We want to understand the underlying patterns in temperature variations, including trends and seasonality, to make informed decisions related to weather forecasts, climate monitoring, or energy management.\nLet’s create the enhanced dataset with temperature data for multiple cities. We’ll use the data.table library to manage the dataset efficiently:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities <- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date <- as.Date(\"2010-01-01\")\nend_date <- as.Date(\"2019-12-31\")\ndate_seq <- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data <- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature <- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp <- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\nWe’ve generated temperature data for each city over the span of ten years, resulting in a diverse and complex dataset.\n\n\nPerforming Time Series Decomposition\nNow that we have our multi-city temperature dataset, let’s apply time series decomposition to analyze temperature trends and seasonality for one of the cities, such as New York (see plot)\n\n\nInterpretation\nThe plot will display the components of the time series for New York, including the original time series, trend, seasonal component, and residual. Similar analyses can be performed for other cities to identify regional temperature patterns.\n\n\nInsights and Applications\nWith our enhanced multi-city temperature dataset and time series decomposition, we can:\n\nRegional Analysis: Compare temperature patterns across different cities to identify regional variations.\nSeasonal Insights: Understand how temperature seasonality differs between cities and regions.\nLong-Term Trends: Analyze temperature trends for each city over the ten-year period.\n\nThis advanced analysis helps us make informed decisions related to climate monitoring, urban planning, and resource management."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#leveraging-distribution-plots-for-in-depth-analysis",
    "href": "posts/Tut1-Ita/Visualization.html#leveraging-distribution-plots-for-in-depth-analysis",
    "title": "Data Visualization",
    "section": "Leveraging Distribution Plots for In-Depth Analysis",
    "text": "Leveraging Distribution Plots for In-Depth Analysis\n\nIntroduction\nIn this section, we’ll illustrate the significance of distribution plots in Exploratory Data Analysis (EDA) by considering a practical scenario. Distribution plots help us understand how data points are distributed and can reveal insights about the underlying data characteristics. We’ll use our simulated dataset and focus on the var_random variable.\n\n\nScenario: Analyzing Exam Scores\nImagine we have a dataset containing exam scores of students in a class. We want to gain insights into the distribution of exam scores to answer questions like:\n\nWhat is the typical exam score?\nAre the exam scores normally distributed?\nAre there any outliers or unusual patterns in the scores?\n\n\n\nAnalyzing the Distribution of Exam Scores\nLet’s create a histogram to visualize the distribution of exam scores using the var_random variable. This will help us answer the questions posed above.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 <- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\nInterpretation\nThe resulting histogram will display the distribution of exam scores. Here’s what we can interpret:\n\nTypical Exam Score: The histogram will show where the majority of exam scores lie, indicating the typical or central value.\nDistribution Shape: We can assess whether the scores follow a normal distribution, are skewed, or have other unique characteristics.\nOutliers: Outliers, if present, will appear as data points far from the central part of the distribution.\n\n\n\nInsights and Applications\nBy analyzing the distribution of exam scores, we can:\n\nIdentify Central Tendency: Determine the typical exam score, which can be useful for setting benchmarks or evaluating student performance.\nUnderstand Data Characteristics: Gain insights into the shape of the distribution, which informs us about the data’s characteristics.\nDetect Outliers: Identify outliers or unusual scores that may require further investigation."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#correlation-analysis",
    "href": "posts/Tut1-Ita/Visualization.html#correlation-analysis",
    "title": "Data Visualization",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nIn this section, we’ll explore advanced correlation analysis using more complex datasets. We’ll create two datasets: one representing students’ academic performance, and the other containing information about their study habits and extracurricular activities. We’ll investigate correlations between various factors to gain deeper insights.\n\nCreating Complex Datasets\nLet’s create the two complex datasets for our correlation analysis:\nAcademic Performance Dataset:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students <- 500\n\nacademic_data <- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data <- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\nAdvanced Correlation Analysis\nNow that we have our complex datasets, let’s perform advanced correlation analysis to explore relationships between academic performance, study habits, and extracurricular activities. We’ll calculate correlations and visualize them using a heatmap:\n\n# Calculate correlations between variables\ncorrelation_matrix <- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\nInterpretation\nThe resulting heatmap visually represents the correlations between academic performance and study-related factors. Here’s what we can interpret:\n\nColor Intensity: The color intensity indicates the strength and direction of the correlation. Positive correlations are shown in blue, while negative correlations are in red. The darker the color, the stronger the correlation.\nCorrelation Coefficients: The heatmap displays the actual correlation coefficients as labels in the lower triangle. These values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n\n\n\nInsights and Applications\nBy conducting advanced correlation analysis, we can:\n\nUnderstand Complex Relationships: Explore intricate correlations between academic performance, study hours, extracurricular activities, and stress levels.\nIdentify Key Factors: Determine which factors have the most significant impact on academic performance.\nOptimize Student Support: Use insights to provide targeted support and interventions for students.\n\nAdvanced correlation analysis helps us uncover nuanced relationships within complex datasets."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#exploring-diverse-distributions-with-box-plots",
    "href": "posts/Tut1-Ita/Visualization.html#exploring-diverse-distributions-with-box-plots",
    "title": "Data Visualization",
    "section": "Exploring Diverse Distributions with Box Plots",
    "text": "Exploring Diverse Distributions with Box Plots\nIn this section, we’ll explore the versatility of box plots by working with diverse and complex datasets. We’ll create two datasets: one representing the distribution of monthly sales for multiple product categories, and the other containing information about customer demographics. These datasets will allow us to visualize various types of distributions and identify outliers.\n\nCreating Complex Datasets\nLet’s create the two complex datasets for our box plot analysis:\nSales Dataset and Customer Demographics Dataset:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months <- 24\nproduct_categories <- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data <- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers <- 300\n\ndemographics_data <- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 <- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 <- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\nInterpretation\nThese box plots help us gain insights into diverse distributions:\n\nSales Distribution: We can observe how sales are distributed across different product categories, identifying variations and potential outliers.\nCustomer Age Distribution: The box plot displays the spread of customer ages, highlighting the central tendency and any potential outliers.\n\n\n\nInsights and Applications\nBy using box plots with complex datasets, we can:\n\nAnalyze Diverse Distributions: Visualize and compare distributions of sales for multiple product categories and customer age distributions.\nOutlier Detection: Identify potential outliers in both sales data and customer demographics.\nSegmentation Insights: Understand how sales vary across product categories and the age distribution of customers.\n\nBox plots are versatile tools for exploring various types of data distributions and making data-driven decisions."
  },
  {
    "objectID": "posts/Tut1-Ita/Visualization.html#interactive-data-visualization-with-plotly",
    "href": "posts/Tut1-Ita/Visualization.html#interactive-data-visualization-with-plotly",
    "title": "Data Visualization",
    "section": "Interactive Data Visualization with Plotly",
    "text": "Interactive Data Visualization with Plotly\n\nCreating an Interactive Time Series Plot\nSuppose we have a dataset containing monthly stock prices for three companies: Company A, Company B, and Company C. We want to create an interactive time series plot that allows users to:\n\nSelect the company they want to visualize.\nZoom in and out to explore specific time periods.\nHover over data points to view detailed information.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months <- 24\n\ntime_series_data <- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot <- plot_ly(data = time_series_data, x = ~Date) %>%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %>%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %>%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\nInteraction Features\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\nInsights and Applications\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\nInteractive data visualizations are a powerful tool for EDA and data presentation. In the next section, we’ll explore another advanced visualization technique: time series decomposition.\n[Wickham (2016)](Schloerke et al. 2021)"
  },
  {
    "objectID": "posts/Fun05/index.html",
    "href": "posts/Fun05/index.html",
    "title": "Coins…again",
    "section": "",
    "text": "library(viridis)  # Import the viridis color palette library\n\nLoading required package: viridisLite\n\nlibrary(ggplot2)\n\nset.seed(123)  # Set a seed for reproducibility\nnum_flips <- 10000\nflips <- sample(c(\"Heads\", \"Tails\"), num_flips, replace = TRUE)\n\n# Image aspect ratio\naspect_ratio <- 1  # You can customize the aspect ratio here\nn_col <- round(sqrt(num_flips) * aspect_ratio)\nn_row <- ceiling(num_flips / n_col)\n\n# Create a color matrix to represent coin flips\ncolors <- ifelse(flips == \"Heads\", \"red\", \"blue\")\n\n# Create matrices for Heads and Tails\nheads_matrix <- matrix(0, nrow = n_row, ncol = n_col)\ntails_matrix <- matrix(0, nrow = n_row, ncol = n_col)\n\nfor (i in 1:num_flips) {\n  if (flips[i] == \"Heads\") {\n    heads_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  } else {\n    tails_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  }\n}\n\n# Function to calculate the number of consecutive sequences\ncalculate_sequences <- function(matrix) {\n  sequences <- matrix(0, nrow = nrow(matrix), ncol = ncol(matrix))\n  for (i in 1:nrow(matrix)) {\n    count <- 0\n    for (j in 1:ncol(matrix)) {\n      if (matrix[i, j] == 1) {\n        count <- count + 1\n        sequences[i, j] <- count\n      } else {\n        count <- 0\n      }\n    }\n  }\n  return(sequences)\n}\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Find the longest sequence for Heads and Tails\nlongest_sequence_heads <- max(sequences_heads)\nlongest_sequence_tails <- max(sequences_tails)\n\n# Create images with sequences and titles\npar(mfrow = c(1, 2))  # Display the two images side by side\nimage(t(sequences_heads), col = viridis(100), main = paste(\"Heads Sequences (Max:\", longest_sequence_heads, \")\"), xaxt = \"n\", yaxt = \"n\")\nimage(t(sequences_tails), col = inferno(100), main = paste(\"Tails Sequences (Max:\", longest_sequence_tails, \")\"), xaxt = \"n\", yaxt = \"n\")\n\n\n\nlibrary(knitr)\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Calculate sequence lengths for Heads and Tails\nsequence_lengths_heads <- table(sequences_heads)\nsequence_lengths_tails <- table(sequences_tails)\n\n# Calculate the percentage of sequence lengths\npercentages_heads <- prop.table(sequence_lengths_heads) * 100\npercentages_tails <- prop.table(sequence_lengths_tails) * 100\n\n# Create data frames with lengths, absolute numbers, and percentages\ndataframe_heads <- data.frame(\n  Length = names(sequence_lengths_heads),\n  Absolute_Numbers = as.numeric(sequence_lengths_heads),\n  Percentage = percentages_heads\n)\ndataframe_tails <- data.frame(\n  Length = names(sequence_lengths_tails),\n  Absolute_Numbers = as.numeric(sequence_lengths_tails),\n  Percentage = percentages_tails\n)\n\n# Create formatted tables\nkable(dataframe_heads, caption = \"Table of Heads Sequence Lengths\")\n\n\nTable of Heads Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_heads\nPercentage.Freq\n\n\n\n\n0\n4983\n0\n49.83\n\n\n1\n2510\n1\n25.10\n\n\n2\n1250\n2\n12.50\n\n\n3\n610\n3\n6.10\n\n\n4\n326\n4\n3.26\n\n\n5\n170\n5\n1.70\n\n\n6\n78\n6\n0.78\n\n\n7\n37\n7\n0.37\n\n\n8\n18\n8\n0.18\n\n\n9\n13\n9\n0.13\n\n\n10\n4\n10\n0.04\n\n\n11\n1\n11\n0.01\n\n\n\n\nkable(dataframe_tails, caption = \"Table of Tails Sequence Lengths\")\n\n\nTable of Tails Sequence Lengths\n\n\nLength\nAbsolute_Numbers\nPercentage.sequences_tails\nPercentage.Freq\n\n\n\n\n0\n5017\n0\n50.17\n\n\n1\n2518\n1\n25.18\n\n\n2\n1214\n2\n12.14\n\n\n3\n619\n3\n6.19\n\n\n4\n307\n4\n3.07\n\n\n5\n142\n5\n1.42\n\n\n6\n77\n6\n0.77\n\n\n7\n45\n7\n0.45\n\n\n8\n25\n8\n0.25\n\n\n9\n14\n9\n0.14\n\n\n10\n10\n10\n0.10\n\n\n11\n6\n11\n0.06\n\n\n12\n4\n12\n0.04\n\n\n13\n1\n13\n0.01\n\n\n14\n1\n14\n0.01"
  },
  {
    "objectID": "posts/Fun05/More on Coins.html",
    "href": "posts/Fun05/More on Coins.html",
    "title": "Coins.. again",
    "section": "",
    "text": "In this blog post, we’ll dive into a fun R code snippet that simulates a series of coin flips and analyzes the resulting sequences. We’ll use the viridis and ggplot2 libraries to create visual representations of the sequences and gain insights into the outcomes. Let’s break down the code step by step.\n\n\n\nlibrary(viridis)  # Import the viridis color palette library\nlibrary(ggplot2)\n\nset.seed(123)  # Set a seed for reproducibility\nnum_flips <- 50000\nflips <- sample(c(\"Heads\", \"Tails\"), num_flips, replace = TRUE)\n\n# Image aspect ratio\naspect_ratio <- 1  # You can customize the aspect ratio here\nn_col <- round(sqrt(num_flips) * aspect_ratio)\nn_row <- ceiling(num_flips / n_col)\n\n# Create a color matrix to represent coin flips\ncolors <- ifelse(flips == \"Heads\", \"red\", \"blue\")\n\n# Create matrices for Heads and Tails\nheads_matrix <- matrix(0, nrow = n_row, ncol = n_col)\ntails_matrix <- matrix(0, nrow = n_row, ncol = n_col)\n\nfor (i in 1:num_flips) {\n  if (flips[i] == \"Heads\") {\n    heads_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  } else {\n    tails_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  }\n}\n\n# Function to calculate the number of consecutive sequences\ncalculate_sequences <- function(matrix) {\n  sequences <- matrix(0, nrow = nrow(matrix), ncol = ncol(matrix))\n  for (i in 1:nrow(matrix)) {\n    count <- 0\n    for (j in 1:ncol(matrix)) {\n      if (matrix[i, j] == 1) {\n        count <- count + 1\n        sequences[i, j] <- count\n      } else {\n        count <- 0\n      }\n    }\n  }\n  return(sequences)\n}\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Find the longest sequence for Heads and Tails\nlongest_sequence_heads <- max(sequences_heads)\nlongest_sequence_tails <- max(sequences_tails)\n\n# Create images with sequences and titles\npar(mfrow = c(1, 2))  # Display the two images side by side\nimage(t(sequences_heads), col = viridis(100), main = paste(\"Heads Sequences (Max:\", longest_sequence_heads, \")\"), xaxt = \"n\", yaxt = \"n\")\nimage(t(sequences_tails), col = inferno(100), main = paste(\"Tails Sequences (Max:\", longest_sequence_tails, \")\"), xaxt = \"n\", yaxt = \"n\")\n\nlibrary(knitr)\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Calculate sequence lengths for Heads and Tails\nsequence_lengths_heads <- table(sequences_heads)\nsequence_lengths_tails <- table(sequences_tails)\n\n# Calculate the percentage of sequence lengths\npercentages_heads <- prop.table(sequence_lengths_heads) * 100\npercentages_tails <- prop.table(sequence_lengths_tails) * 100\n\n# Create data frames with lengths, absolute numbers, and percentages\ndataframe_heads <- data.frame(\n  Length = names(sequence_lengths_heads),\n  Absolute_Numbers = as.numeric(sequence_lengths_heads),\n  Percentage = percentages_heads\n)\ndataframe_tails <- data.frame(\n  Length = names(sequence_lengths_tails),\n  Absolute_Numbers = as.numeric(sequence_lengths_tails),\n  Percentage = percentages_tails\n)\n\n# Create formatted tables\nkable(dataframe_heads, caption = \"Table of Heads Sequence Lengths\")\nkable(dataframe_tails, caption = \"Table of Tails Sequence Lengths\")"
  },
  {
    "objectID": "posts/Fun06/index.html",
    "href": "posts/Fun06/index.html",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "",
    "text": "The article delves into the “Birthday Paradox,” a counterintuitive aspect of probability. It utilizes the R programming language to simulate and visualize the paradox, emphasizing how probability can defy expectations\nThe Birthday Paradox involves the likelihood of two people in a group sharing the same birthday, with the actual probability higher than initially perceived. The paradox arises from the multitude of possible combinations of birthdays within a group.\nR code for simulating the Birthday Paradox is provided, illustrating how probabilities change with increasing group size. The simulation involves checking for shared birthdays through multiple iterations to calculate the overall probability.\nDespite intuition, the Birthday Paradox illustrates that the probability of two people sharing a birthday is higher than anticipated."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html",
    "href": "posts/Fun07/Illusion Of Luck.html",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "",
    "text": "Introduction:\n\nExplores the profitability of casinos and introduces a gambling paradox related to the number zero favoring the house.\n\nRoulette Dilemma and Simulating the Game:\n\nDescribes the classic game of roulette and its subtle advantage for the house.\nIntroduces R programming for simulating roulette plays, focusing on predicting red or black outcomes.\nHighlights the impact of the number zero on altering the game dynamics.\n\nplay_roulette Function and Simulating Plays:\n\nDefines the play_roulette function to simulate roulette plays based on specified parameters.\nUtilizes set.seed(123) for reproducibility and simulates plays both with and without a 5% house advantage.\n\nComparing Results and Data Preparation:\n\nCompares total winnings for both scenarios using cat statements.\nPrepares data for plotting, creating a data frame with scenarios and total winnings.\n\nCreating a Bar Plot, The House Edge, and Conclusion:\n\nUtilizes ggplot2 to create a bar plot, visually comparing total winnings for each scenario.\nDiscusses the house edge introduced by modifying payouts.\nConcludes by emphasizing the illusion of luck in casinos, the role of zero, and why the house consistently wins."
  },
  {
    "objectID": "posts/Fun08/Illusion Of Luck.html",
    "href": "posts/Fun08/Illusion Of Luck.html",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "",
    "text": "Introduction: Have you ever wondered why casinos seem to have a mysterious edge, making them consistently profitable? Let’s explore a paradox in the world of gambling, where the number zero takes center stage and helps the house always come out on top.\nThe Roulette Dilemma: Consider the classic game of roulette, a staple in any casino. The thrill of the spinning wheel and the anticipation of where the ball will land attract countless gamblers. However, beneath the excitement lies a subtle advantage that consistently favors the house.\nSimulating the Game: To illustrate this phenomenon, let’s simulate the game of roulette using the R programming language. We’ll focus on a simple bet: predicting whether the ball will land on red or black. In a fair game, the odds of winning such a bet would be 1 to 1. However, the introduction of the number zero alters the dynamics.\n\nlibrary(ggplot2)\n\nplay_roulette <- function(bet, chosen_number, num_spins) {\n  roulette_numbers <- 1:36\n  results <- sample(roulette_numbers, num_spins, replace = TRUE)\n  winnings <- ifelse(results == chosen_number, 36, 0)  # 35 to 1 payout\n  winnings[bet == \"red\" & results %% 2 == 0] <- 2  # win if the color is red\n  winnings[bet == \"black\" & results %% 2 != 0] <- 2  # win if the color is black\n  total_winnings <- sum(winnings)\n  return(total_winnings)\n}\n\n# Simulating plays with and without the house advantage\nset.seed(123)  # Set the seed for reproducibility\nwithout_advantage <- play_roulette(\"red\", 17, 5000)\n\nset.seed(123)  # Reset the same seed for comparison\nwith_advantage <- play_roulette(\"red\", 17, 5000) - 0.05 * 1000  # 5% house advantage\n\n# Comparing results\ncat(\"Without the house advantage: \", without_advantage, \"\\n\")\n\nWithout the house advantage:  10538 \n\ncat(\"With the house advantage: \", with_advantage, \"\\n\")\n\nWith the house advantage:  10488 \n\n# Data for plotting\ndata <- data.frame(\n  Scenario = c(\"Without Advantage\", \"With Advantage\"),\n  Total_Winnings = c(without_advantage, with_advantage)\n)\n\n# Create a bar plot\nggplot(data, aes(x = Scenario, y = Total_Winnings, fill = Scenario)) +\n  geom_bar(stat = \"identity\", position = \"dodge\",fill = c(\"lightblue\", \"papayawhip\")) +\n  labs(title = \"Roulette Simulation: Comparing Outcomes\",\n       x = \"Scenario\",\n       y = \"Total Winnings\") +\n  theme_minimal()\n\n\n\n\nThe House Edge: In our simulation, we introduced a slight modification to the payouts, reducing them just enough to create a 5% advantage for the house. This mirrors the real-world scenario where the presence of zero in roulette gives the casino an edge.\nComparing Outcomes: Running our simulation both with and without the house advantage reveals a stark contrast. The casino consistently comes out ahead in the long run, showcasing how the number zero plays a crucial role in tipping the odds in favor of the house.\nConclusion: The illusion of luck in casinos often stems from subtle yet significant factors, such as the presence of zero in games like roulette. Understanding these nuances can provide valuable insights into the mechanics of gambling and why, ultimately, the house always wins.\nAs you ponder the next spin of the roulette wheel, remember that behind the excitement lies a carefully crafted system where even the seemingly neutral zero plays a pivotal role in ensuring the casino’s enduring success."
  },
  {
    "objectID": "posts/Fun08/Card Shuffling.html",
    "href": "posts/Fun08/Card Shuffling.html",
    "title": "Exploring Card Shuffling: A Visual Journey",
    "section": "",
    "text": "The Faro Shuffle Unveiled: Introducing the Faro shuffle, a captivating technique that promises not only randomness but a mathematical symphony in the arrangement of cards, creating a dance that echoes with the precision of numbers.\nSimulating the Faro Shuffle: into R code that simulates the Faro shuffle, using a function to emulate the interleaving of cards, allowing for the visualization of the ace of hearts’ position after multiple shuffles.\nSimulation Parameters and Results:Parameters like the number of Faro shuffles to explore and the simulations to run are defined. The resulting data frame captures the positions of the ace of hearts, forming the basis for the subsequent visual representation.\nVisual Exploration with ggplot2: The code utilizes ggplot2 to create a line plot with a ribbon, showcasing the dynamic journey of the ace of hearts through Faro shuffles. The visualization offers insights into the unpredictability introduced by this unique shuffling technique.\nThis visual exploration not only provides a hands-on experience with the Faro shuffle but also invites readers to appreciate the art and science behind the mesmerizing world of card shuffling."
  },
  {
    "objectID": "posts/Fun06/index.html#what-is-the-birthday-paradox",
    "href": "posts/Fun06/index.html#what-is-the-birthday-paradox",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "What Is the Birthday Paradox?",
    "text": "What Is the Birthday Paradox?\nThe Birthday Paradox is a probabilistic problem concerning the likelihood that two people in a group share the same birthday. At first glance, it might seem like the probability is very low, but in reality, it’s higher than you might think.\n\nExplanation of the Paradox\nThe paradox is based on the fact that there are many possible combinations of people’s birthdays within a group. While it might appear that there is only a small chance that two people share a birthday, things change when we consider the entire group."
  },
  {
    "objectID": "posts/Fun06/index.html#simulating-the-birthday-paradox-in-r",
    "href": "posts/Fun06/index.html#simulating-the-birthday-paradox-in-r",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "Simulating the Birthday Paradox in R",
    "text": "Simulating the Birthday Paradox in R\nLet’s dive into action and use the R programming language to simulate the Birthday Paradox. We will see how the probabilities change as the group size increases.\n\n# Number of simulations\nnum_simulations <- 10000\n\n# Function to check if there are shared birthdays in a group\ncheck_shared_birthday <- function(group_size) {\n  birthdays <- sample(1:365, group_size, replace = TRUE)\n  if (length(birthdays) == length(unique(birthdays))) {\n    return(FALSE)  # No shared birthdays\n  } else {\n    return(TRUE)   # Shared birthdays\n  }\n}\n\n# Simulate the Birthday Paradox\nsimulate_birthday_paradox <- function(group_size) {\n  shared_birthday_count <- 0\n  for (i in 1:num_simulations) {\n    if (check_shared_birthday(group_size)) {\n      shared_birthday_count <- shared_birthday_count + 1\n    }\n  }\n  return(shared_birthday_count / num_simulations)\n}\n\n# Test the Birthday Paradox simulation for different group sizes\ngroup_sizes <- 2:100\nresults <- numeric(length(group_sizes))\n\nfor (i in 1:length(group_sizes)) {\n  results[i] <- simulate_birthday_paradox(group_sizes[i])\n}\n\n# Plot the results\nplot(group_sizes, results, type = \"l\", xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\", main = \"Birthday Paradox Simulation\")\nabline(h = 0.5, col = \"red\", lty = 2)  # Add a line at 0.5 for reference"
  },
  {
    "objectID": "posts/Fun06/index.html#conclusions",
    "href": "posts/Fun06/index.html#conclusions",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "Conclusions",
    "text": "Conclusions\nThe Birthday Paradox is a captivating example of how probability can be counterintuitive. Despite our intuitions, the probability of finding two people with the same birthday is higher than it seems. This paradox is an opportunity to explore the laws of probability and how they can surprise us.\nIn the next article, we may delve into other paradoxes or further applications of probability concepts. In the meantime, feel free to try out the R code and experiment with the Birthday Paradox. Randomness can always bring surprises!"
  },
  {
    "objectID": "posts/Fun06/index.html#introduction",
    "href": "posts/Fun06/index.html#introduction",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "Introduction?",
    "text": "Introduction?\nIn the realm of probability, there exist situations that are surprising and counterintuitive, and the “Birthday Paradox” is one of the most well-known examples. In this article, we will explore this intriguing paradox and use R to simulate and visualize its application. The Birthday Paradox is a classic demonstration of how probability can defy our expectations."
  },
  {
    "objectID": "posts/Fun08/Card Shuffling.html#introduction-a-rainy-day-a-fathers-passion-and-card-games",
    "href": "posts/Fun08/Card Shuffling.html#introduction-a-rainy-day-a-fathers-passion-and-card-games",
    "title": "Exploring Card Shuffling: A Visual Journey",
    "section": "Introduction: A Rainy Day, a Father’s Passion, and Card Games",
    "text": "Introduction: A Rainy Day, a Father’s Passion, and Card Games\nAs a child, on days like today, when raindrops gently tapped against the window pane, my father, armed with a degree in economics and a love for statistics, would weave a world of numbers and cards to pass the time. His eyes would light up with enthusiasm as he shared the intricacies of card games, imparting not just the rules, but a deep appreciation for the unpredictable dance of shuffled decks.\nToday, we embark on a visual exploration of card shuffling, delving into a captivating technique known as the Faro shuffle. Unlike conventional shuffling, the Faro shuffle promises not just randomness but a mathematical symphony that unfolds card by card, a dance that echoes with the precision of numbers."
  },
  {
    "objectID": "posts/Fun08/Card Shuffling.html#simulating-the-faro-shuffle-understanding-the-code",
    "href": "posts/Fun08/Card Shuffling.html#simulating-the-faro-shuffle-understanding-the-code",
    "title": "Exploring Card Shuffling: A Visual Journey",
    "section": "Simulating the Faro Shuffle: Understanding the Code",
    "text": "Simulating the Faro Shuffle: Understanding the Code\nIn the quest to unravel the intricacies of card shuffling, we turn to the Faro shuffle, a technique that mimics the graceful dance of cards. The accompanying R code brings this technique to life, allowing us to simulate the position of the ace of hearts after multiple Faro shuffles.\n\n# Function to perform a faro shuffle on a deck of cards\nfaro &lt;- function(deck, num_shuffles) {\n  for (i in 1:num_shuffles) {\n    half1 &lt;- deck[1:(length(deck)/2)]\n    half2 &lt;- deck[(length(deck)/2 + 1):length(deck)]\n    deck &lt;- c()\n    for (j in 1:length(half1)) {\n      deck &lt;- c(deck, half1[j], half2[j])\n    }\n  }\n  return(deck)\n}\n\nHere, the faro function is defined to simulate the Faro shuffle. It takes a deck of cards (deck) and the number of shuffles to perform (num_shuffles). The function iterates through the halves of the deck, interleaving the cards to simulate the Faro shuffle. This process is repeated for the specified number of shuffles.\n\n# Install the required packages if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\n# Load the necessary packages\nlibrary(ggplot2)\n\n# Function to simulate the position of the ace of hearts after shuffling\nsimulate_ace_of_hearts_position &lt;- function(num_shuffles, num_simulations) {\n  original_deck &lt;- 1:52\n  ace_position &lt;- numeric(num_simulations)\n  \n  for (i in 1:num_simulations) {\n    shuffled_deck &lt;- faro(original_deck, num_shuffles)\n    ace_position[i] &lt;- which(shuffled_deck == 12)\n  }\n  \n  return(ace_position)\n}\n\n# Simulation parameters\nnum_shuffles_list &lt;- seq(1, 30, by = 1)\nnum_simulations &lt;- 100\n\n# Run the simulation\nace_positions &lt;- lapply(num_shuffles_list, function(num_shuffles) {\n  simulate_ace_of_hearts_position(num_shuffles, num_simulations)\n})\n\n# Prepare data for the plot\ndf &lt;- data.frame(\n  Num_Shuffles = rep(num_shuffles_list, each = num_simulations),\n  Ace_Position = unlist(ace_positions)\n)\n\n# Create a line plot with a ribbon in ggplot2\nggplot(df, aes(x = Num_Shuffles, y = Ace_Position)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Ace_Position - 5, ymax = Ace_Position + 5), fill = \"lightblue\", alpha = 0.3) +\n  geom_text(aes(label = Ace_Position), vjust = -0.5, hjust = 0.5, size = 3) +\n  labs(title = \"Ace of Hearts Position relative to Initial Position\",\n       x = \"Number of Shuffles\",\n       y = \"Ace of Hearts Position\") +\n  theme_minimal()\n\n\n\n\nIn the simulation parameters section, we define the number of Faro shuffles to explore (num_shuffles_list) and the number of simulations to run for each shuffle scenario (num_simulations). The resulting dataframe (’df’) contains the number of shuffles and the corresponding positions of the ace of hearts after simulation.Finallym the code utilizes ggplot2 to create a line plot with a ribbon that represents the range of possible ace of hearts positions after each Faro shuffle. The resulting visualization allows us to witness the mesmerizing journey of the ace of hearts through the rhytmic Faro shuffle."
  },
  {
    "objectID": "posts/Fun08/Card Shuffling.html#conclusion",
    "href": "posts/Fun08/Card Shuffling.html#conclusion",
    "title": "Exploring Card Shuffling: A Visual Journey",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this visual exploration, we gain insights into the inherent randomness of card shuffling. The fluctuating position of the ace of hearts highlights the complex interplay of probability and chance in card games.\nFeel free to experiment with the provided R code, adjusting parameters and exploring different aspects of card shuffling. The visual representation serves as a captivating way to understand the nuances of this seemingly simple yet intriguing process.\nStay tuned for more explorations and visualizations in future blog posts!"
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#introduction",
    "href": "posts/Fun07/Illusion Of Luck.html#introduction",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "Introduction:",
    "text": "Introduction:\nHave you ever wondered why casinos seem to have a mysterious edge, making them consistently profitable? Let’s explore a paradox in the world of gambling, where the number zero takes center stage and helps the house always come out on top."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#the-roulette-dilemma",
    "href": "posts/Fun07/Illusion Of Luck.html#the-roulette-dilemma",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "The Roulette Dilemma:",
    "text": "The Roulette Dilemma:\nTake a moment to delve into the rich history surrounding the classic roulette game, an indelible fixture in the tapestry of any casino. Fanciful stories about its origin abound, including tales of its invention by the 17th-century French mathematician Blaise Pascal, a French monk, or even the Chinese—supposedly transmitted to France by Dominican monks. However, the historical reality reveals a more nuanced narrative.\nIn truth, roulette’s roots trace back to France in the early 18th century, emerging from the older games of hoca and portique. It wasn’t until 1716 in Bordeaux that it was first mentioned under its current name. Over the years, roulette underwent several modifications, ultimately achieving its present layout and wheel structure around 1790. This pivotal moment marked its rapid ascent to the status of the leading game in the burgeoning casinos and gambling houses of Europe.\nThe game faced adversity during the years 1836 to 1933 when it was banned in France. Despite these challenges, roulette endured, eventually making a triumphant return to the gaming scene. The ebb and flow of its history reflect not only the evolution of a game but also its resilience and enduring appeal, contributing to the perpetual dilemma faced by those enticed by the timeless charm of the roulette wheel."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#simulating-the-game",
    "href": "posts/Fun07/Illusion Of Luck.html#simulating-the-game",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "Simulating the Game:",
    "text": "Simulating the Game:\nTo illustrate this phenomenon, let’s simulate the game of roulette using the R programming language. We’ll focus on a simple bet: predicting whether the ball will land on red or black. In a fair game, the odds of winning such a bet would be 1 to 1. However, the introduction of the number zero alters the dynamics.\n\nlibrary(ggplot2)\n\nplay_roulette &lt;- function(bet, chosen_number, num_spins) {\n  roulette_numbers &lt;- 1:36\n  results &lt;- sample(roulette_numbers, num_spins, replace = TRUE)\n  winnings &lt;- ifelse(results == chosen_number, 36, 0)  # 35 to 1 payout\n  winnings[bet == \"red\" & results %% 2 == 0] &lt;- 2  # win if the color is red\n  winnings[bet == \"black\" & results %% 2 != 0] &lt;- 2  # win if the color is black\n  total_winnings &lt;- sum(winnings)\n  return(total_winnings)\n}\n\n# Simulating plays with and without the house advantage\nset.seed(123)  # Set the seed for reproducibility\nwithout_advantage &lt;- play_roulette(\"red\", 17, 5000)\n\nset.seed(123)  # Reset the same seed for comparison\nwith_advantage &lt;- play_roulette(\"red\", 17, 5000) - 0.05 * 1000  # 5% house advantage\n\n# Comparing results\ncat(\"Without the house advantage: \", without_advantage, \"\\n\")\n\nWithout the house advantage:  10538 \n\ncat(\"With the house advantage: \", with_advantage, \"\\n\")\n\nWith the house advantage:  10488 \n\n# Data for plotting\ndata &lt;- data.frame(\n  Scenario = c(\"Without Advantage\", \"With Advantage\"),\n  Total_Winnings = c(without_advantage, with_advantage)\n)\n\n# Create a bar plot\nggplot(data, aes(x = Scenario, y = Total_Winnings, fill = Scenario)) +\n  geom_bar(stat = \"identity\", position = \"dodge\",fill = c(\"lightblue\", \"papayawhip\")) +\n  labs(title = \"Roulette Simulation: Comparing Outcomes\",\n       x = \"Scenario\",\n       y = \"Total Winnings\") +\n  theme_minimal()\n\n\n\n\n\nplay_roulette Function:\n\nThis function simulates playing roulette based on specified parameters.\nIt generates random roulette numbers for a given number of spins.\nCalculates winnings based on the chosen number and adjusts for bets on red or black, considering a 5% house advantage.\n\nSimulating Plays:\n\nThe code uses set.seed(123) to ensure reproducibility in random number generation.\nIt simulates roulette plays both with and without a house advantage (5% reduction in winnings for the scenario with the house advantage).\n\nComparing Results:\n\nThe cat statements print the total winnings for each scenario, allowing a direct comparison of outcomes.\n\nData Preparation for Plotting:\n\nCreates a data frame (data) containing scenarios (“Without Advantage” and “With Advantage”) and their corresponding total winnings.\n\nCreating a Bar Plot with ggplot2:\n\nUtilizes ggplot2 to generate a bar plot.\ngeom_bar represents the data as bars, positioned side by side (position = \"dodge\").\nThe plot is customized with a minimal theme, a title (“Roulette Simulation: Comparing Outcomes”), and axis labels (“Scenario” and “Total Winnings”).\nDifferent colors (“lightblue” and “papayawhip”) are assigned to each scenario for clarity."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#the-house-edge",
    "href": "posts/Fun07/Illusion Of Luck.html#the-house-edge",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "The House Edge:",
    "text": "The House Edge:\nIn our simulation, we introduced a slight modification to the payouts, reducing them just enough to create a 5% advantage for the house. This mirrors the real-world scenario where the presence of zero in roulette gives the casino an edge."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#comparing-outcomes",
    "href": "posts/Fun07/Illusion Of Luck.html#comparing-outcomes",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "Comparing Outcomes:",
    "text": "Comparing Outcomes:\nRunning our simulation both with and without the house advantage reveals a stark contrast. The casino consistently comes out ahead in the long run, showcasing how the number zero plays a crucial role in tipping the odds in favor of the house."
  },
  {
    "objectID": "posts/Fun07/Illusion Of Luck.html#conclusion",
    "href": "posts/Fun07/Illusion Of Luck.html#conclusion",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe illusion of luck in casinos often stems from subtle yet significant factors, such as the presence of zero in games like roulette. Understanding these nuances can provide valuable insights into the mechanics of gambling and why, ultimately, the house always wins.\nAs you ponder the next spin of the roulette wheel, remember that behind the excitement lies a carefully crafted system where even the seemingly neutral zero plays a pivotal role in ensuring the casino’s enduring success."
  },
  {
    "objectID": "posts/Fun04/index.html#analyzing-coin-flip-sequences-with-r",
    "href": "posts/Fun04/index.html#analyzing-coin-flip-sequences-with-r",
    "title": "Analyzing Coin Flip Sequences with R",
    "section": "Analyzing Coin Flip Sequences with R",
    "text": "Analyzing Coin Flip Sequences with R\nIn this blog post, we’ll dive into a fun R code snippet that simulates a series of coin flips and analyzes the resulting sequences. We’ll use the viridis and ggplot2 libraries to create visual representations of the sequences and gain insights into the outcomes. Let’s break down the code step by step.\nSetting Up the Environment\n\nlibrary(viridis)  # Import the viridis color palette library\n\nWarning: il pacchetto 'viridis' è stato creato con R versione 4.3.2\n\n\nCaricamento del pacchetto richiesto: viridisLite\n\nlibrary(ggplot2)\n\nset.seed(123)  # Set a seed for reproducibility\nnum_flips &lt;- 50000\nflips &lt;- sample(c(\"Heads\", \"Tails\"), num_flips, replace = TRUE)\n\nHere, we load the necessary libraries and set a seed for reproducibility. We simulate 50,000 coin flips, storing the results in the flips variable.\nCreating a Color Matrix\n\n# Image aspect ratio\naspect_ratio &lt;- 1  # You can customize the aspect ratio here\nn_col &lt;- round(sqrt(num_flips) * aspect_ratio)\nn_row &lt;- ceiling(num_flips / n_col)\n\n# Create a color matrix to represent coin flips\ncolors &lt;- ifelse(flips == \"Heads\", \"red\", \"blue\")\n\nMatrix Manipulation\n\n# Create matrices for Heads and Tails\nheads_matrix &lt;- matrix(0, nrow = n_row, ncol = n_col)\ntails_matrix &lt;- matrix(0, nrow = n_row, ncol = n_col)\n\nfor (i in 1:num_flips) {\n  if (flips[i] == \"Heads\") {\n    heads_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] &lt;- 1\n  } else {\n    tails_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] &lt;- 1\n  }\n}\n\nWe create matrices heads_matrix and tails_matrix to represent the sequences of heads and tails. These matrices help us analyze the coin flip sequences."
  },
  {
    "objectID": "posts/Fun04/index.html#simulating-coin-flips",
    "href": "posts/Fun04/index.html#simulating-coin-flips",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "Simulating Coin Flips",
    "text": "Simulating Coin Flips\nLet’s kick things off by simulating coin flips using R. We’ll conduct experiments with varying numbers of flips to observe how the results evolve.\n\n# Function to simulate coin tosses\nsimulate_tosses &lt;- function(n) {\n  set.seed(42)  # Set seed for reproducibility\n  flips &lt;- sample(c(\"Heads\", \"Tails\"), n, replace = TRUE)\n  return(flips)\n}\n\n# Simulate coin flips for different scenarios\nflips_100 &lt;- simulate_tosses(100)\nflips_1000 &lt;- simulate_tosses(1000)\nflips_10000 &lt;- simulate_tosses(10000)\nflips_100000 &lt;- simulate_tosses(100000)"
  },
  {
    "objectID": "posts/Fun04/index.html#analyzing-consecutive-sequences",
    "href": "posts/Fun04/index.html#analyzing-consecutive-sequences",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "Analyzing Consecutive Sequences",
    "text": "Analyzing Consecutive Sequences\nMoving on, we’ll analyze the consecutive sequences of heads or tails in each simulation. We’ll count the length of these sequences to gain insights into their distribution.\n\n# Function to count consecutive sequences\ncount_consecutive_sequences &lt;- function(flips) {\n  count &lt;- rep(0, length(flips))\n  counter &lt;- 1\n\n  for (i in 2:length(flips)) {\n    if (flips[i] == flips[i-1]) {\n      counter &lt;- counter + 1\n    } else {\n      counter &lt;- 1\n    }\n    count[i] &lt;- counter\n  }\n\n  return(count)\n}\n\n# Function to create a plot for consecutive sequences\ncreate_plot &lt;- function(consecutive_counts, n) {\n  data &lt;- data.frame(Launch = 1:length(consecutive_counts), Consecutive = consecutive_counts)\n  \n  library(ggplot2)\n  \n  plot &lt;- ggplot(data, aes(x = Launch, y = Consecutive)) +\n    geom_line() +\n    labs(title = paste(\"Consecutive Sequences -\", n, \"Tosses\"),\n         x = \"Toss Number\",\n         y = \"Consecutive Count\") +\n    theme_minimal()\n  \n  return(plot)\n}\n\n# Analyze consecutive sequences for different scenarios\nconsecutive_counts_100 &lt;- count_consecutive_sequences(flips_100)\nconsecutive_counts_1000 &lt;- count_consecutive_sequences(flips_1000)\nconsecutive_counts_10000 &lt;- count_consecutive_sequences(flips_10000)\nconsecutive_counts_100000 &lt;- count_consecutive_sequences(flips_100000)"
  },
  {
    "objectID": "posts/Fun04/index.html#visualizing-the-results",
    "href": "posts/Fun04/index.html#visualizing-the-results",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nNow, let’s create visualizations to better understand the distribution of consecutive sequences. We’ll use the ggplot2 library to craft insightful graphs and gridExtra to arrange them together.\n\n# Create plots\nplot_100 &lt;- create_plot(consecutive_counts_100, 100)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.2\n\nplot_1000 &lt;- create_plot(consecutive_counts_1000, 1000)\nplot_10000 &lt;- create_plot(consecutive_counts_10000, 10000)\nplot_100000 &lt;- create_plot(consecutive_counts_100000, 100000)\n\n# Use gridExtra to arrange the plots\nlibrary(gridExtra)\ngrid.arrange(plot_100, plot_1000, plot_10000, plot_100000, ncol = 2)\n\n\n\n\n\n# Function to simulate coin tosses and plot the maximum consecutive sequence\nsimulate_and_plot_max_sequence &lt;- function(n) {\n  # Simulate coin tosses\n  flips &lt;- simulate_tosses(n)\n  \n  # Analyze consecutive sequences\n  consecutive_counts &lt;- count_consecutive_sequences(flips)\n  \n  # Return the maximum consecutive sequence\n  return(max(consecutive_counts))\n}\n\n# Vector to store results\nmax_sequences &lt;- c()\n\n# Perform simulations with increasing numbers of tosses\nfor (n_tosses in c(100, 2000, 10000, 100000)) {\n  max_sequence &lt;- simulate_and_plot_max_sequence(n_tosses)\n  max_sequences &lt;- c(max_sequences, max_sequence)\n}\n\n# Plot the results\nlibrary(ggplot2)\n\ndata &lt;- data.frame(Num_Tosses = c(100, 2000, 10000, 100000), Max_Sequence = max_sequences)\n\nplot &lt;- ggplot(data, aes(x = Num_Tosses, y = Max_Sequence)) +\n  geom_line() +\n  labs(title = \"Maximum Consecutive Sequence vs. Number of Tosses\",\n       x = \"Number of Tosses\",\n       y = \"Max Consecutive Sequence\") +\n  theme_minimal()\n\nprint(plot)"
  },
  {
    "objectID": "posts/Fun04/index.html#results-and-insights",
    "href": "posts/Fun04/index.html#results-and-insights",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "Results and Insights",
    "text": "Results and Insights\nAfter simulating coin flips and analyzing consecutive sequences, we’ve gained valuable insights into the probability of encountering various sequences. As expected, with a larger number of flips, the likelihood of encountering longer consecutive sequences increases.\nConclusion Exploring probability through simulation is a powerful way to grasp the nuances of random processes. Whether you’re a statistician, data scientist, or simply curious, these simulations provide a fascinating glimpse into the world of chance.\nFeel free to experiment with different parameters, such as the number of flips or the probability of getting heads or tails, and observe how the results change. Happy exploring!"
  },
  {
    "objectID": "posts/Fun04/index.html#introduction",
    "href": "posts/Fun04/index.html#introduction",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "Introduction",
    "text": "Introduction\nIn this blog post, we will dive into the fascinating world of coin flip sequences. Leveraging the capabilities of R, we will conduct simulations to gain insights into the probabilities of consecutive sequences of heads or tails."
  },
  {
    "objectID": "posts/Fun09/index.html",
    "href": "posts/Fun09/index.html",
    "title": "Exploring the Frequency of Number 0 in a Year of Roulette Spins: A Simulation in R",
    "section": "",
    "text": "In this blog post, we’ll delve into the realm of casino roulette and use R simulations to estimate the frequency of the number 0 appearing over the span of a year (40 spins per hour x 24 hours x 365 days) . Roulette, a classic casino game, is known for its unpredictability, making it an interesting subject for probability exploration.\n\n\n\n\n# Function to simulate roulette spins\nsimulate_roulette_spins <- function(n) {\n  set.seed(42)  # Set seed for reproducibility\n  numbers <- sample(0:36, n, replace = TRUE)\n  return(numbers)\n}\n\n# Simulate roulette spins for different scenarios\nspins_1000 <- simulate_roulette_spins(1000)\nspins_10000 <- simulate_roulette_spins(10000)\nspins_100000 <- simulate_roulette_spins(100000)\nspins_1_year <- simulate_roulette_spins(365 * 24 * 40)  # One year of spins\n\n\n\n\nMoving on, let’s analyze the frequency of the number 0 in each simulation. We’ll count the occurrences to gain insights into its distribution.\n\n# Function to count occurrences of number 0\ncount_zero_occurrences <- function(spins) {\n  return(sum(spins == 0))\n}\n\n# Count occurrences for different scenarios\nzero_count_1000 <- count_zero_occurrences(spins_1000)\nzero_count_10000 <- count_zero_occurrences(spins_10000)\nzero_count_100000 <- count_zero_occurrences(spins_100000)\nzero_count_1_year <- count_zero_occurrences(spins_1_year)\n\n\n\n\n\n# Create a bar plot for zero occurrences\nlibrary(ggplot2)\n\ndata <- data.frame(Scenario = c(\"1000 spins\", \"10000 spins\", \"100000 spins\", \"1 year\"),\n                   Zero_Count = c(zero_count_1000, zero_count_10000, zero_count_100000, zero_count_1_year))\n\nplot <- ggplot(data, aes(x = Scenario, y = Zero_Count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency of Number 0 in Roulette Spins\",\n       x = \"Simulation Scenario\",\n       y = \"Number of 0 occurrences\") +\n  theme_minimal()\n\nprint(plot)\n\n\n\n\n\n\n\nAfter simulating roulette spins and analyzing the frequency of the number 0, we’ll gain valuable insights into the likelihood of encountering this specific outcome over different scenarios.\n\n\n\nExploring the probability of specific outcomes in casino games through simulation provides a fascinating glimpse into the world of chance. Whether you’re interested in statistics, data science, or simply curious, these simulations offer an engaging way to understand the dynamics of games of chance.\nFeel free to experiment with different parameters, such as the number of spins or the distribution of numbers on the roulette wheel, and observe how the results change. Happy exploring!\nNote: a fast search with bing about how many time does a roulette spin in a vegas casino : Explore\nThe average roulette wheel spins around 40-50 times per hour when playing roulette at a casino in Las Vegas1. However, this can vary depending on the type of wheel, the number of players at the table, and how quickly bets are placed1. If you want to know more about roulette and the game’s current standing around the world, you can check out this guide which provides a list of the most relevant roulette wheel odds and stats about the game, where you’ll learn everything about one of the world’s most popular casino games and how some odds work in your favor2."
  },
  {
    "objectID": "posts/Riddle01/index.html",
    "href": "posts/Riddle01/index.html",
    "title": "Unraveling the DnD Dice Duel Riddle with Monte Carlo Simulation in R",
    "section": "",
    "text": "Embark on a journey into the realm of Dungeons & Dragons as we unravel a captivating fiddle riddle involving a dice duel. Using the power of the R programming language and the Monte Carlo simulation method, we’ll simulate the outcomes of duels between two players, each armed with a bag containing six distinct DnD dice. Prepare to explore the fascinating world of probability and randomness! See the riddle posted here by Fiddler on the Proof\nAt a table sit two individuals, each equipped with a bag housing six DnD dice: a d4, a d6, a d8, a d10, a d12, and a d20. The challenge is to randomly select one die from each bag and roll them simultaneously. For example, if a d4 and a d12 are chosen, both players roll their respective dice, hoping for fortuitous results. Monte Carlo Simulation in R:\nTo confront this enigma, we turn to the Monte Carlo method. The following R code snippet initiates a simulation of multiple dice duels, offering a glimpse into the complexities of DnD dice outcomes.\nwe can break down the analysis into different cases:\n\nCase 1: Both players take the same type of dice.\nCase 2: Both players take different types of dice (without repetition of the same combination).\n\nWe’ll generate plots for each case and then provide a summary of the results. Here’s the code:\n\nlibrary(ggplot2)\n# Function to simulate a single dice duel with both players taking the same type of dice\nsimulate_same_dice_duel <- function() {\n  dice_type <- sample(c(4, 6, 8, 10, 12,20,40,64,80,120,128), 1)\n  roll_player1 <- sample(1:dice_type, 1)\n  roll_player2 <- sample(1:dice_type, 1)\n  return(c(dice_type, roll_player1, dice_type, roll_player2))\n}\n\n# Function to simulate a single dice duel with both players taking different types of dice\nsimulate_different_dice_duel <- function() {\n  dice_types <- sample(c(4, 6, 8, 10, 12,20,40,64,80,120,128), 2, replace = FALSE)\n  roll_player1 <- sample(1:dice_types[1], 1)\n  roll_player2 <- sample(1:dice_types[2], 1)\n  return(c(dice_types[1], roll_player1, dice_types[2], roll_player2))\n}\n\n# Monte Carlo simulation for both cases\nnum_trials <- 10000\n\n# Case 1: Both players take the same type of dice\nsame_dice_simulation_results <- replicate(num_trials, simulate_same_dice_duel())\nsame_dice_data <- data.frame(Player = rep(c(\"Player 1\", \"Player 2\"), each = ncol(same_dice_simulation_results)),\n                             Dice_Type = rep(same_dice_simulation_results[1, ], 2),\n                             Roll_Value = as.integer(c(same_dice_simulation_results[2, ], same_dice_simulation_results[4, ])))\n\n# Visualize the results for Case 1 using ggplot2\nggplot(same_dice_data, aes(x = factor(Dice_Type), y = Roll_Value, fill = Player)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 1: Both Players Take the Same Dice (\", num_trials, \"trials)\"),\n       x = \"Dice Type\",\n       y = \"Roll Value\",\n       fill = \"Player\") +\n  theme_minimal()\n\n\n\n# Case 2: Both players take different types of dice\ndifferent_dice_simulation_results <- replicate(num_trials, simulate_different_dice_duel())\ndifferent_dice_data <- data.frame(Player = rep(c(\"Player 1\", \"Player 2\"), each = ncol(different_dice_simulation_results)),\n                                  Dice_Type_Player1 = rep(different_dice_simulation_results[1, ], 2),\n                                  Roll_Value_Player1 = as.integer(c(different_dice_simulation_results[2, ])),\n                                  Dice_Type_Player2 = rep(different_dice_simulation_results[3, ], 2),\n                                  Roll_Value_Player2 = as.integer(c(different_dice_simulation_results[4, ])))\n\n# Visualize the results for Case 2 - Player 1 (Dice 4 vs. Dice 20)\nggplot(subset(different_dice_data, Dice_Type_Player1 %in% c(4, 20)), aes(x = factor(Dice_Type_Player2), y = Roll_Value_Player1)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 1: Dice 4 vs. Dice 20 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 2\",\n       y = \"Roll Value Player 1\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 20)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 20)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 20 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 1 (Dice 4 vs. Dice 12)\nggplot(subset(different_dice_data, Dice_Type_Player1 %in% c(4, 12)), aes(x = factor(Dice_Type_Player2), y = Roll_Value_Player1)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 1: Dice 4 vs. Dice 12 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 2\",\n       y = \"Roll Value Player 1\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 12)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 12)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 12 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 128)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 128)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 128 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Summarize the results for Case 1 (Same Dice)\nsummary_case1 <- table(same_dice_data$Roll_Value)\n\n# Summarize the results for Case 2 (Different Dice)\nsummary_case2 <- table(different_dice_data$Roll_Value_Player1 == different_dice_data$Roll_Value_Player2)\n\n# Display summaries\ncat(\"\\nSummary of Case 1 - Same Dice:\\n\")\n\n\nSummary of Case 1 - Same Dice:\n\nprint(summary_case1)\n\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n1578 1522 1546 1538 1036 1080  752  779  580  548  362  394  214  228  210  227 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n 228  191  230  207  103  124  126  145  115  120  113  113  135  119  140  126 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n 121  139  144  118  105  107  134  132   73   69   78   81   85   97   82   92 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n  90   89   83   78   66   71   76   91   87   84   75   66   85   78   79   85 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n  56   52   53   46   53   63   53   35   55   58   57   47   54   48   43   71 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n  27   41   26   28   33   28   31   30   34   35   32   28   32   23   26   34 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n  29   35   37   27   28   25   26   23   26   28   28   24   31   26   34   35 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n  33   25   21   34   31   36   25   37   12   14   10    9   15   12   14    9 \n\ncat(\"\\nSummary of Case 2 - Different Dice:\\n\")\n\n\nSummary of Case 2 - Different Dice:\n\nprint(summary_case2)\n\n\nFALSE  TRUE \n19292   708 \n\n\n\n\n\nThrough the marriage of R programming and Monte Carlo simulation, we’ve successfully deciphered the intricacies of the DnD dice duel riddle. Whether you’re a seasoned tabletop gamer or a data science enthusiast, this approach serves as a versatile tool for exploring and comprehending complex scenarios governed by chance. As you embark on your own coding adventures, may the rolls be ever in your favor! Happy coding!"
  },
  {
    "objectID": "posts/Fun10/index.html",
    "href": "posts/Fun10/index.html",
    "title": "A Mathematical Exploration of Pizza Sizes",
    "section": "",
    "text": "Introduction\nPizza, a beloved culinary delight, comes in various sizes. To better understand the implications of pizza size on the amount of pizza consumed, we establish a new standard unit called the standard pizza radius, denoted by the letter \\(a\\), which measures 6 inches. This article examines how the area of a pizza changes with size and demonstrates that one extra-large pizza can provide more pizza than two standard-sized pizzas.\n\n\nThe Area of a Standard Pizza\nThe area of a pizza, approximated as a circle, with a radius of one standard pizza radius (\\(a\\)) is given by the formula:\n\\[\n\\text{Area}_{\\text{standard}} = \\pi a^2\n\\]\n\n\nThe Area of an Extra-Large Pizza\nFor an extra-large pizza with a radius \\(r = 1.5a\\), the area can be calculated as follows:\n\\[\n\\text{Area}_{\\text{extra-large}} = \\pi (1.5a)^2 = \\pi \\cdot 1.5^2 \\cdot a^2 = \\pi \\cdot 2.25 \\cdot a^2\n\\]\n\n\nComparison: Two Standard Pizzas vs. One Extra-Large Pizza\nThe combined area of two standard pizzas with radius \\(a\\) is:\n\\[\n\\text{Area}_{\\text{two standard}} = 2 \\cdot \\pi a^2\n\\]\nComparing this to the area of one extra-large pizza:\n\\[\n\\pi \\cdot 2.25 \\cdot a^2 > 2 \\cdot \\pi a^2\n\\]\nSimplifying, we find:\n\\[\n2.25 \\pi a^2 > 2 \\pi a^2\n\\]\nThus, the area of one extra-large pizza is greater than the combined area of two standard pizzas.\n\n\nMinimum Radius for Extra-Large Pizza to Always Be Larger\nTo determine the minimum radius \\(r = n \\cdot a\\) for the extra-large pizza to always have a greater area than two standard pizzas, we start with the inequality:\n\\[\n\\pi (n \\cdot a)^2 > 2 \\cdot \\pi a^2\n\\]\nSimplifying, we get:\n\\[\nn^2 \\cdot \\pi a^2 > 2 \\cdot \\pi a^2\n\\]\n\\[\nn^2 > 2\n\\]\n\\[\nn > \\sqrt{2}\n\\]\n\\[\nn > 1.4142\n\\]\nTherefore, the radius of the extra-large pizza must be at least \\(\\sqrt{2}\\) times the radius of a standard pizza to ensure its area is always greater than that of two standard pizzas.\n\n\nA Practical Example: Pizza Napoletana\nIn Italy, according to the Disciplinare verace pizza napoletana (guidelines for authentic Neapolitan pizza), the radius of a pizza ranges from 22 to 35 cm. Let’s compare the area of two pizzas with a 22 cm radius to one pizza with a 33 cm radius.\n\nTwo pizzas with 22 cm radius:\n\n\\[\n2 \\cdot \\pi \\cdot 22^2 = 2 \\cdot \\pi \\cdot 484 = 2 \\cdot 1520.56 = 3039.52 \\, \\text{cm}^2\n\\]\n\nOne pizza with 33 cm radius:\n\n\\[\n\\pi \\cdot 33^2 = \\pi \\cdot 1089 = 3419.46 \\, \\text{cm}^2\n\\]\nThis calculation confirms that one pizza with a 33 cm radius has a greater area than two pizzas with a 22 cm radius. Therefore, it is mathematically established that consuming one extra-large pizza results in more pizza than consuming two smaller ones."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html",
    "href": "posts/Packages Review 2024/index.html",
    "title": "R Packages 2024 list so far",
    "section": "",
    "text": "List of packages I’ve found useful in my workflow during 2024 (so far)"
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#plot",
    "href": "posts/Packages Review 2024/index.html#plot",
    "title": "R Packages 2024 list so far",
    "section": "Plot",
    "text": "Plot\n\nspiralize: Visualize Data on Spirals\ntags: #plot\n[cran package link] https://CRAN.R-project.org/package=spiralize\ndescription from the author/vignette\n\n\nIt visualizes data along an Archimedean spiral https://en.wikipedia.org/wiki/Archimedean_spiral, makes so-called spiral graph or spiral chart. It has two major advantages for visualization: 1. It is able to >visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns.\n\n\n\npanelView: Visualizing Panel Data\ntags: #plot\n[cran package link] https://CRAN.R-project.org/package=panelView\ndescription from the author/vignette\n\n\nVisualizes panel data. It has three main functionalities: (1) it plots the treatment status and missing values in a panel dataset; (2) it visualizes the temporal dynamics of a main variable of interest; (3) it depicts the bivariate relationships between a treatment variable and an outcome variable either by unit or in aggregate. For details, see doi:10.18637/jss.v107.i07."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#spectroscopy",
    "href": "posts/Packages Review 2024/index.html#spectroscopy",
    "title": "R Packages 2024 list so far",
    "section": "Spectroscopy",
    "text": "Spectroscopy\n\nOpenSpecy: Analyze, Process, Identify, and Share Raman and (FT)IR Spectra\ntags: #spectroscopy\n[cran package link] https://CRAN.R-project.org/package=OpenSpecy\ndescription from the author/vignette\n\n\nRaman and (FT)IR spectral analysis tool for plastic particles and other environmental samples (Cowger et al. 2021, doi:10.1021/acs.analchem.1c00123). With read_any(), Open Specy provides a single function for reading individual, batch, or map spectral data files like .asp, .csv, .jdx, .spc, .spa, .0, and .zip. process_spec() simplifies processing spectra, including smoothing, baseline correction, range restriction and flattening, intensity conversions, wavenumber alignment, and min-max normalization. Spectra can be identified in batch using an onboard reference library (Cowger et al. 2020, doi:10.1177/0003702820929064) using match_spec(). A Shiny app is available via run_app() or online at> https://openanalysis.org/openspecy/.\n\n\n\nplsVarSel: Variable Selection in Partial Least Squares\ntags: #pls #partial least squares #regression\n[cran package link] https://CRAN.R-project.org/package=plsVarSel\ndescription from the author/vignette\n\n\nInterfaces and methods for variable selection in Partial Least Squares. The methods include filter methods, wrapper methods and embedded methods. Both regression and classification is supported."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#statistics",
    "href": "posts/Packages Review 2024/index.html#statistics",
    "title": "R Packages 2024 list so far",
    "section": "Statistics",
    "text": "Statistics\n\nqreport: Statistical Reporting with ‘Quarto’\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=qreport\ndescription from the author/vignette\n\n\nProvides statistical components, tables, and graphs that are useful in ‘Quarto’ and ‘RMarkdown’ reports and that produce ‘Quarto’ elements for special formatting such as tabs and marginal notes and graphs. Some of the functions produce entire report sections with tabs, e.g., the missing data report created by missChk(). Functions for inserting variables and tables inside ‘graphviz’ and ‘mermaid’ diagrams are included, and so are special clinical trial graphics for adverse event reporting.\n\n\n\nsjPlot: Data Visualization for Statistics in Social Science\ntags: #statistics #social science [cran package link] https://CRAN.R-project.org/package=sjPlot\ndescription from the author/vignette\n\n\nCollection of plotting and table output functions for data visualization. Results of various statistical analyses (that are commonly used in social sciences) can be visualized using this package, including simple and cross tabulated frequencies, histograms, box plots, (generalized) linear models, mixed effects models, principal component analysis and correlation matrices, cluster analyses, scatter plots, stacked scales, effects plots of regression models (including interaction terms) and much more. This package supports labelled data.\n\n\n\nMVET: Multivariate Estimates and Tests\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=MVET\ndescription from the author/vignette\n\n\nMultivariate estimation and testing, currently a package for testing parametric data. To deal with parametric data, various multivariate normality tests and outlier detection are performed and visualized using the ‘ggplot2’ package. Homogeneity tests for covariance matrices are also possible, as well as the Hotelling’s T-square test and the multivariate analysis of variance test. We are exploring additional tests and visualization techniques, such as profile analysis and randomized complete block design, to be made available in the future and making them easily accessible to users.\n\n\n\npbox: Exploring Multivariate Spaces with Probability Boxes\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=pbox\ndescription from the author/vignette\n\n\nAdvanced statistical library offering a method to encapsulate and query the probability space of a dataset effortlessly using Probability Boxes (p-boxes). Its distinctive feature lies in the ease with which users can navigate and analyze marginal, joint, and conditional probabilities while taking into account the underlying correlation structure inherent in the data using copula theory and models. A comprehensive explanation is available in the paper “pbox: Exploring Multivariate Spaces with Probability Boxes” to be published in the Journal of Statistical Software.\n\n\n\nequatiomatic: Transform Models into ‘LaTeX’ Equations\ntags: #statistics #latex #regression #models\n[cran package link] https://CRAN.R-project.org/package=equatiomatic\ndescription from the author/vignette\n\n\nThe goal of ‘equatiomatic’ is to reduce the pain associated with writing ‘LaTeX’ formulas from fitted models. The primary function of the package, extract_eq(), takes a fitted model object as its input and returns the corresponding ‘LaTeX’ code for the model.\n\n\n\nbulkreadr: The Ultimate Tool for Reading Data in Bulk\ntags: #bulk import\n[cran package link] https://CRAN.R-project.org/package=bulkreadr\ndescription from the author/vignette\n\n\nDesigned to simplify and streamline the process of reading and processing large volumes of data in R, this package offers a collection of functions tailored for bulk data operations. It enables users to efficiently read multiple sheets from Microsoft Excel and Google Sheets workbooks, as well as various CSV files from a directory. The data is returned as organized data frames, facilitating further analysis and manipulation. Ideal for handling extensive data sets or batch processing tasks, bulkreadr empowers users to manage data in bulk effortlessly, saving time and effort in data preparation workflows. Additionally, the package seamlessly works with labelled data from SPSS and Stata."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#simulated-data",
    "href": "posts/Packages Review 2024/index.html#simulated-data",
    "title": "R Packages 2024 list so far",
    "section": "Simulated data",
    "text": "Simulated data\n\nrsurv: Random Generation of Survival Data\ntags: #rsurv\n[cran package link] https://CRAN.R-project.org/package=rsurv\ndescription from the author/vignette\n\n\nRandom generation of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package ‘rsurv’ also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package ‘rsurv’ lies in the fact that linear predictors are specified via a formula-based approach, facilitating the inclusion of categorical variables and interaction terms. The functions implemented in the package ‘rsurv’ can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivariate survival data, and competing risks survival data. Details about the R package ‘rsurv’ can be found in Demarqui (2024) doi:10.48550/arXiv.2406.01750."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#reporting-and-formatting",
    "href": "posts/Packages Review 2024/index.html#reporting-and-formatting",
    "title": "R Packages 2024 list so far",
    "section": "Reporting and Formatting",
    "text": "Reporting and Formatting\n\nftExtra: Extensions for ‘Flextable’\ntags: #tables #flextables\n[cran package link] https://CRAN.R-project.org/package=ftExtra\ndescription from the author/vignette\n\n\nBuild display tables easily by extending the functionality of the ‘flextable’ package. Features include spanning header, grouping rows, parsing markdown and so on."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#fun",
    "href": "posts/Packages Review 2024/index.html#fun",
    "title": "R Packages 2024 list so far",
    "section": "Fun",
    "text": "Fun\n\nPlayerChart: Generate Pizza Chart: Player Stats 0-100\ntags: #statistics  [cran package link] https://CRAN.R-project.org/package=PlayerChart\ndescription from the author/vignette\n\n\nCreate an interactive pizza chart visualizing a specific player’s statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: ‘data’, a dataframe containing player data for any sports; ‘player_stats_col’, a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; ‘name_col’, specifying the name of the column in the dataframe that contains the player names; and ‘player_name’, representing the specific player whose statistics will be visualized in the chart, serving as the chart title.\n\n\n\ngameR: Color Palettes Inspired by Video Games\ntags: #statistics  [cran package link] https://CRAN.R-project.org/package=gameR\ndescription from the author/vignette\n\n\nPalettes based on video games."
  }
]