[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "giorgioluciano.github.io",
    "section": "",
    "text": "Blog\nProvides tutorials alongside practical examples in R. Topics include statistical simulations, data visualization, and common challenges in scientific analysis. The content is designed to offer useful insights for those working with data, without unnecessary complexity.\n\n\nCode\nRepository featuring a collection of tutorials and 3D Blender projects. It focuses on Python programming applied to statistics. Contributions and collaborations are welcome!\n\n\nCrystalNodes\nCrystal Nodes (CN) is an addon for the 3D modelling & animation program Blender that enables easy creation of crystals illustrations. CN is focused on providing tools for drawing Crystallography illustrations and it is not (for now) a computational tool for calculating inorganic structures\n\n\nEssential Computer Graphics Techniques book\nAll the images of my book in high resolution\n\n\nAbout me\nI got a Ph.D in materials science and I’m a researcher with a variety of interests. I currently works as a researcher at the Italian National Council of Research (CNR) on topics related to the characterization and development of new materials. My interests are also in 3D CG (lighting and rendering), and creating scientific illustration. Here you will find my code tutorials, blend files and other doodles"
  },
  {
    "objectID": "listing.html",
    "href": "listing.html",
    "title": "giorgioluciano.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Pitfall of Arbitrary Curve Deconvolution: A Cautionary Tale for Scientists and Students\n\n\n\n\n\n\n\nEssential\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguring Your RSS Feed in Quarto: A Crucial Naming Tip\n\n\n\n\n\n\n\nQuarto\n\n\nRSS\n\n\nWeb Development\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriteri Essenziali per La Gestione Dati\n\n\n\n\n\n\n\nEssential\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Negotiables for Your Data\n\n\n\n\n\n\n\nEssential\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nFormattazione condizionale in Excel\n\n\n\n\n\n\n\nEssential\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfronto tra Excel e Database nella Gestione Clinica dei Dati: Errori E Sicurezza\n\n\n\n\n\n\n\nEssential\n\n\nTutorial\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Packages 2024 list so far\n\n\n\n\n\n\n\nR\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Mathematical Exploration of Pizza Sizes\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling the DnD Dice Duel Riddle with Monte Carlo Simulation in R\n\n\n\n\n\n\n\nR\n\n\nFun\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Frequency of Number 0 in a Year of Roulette Spins: A Simulation in R\n\n\n\n\n\n\n\nR\n\n\nFun\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Coin Flip Sequences with Simulation in R\n\n\n\n\n\n\n\nR\n\n\nSimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Illusion of Luck: How the Number Zero Always Wins in the Casino\n\n\n\n\n\n\n\nR\n\n\nfun\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Card Shuffling: A Visual Journey\n\n\n\n\n\n\n\nR\n\n\nfun\n\n\ncards\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonty Hall simulation\n\n\n\n\n\n\n\nR\n\n\nfun\n\n\nMonty Hall problem\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem with Dice Rolls\n\n\n\n\n\n\n\nR\n\n\nfun\n\n\nDice\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNamespaces in shiny: Why you need them\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\nggplot\n\n\nshiny\n\n\ntutorials\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\nggplot\n\n\nvisualization\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Birthday Paradox: When Probability Plays Tricks\n\n\n\n\n\n\n\nR\n\n\nFun\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\n\nR\n\n\nFun\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Packages 2022 list\n\n\n\n\n\n\n\nR\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #4: boxplots and scatterplots: simple recipes\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\nggplot\n\n\nvisualization\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #3: Functions for simulating data\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #1: ggplot loops\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnippet #2: Cleaning column names of an imported csv\n\n\n\n\n\n\n\nR\n\n\ntutorials\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_Zero_Roulette/index.html",
    "href": "posts/001_Zero_Roulette/index.html",
    "title": "Exploring the Frequency of Number 0 in a Year of Roulette Spins: A Simulation in R",
    "section": "",
    "text": "In this blog post, we’ll delve into the realm of casino roulette and use R simulations to estimate the frequency of the number 0 appearing over the span of a year (40 spins per hour x 24 hours x 365 days) . Roulette, a classic casino game, is known for its unpredictability, making it an interesting subject for probability exploration.\n\n# Function to simulate roulette spins\nsimulate_roulette_spins &lt;- function(n) {\n  set.seed(42)  # Set seed for reproducibility\n  numbers &lt;- sample(0:36, n, replace = TRUE)\n  return(numbers)\n}\n\n# Simulate roulette spins for different scenarios\nspins_1000 &lt;- simulate_roulette_spins(1000)\nspins_10000 &lt;- simulate_roulette_spins(10000)\nspins_100000 &lt;- simulate_roulette_spins(100000)\nspins_1_year &lt;- simulate_roulette_spins(365 * 24 * 40)  # One year of spins\n\nMoving on, let’s analyze the frequency of the number 0 in each simulation. We’ll count the occurrences to gain insights into its distribution.\n\n# Function to count occurrences of number 0\ncount_zero_occurrences &lt;- function(spins) {\n  return(sum(spins == 0))\n}\n\n# Count occurrences for different scenarios\nzero_count_1000 &lt;- count_zero_occurrences(spins_1000)\nzero_count_10000 &lt;- count_zero_occurrences(spins_10000)\nzero_count_100000 &lt;- count_zero_occurrences(spins_100000)\nzero_count_1_year &lt;- count_zero_occurrences(spins_1_year)\n\n\n# Create a bar plot for zero occurrences\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\ndata &lt;- data.frame(Scenario = c(\"1000 spins\", \"10000 spins\", \"100000 spins\", \"1 year\"),\n                   Zero_Count = c(zero_count_1000, zero_count_10000, zero_count_100000, zero_count_1_year))\n\nplot &lt;- ggplot(data, aes(x = Scenario, y = Zero_Count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency of Number 0 in Roulette Spins\",\n       x = \"Simulation Scenario\",\n       y = \"Number of 0 occurrences\") +\n  theme_minimal()\n\nprint(plot)\n\n\n\n\n\n\n\n\nAfter simulating roulette spins and analyzing the frequency of the number 0, we’ll gain valuable insights into the likelihood of encountering this specific outcome over different scenarios.\nExploring the probability of specific outcomes in casino games through simulation provides a fascinating glimpse into the world of chance. Whether you’re interested in statistics, data science, or simply curious, these simulations offer an engaging way to understand the dynamics of games of chance.\nFeel free to experiment with different parameters, such as the number of spins or the distribution of numbers on the roulette wheel, and observe how the results change. Happy exploring!\nNote: a fast search with bing about how many time does a roulette spin in a vegas casino : Explore\nThe average roulette wheel spins around 40-50 times per hour when playing roulette at a casino in Las Vegas1. However, this can vary depending on the type of wheel, the number of players at the table, and how quickly bets are placed1. If you want to know more about roulette and the game’s current standing around the world, you can check out this guide which provides a list of the most relevant roulette wheel odds and stats about the game, where you’ll learn everything about one of the world’s most popular casino games and how some odds work in your favor2."
  },
  {
    "objectID": "posts/002_Shiny_Namespace/index.html",
    "href": "posts/002_Shiny_Namespace/index.html",
    "title": "Namespaces in shiny: Why you need them",
    "section": "",
    "text": "library(viridis)  # Import the viridis color palette library\nlibrary(ggplot2)\n\nset.seed(123)  # Set a seed for reproducibility\nnum_flips <- 50000\nflips <- sample(c(\"Heads\", \"Tails\"), num_flips, replace = TRUE)\n\n# Image aspect ratio\naspect_ratio <- 1  # You can customize the aspect ratio here\nn_col <- round(sqrt(num_flips) * aspect_ratio)\nn_row <- ceiling(num_flips / n_col)\n\n# Create a color matrix to represent coin flips\ncolors <- ifelse(flips == \"Heads\", \"red\", \"blue\")\n\n# Create matrices for Heads and Tails\nheads_matrix <- matrix(0, nrow = n_row, ncol = n_col)\ntails_matrix <- matrix(0, nrow = n_row, ncol = n_col)\n\nfor (i in 1:num_flips) {\n  if (flips[i] == \"Heads\") {\n    heads_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  } else {\n    tails_matrix[(i - 1) %/% n_col + 1, (i - 1) %% n_col + 1] <- 1\n  }\n}\n\n# Function to calculate the number of consecutive sequences\ncalculate_sequences <- function(matrix) {\n  sequences <- matrix(0, nrow = nrow(matrix), ncol = ncol(matrix))\n  for (i in 1:nrow(matrix)) {\n    count <- 0\n    for (j in 1:ncol(matrix)) {\n      if (matrix[i, j] == 1) {\n        count <- count + 1\n        sequences[i, j] <- count\n      } else {\n        count <- 0\n      }\n    }\n  }\n  return(sequences)\n}\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Find the longest sequence for Heads and Tails\nlongest_sequence_heads <- max(sequences_heads)\nlongest_sequence_tails <- max(sequences_tails)\n\n# Create images with sequences and titles\npar(mfrow = c(1, 2))  # Display the two images side by side\nimage(t(sequences_heads), col = viridis(100), main = paste(\"Heads Sequences (Max:\", longest_sequence_heads, \")\"), xaxt = \"n\", yaxt = \"n\")\nimage(t(sequences_tails), col = inferno(100), main = paste(\"Tails Sequences (Max:\", longest_sequence_tails, \")\"), xaxt = \"n\", yaxt = \"n\")\n\nlibrary(knitr)\n\n# Calculate sequences for Heads and Tails matrices\nsequences_heads <- calculate_sequences(heads_matrix)\nsequences_tails <- calculate_sequences(tails_matrix)\n\n# Calculate sequence lengths for Heads and Tails\nsequence_lengths_heads <- table(sequences_heads)\nsequence_lengths_tails <- table(sequences_tails)\n\n# Calculate the percentage of sequence lengths\npercentages_heads <- prop.table(sequence_lengths_heads) * 100\npercentages_tails <- prop.table(sequence_lengths_tails) * 100\n\n# Create data frames with lengths, absolute numbers, and percentages\ndataframe_heads <- data.frame(\n  Length = names(sequence_lengths_heads),\n  Absolute_Numbers = as.numeric(sequence_lengths_heads),\n  Percentage = percentages_heads\n)\ndataframe_tails <- data.frame(\n  Length = names(sequence_lengths_tails),\n  Absolute_Numbers = as.numeric(sequence_lengths_tails),\n  Percentage = percentages_tails\n)\n\n# Create formatted tables\nkable(dataframe_heads, caption = \"Table of Heads Sequence Lengths\")\nkable(dataframe_tails, caption = \"Table of Tails Sequence Lengths\")\nIn this example, both module_1 and module_2 share the same IDs for input elements (text_input and action_button). If we interact with one module, it will affect the other module as well, leading to unexpected behavior."
  },
  {
    "objectID": "posts/002_Shiny_Namespace/index.html#using-namespaces-recommended",
    "href": "posts/002_Shiny_Namespace/index.html#using-namespaces-recommended",
    "title": "Namespaces in shiny: Why you need them",
    "section": "Using Namespaces (Recommended)",
    "text": "Using Namespaces (Recommended)\nNow, let’s use namespaces to create unique IDs for each module instance:\n\nlibrary(shiny)\n\nmy_module <- function(id) {\n  ns <- NS(id)\n  tagList(\n    textInput(inputId = ns(\"text_input\"), label = \"Enter text:\"),\n    actionButton(inputId = ns(\"action_button\"), label = \"Click me\")\n  )\n}\n\nui <- fluidPage(\n  my_module(\"module_1\"),\n  my_module(\"module_2\")\n)\n\nIn this example, we use NS to generate unique namespaces for each module instance (module_1 and module_2). As a result, the input element IDs are unique between instances, ensuring that interactions within one module do not affect the other module.\n\nConclusion\nWhen creating Shiny modules, it’s highly recommended to use namespaces (NS) to prevent ID conflicts between module instances. This practice ensures that each module operates independently and avoids unexpected behavior when working with multiple modules in your Shiny app."
  },
  {
    "objectID": "posts/003_Scatterplot/index.html",
    "href": "posts/003_Scatterplot/index.html",
    "title": "Snippet #4: boxplots and scatterplots: simple recipes",
    "section": "",
    "text": "Simulate data, check and assign data types\nCreate a scatterplot with ggplot\nCreate violin plot with ggstatsplot\n\nExample 1: We want to visualize the difference between two groups of patients that follow two different diets. Group A has an average of total cholesterol of 180 with a standard deviation of 20 while Group B and average of 200 with a standard deviation of 40\n\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(data.table)\n\n\nnpatientsA <- 500\nnpatientsB <- 520\ncholA <- mvrnorm(n=npatientsA, mu=180, Sigma=20, empirical=T)\ncholB <- mvrnorm(n=npatientsB, mu=200, Sigma=40, empirical=T)\n\ndataA <- cbind(cholA,rep(\"A\",npatientsA))  \ndataB <- cbind(cholB,rep(\"B\",npatientsB))  \n\ndata <- data.frame(rbind(dataA,dataB))\ncolnames(data) <- c(\"Cholesterol\",\"group\")\ndata$Cholesterol <- as.numeric(data$Cholesterol)\n\np1 <-ggplot(data, aes(x = group, y = Cholesterol)) + geom_jitter(alpha=0.05) \n\np1\n\n\n\n\nA few observations on the code. First of all, we need to input the data in a data.frame otherwise ggplot will give us an error. The second observation is that since we put chr labels on our groups we needed to define Cholesterol as.numeric in order to avoid unwanted resultsstrange results. Try to comment the line data$Cholesterol <- as.numeric(data$Cholesterol) and you can see by yourself what will happen. (hint: a “labelstorm!”)\nJiiter plots is one of my favorite way to represent data. data and immediately understand the distribution of your data and also avoid the pitfall of boxplot (see (Matejka and Fitzmaurice 2017))\nIf you need inferential statistics on your data another resource is (Patil 2021). See the following example with our data. NOTE that we nee to transform the group label as.factor\n\nlibrary(ggstatsplot)\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\ndata$group <- as.factor(data$group)\n\npstack  <- ggbetweenstats(data,group,Cholesterol)\n                          \npstack    \n\n\n\n\n\n\n\n\nReferences\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, May. https://doi.org/10.1145/3025453.3025912.\n\n\nPatil, Indrajeet. 2021. “Visualizations with Statistical Details: The ’Ggstatsplot’ Approach” 6: 3167. https://doi.org/10.21105/joss.03167."
  },
  {
    "objectID": "posts/004_Riddle01/index.html",
    "href": "posts/004_Riddle01/index.html",
    "title": "Unraveling the DnD Dice Duel Riddle with Monte Carlo Simulation in R",
    "section": "",
    "text": "Embark on a journey into the realm of Dungeons & Dragons as we unravel a captivating fiddle riddle involving a dice duel. Using the power of the R programming language and the Monte Carlo simulation method, we’ll simulate the outcomes of duels between two players, each armed with a bag containing six distinct DnD dice. Prepare to explore the fascinating world of probability and randomness! See the riddle posted here by Fiddler on the Proof\nAt a table sit two individuals, each equipped with a bag housing six DnD dice: a d4, a d6, a d8, a d10, a d12, and a d20. The challenge is to randomly select one die from each bag and roll them simultaneously. For example, if a d4 and a d12 are chosen, both players roll their respective dice, hoping for fortuitous results. Monte Carlo Simulation in R:\nTo confront this enigma, we turn to the Monte Carlo method. The following R code snippet initiates a simulation of multiple dice duels, offering a glimpse into the complexities of DnD dice outcomes.\nwe can break down the analysis into different cases:\n\nCase 1: Both players take the same type of dice.\nCase 2: Both players take different types of dice (without repetition of the same combination).\n\nWe’ll generate plots for each case and then provide a summary of the results. Here’s the code:\n\nlibrary(ggplot2)\n# Function to simulate a single dice duel with both players taking the same type of dice\nsimulate_same_dice_duel <- function() {\n  dice_type <- sample(c(4, 6, 8, 10, 12,20,40,64,80,120,128), 1)\n  roll_player1 <- sample(1:dice_type, 1)\n  roll_player2 <- sample(1:dice_type, 1)\n  return(c(dice_type, roll_player1, dice_type, roll_player2))\n}\n\n# Function to simulate a single dice duel with both players taking different types of dice\nsimulate_different_dice_duel <- function() {\n  dice_types <- sample(c(4, 6, 8, 10, 12,20,40,64,80,120,128), 2, replace = FALSE)\n  roll_player1 <- sample(1:dice_types[1], 1)\n  roll_player2 <- sample(1:dice_types[2], 1)\n  return(c(dice_types[1], roll_player1, dice_types[2], roll_player2))\n}\n\n# Monte Carlo simulation for both cases\nnum_trials <- 10000\n\n# Case 1: Both players take the same type of dice\nsame_dice_simulation_results <- replicate(num_trials, simulate_same_dice_duel())\nsame_dice_data <- data.frame(Player = rep(c(\"Player 1\", \"Player 2\"), each = ncol(same_dice_simulation_results)),\n                             Dice_Type = rep(same_dice_simulation_results[1, ], 2),\n                             Roll_Value = as.integer(c(same_dice_simulation_results[2, ], same_dice_simulation_results[4, ])))\n\n# Visualize the results for Case 1 using ggplot2\nggplot(same_dice_data, aes(x = factor(Dice_Type), y = Roll_Value, fill = Player)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 1: Both Players Take the Same Dice (\", num_trials, \"trials)\"),\n       x = \"Dice Type\",\n       y = \"Roll Value\",\n       fill = \"Player\") +\n  theme_minimal()\n\n\n\n# Case 2: Both players take different types of dice\ndifferent_dice_simulation_results <- replicate(num_trials, simulate_different_dice_duel())\ndifferent_dice_data <- data.frame(Player = rep(c(\"Player 1\", \"Player 2\"), each = ncol(different_dice_simulation_results)),\n                                  Dice_Type_Player1 = rep(different_dice_simulation_results[1, ], 2),\n                                  Roll_Value_Player1 = as.integer(c(different_dice_simulation_results[2, ])),\n                                  Dice_Type_Player2 = rep(different_dice_simulation_results[3, ], 2),\n                                  Roll_Value_Player2 = as.integer(c(different_dice_simulation_results[4, ])))\n\n# Visualize the results for Case 2 - Player 1 (Dice 4 vs. Dice 20)\nggplot(subset(different_dice_data, Dice_Type_Player1 %in% c(4, 20)), aes(x = factor(Dice_Type_Player2), y = Roll_Value_Player1)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 1: Dice 4 vs. Dice 20 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 2\",\n       y = \"Roll Value Player 1\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 20)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 20)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 20 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 1 (Dice 4 vs. Dice 12)\nggplot(subset(different_dice_data, Dice_Type_Player1 %in% c(4, 12)), aes(x = factor(Dice_Type_Player2), y = Roll_Value_Player1)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 1: Dice 4 vs. Dice 12 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 2\",\n       y = \"Roll Value Player 1\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 12)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 12)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 12 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Visualize the results for Case 2 - Player 2 (Dice 4 vs. Dice 128)\nggplot(subset(different_dice_data, Dice_Type_Player2 %in% c(4, 128)), aes(x = factor(Dice_Type_Player1), y = Roll_Value_Player2)) +\n  geom_boxplot() +\n  labs(title = paste(\"Case 2 - Player 2: Dice 4 vs. Dice 128 (\", num_trials, \"trials)\"),\n       x = \"Dice Type Player 1\",\n       y = \"Roll Value Player 2\") +\n  theme_minimal()\n\n\n\n# Summarize the results for Case 1 (Same Dice)\nsummary_case1 <- table(same_dice_data$Roll_Value)\n\n# Summarize the results for Case 2 (Different Dice)\nsummary_case2 <- table(different_dice_data$Roll_Value_Player1 == different_dice_data$Roll_Value_Player2)\n\n# Display summaries\ncat(\"\\nSummary of Case 1 - Same Dice:\\n\")\n\n\nSummary of Case 1 - Same Dice:\n\nprint(summary_case1)\n\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n1578 1571 1574 1491 1059 1072  766  766  586  558  407  332  197  210  208  226 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n 217  205  211  222  139  130  116  118  147  130  136  135  121  118  135  128 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n 133  133  132  154  113  148  114  129   62   71   62   79   78   73   83   68 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n  95   76   68   77   78   68   86   92   71   79   85   82   69   72   83   72 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n  57   49   46   51   62   54   44   50   62   49   41   54   48   58   63   64 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n  23   33   27   27   23   39   33   31   30   19   22   33   26   26   24   27 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n  30   41   24   30   39   24   33   34   28   29   31   30   32   30   27   31 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n  29   26   30   24   25   33   23   23    9    7   18   15   10   16   17   13 \n\ncat(\"\\nSummary of Case 2 - Different Dice:\\n\")\n\n\nSummary of Case 2 - Different Dice:\n\nprint(summary_case2)\n\n\nFALSE  TRUE \n19348   652 \n\n\nThrough the marriage of R programming and Monte Carlo simulation, we’ve successfully deciphered the intricacies of the DnD dice duel riddle. Whether you’re a seasoned tabletop gamer or a data science enthusiast, this approach serves as a versatile tool for exploring and comprehending complex scenarios governed by chance. As you embark on your own coding adventures, may the rolls be ever in your favor! Happy coding!"
  },
  {
    "objectID": "posts/005_Pizza/index.html",
    "href": "posts/005_Pizza/index.html",
    "title": "A Mathematical Exploration of Pizza Sizes",
    "section": "",
    "text": "Pizza, a beloved culinary delight, comes in various sizes. To better understand the implications of pizza size on the amount of pizza consumed, we establish a new standard unit called the standard pizza radius, denoted by the letter \\(a\\), which measures 6 inches. This article examines how the area of a pizza changes with size and demonstrates that one extra-large pizza can provide more pizza than two standard-sized pizzas.\nThe area of a pizza, approximated as a circle, with a radius of one standard pizza radius (\\(a\\)) is given by the formula:\n\\[\n\\text{Area}_{\\text{standard}} = \\pi a^2\n\\]\nFor an extra-large pizza with a radius \\(r = 1.5a\\), the area can be calculated as follows:\n\\[\n\\text{Area}_{\\text{extra-large}} = \\pi (1.5a)^2 = \\pi \\cdot 1.5^2 \\cdot a^2 = \\pi \\cdot 2.25 \\cdot a^2\n\\]\nThe combined area of two standard pizzas with radius \\(a\\) is:\n\\[\n\\text{Area}_{\\text{two standard}} = 2 \\cdot \\pi a^2\n\\]\nComparing this to the area of one extra-large pizza:\n\\[\n\\pi \\cdot 2.25 \\cdot a^2 > 2 \\cdot \\pi a^2\n\\]\nSimplifying, we find:\n\\[\n2.25 \\pi a^2 > 2 \\pi a^2\n\\]\nThus, the area of one extra-large pizza is greater than the combined area of two standard pizzas.\nTo determine the minimum radius \\(r = n \\cdot a\\) for the extra-large pizza to always have a greater area than two standard pizzas, we start with the inequality:\n\\[\n\\pi (n \\cdot a)^2 > 2 \\cdot \\pi a^2\n\\]\nSimplifying, we get:\n\\[\nn^2 \\cdot \\pi a^2 > 2 \\cdot \\pi a^2\n\\]\n\\[\nn^2 > 2\n\\]\n\\[\nn > \\sqrt{2}\n\\]\n\\[\nn > 1.4142\n\\]\nTherefore, the radius of the extra-large pizza must be at least \\(\\sqrt{2}\\) times the radius of a standard pizza to ensure its area is always greater than that of two standard pizzas.\nIn Italy, according to the Disciplinare verace pizza napoletana (guidelines for authentic Neapolitan pizza), the radius of a pizza ranges from 22 to 35 cm. Let’s compare the area of two pizzas with a 22 cm radius to one pizza with a 33 cm radius.\n\\[\n2 \\cdot \\pi \\cdot 22^2 = 2 \\cdot \\pi \\cdot 484 = 2 \\cdot 1520.56 = 3039.52 \\, \\text{cm}^2\n\\]\n\\[\n\\pi \\cdot 33^2 = \\pi \\cdot 1089 = 3419.46 \\, \\text{cm}^2\n\\]\nThis calculation confirms that one pizza with a 33 cm radius has a greater area than two pizzas with a 22 cm radius. Therefore, it is mathematically established that consuming one extra-large pizza results in more pizza than consuming two smaller ones."
  },
  {
    "objectID": "posts/006_Monthy_Hall/index.html",
    "href": "posts/006_Monthy_Hall/index.html",
    "title": "Monty Hall simulation",
    "section": "",
    "text": "The Monty Hall problem is a famous probability puzzle. In this scenario, a contestant on a game show is presented with three doors. Behind one door, there is a car, while behind the other two, there are goats. The contestant chooses one door, and then the host, Monty Hall, who knows what is behind each door, opens one of the remaining two doors to reveal a goat.\nThe contestant is then faced with a choice: stick with their initial choice or switch to the other unopened door. What should the contestant do to maximize their chances of winning the car?\nSimulation in R Let’s use R to simulate this problem and analyze the results.\n\n# Set the number of simulations\nnum_simulations &lt;- 10000\n\n# Initialize variables to keep track of wins when switching and staying\nswitch_wins &lt;- 0\nstay_wins &lt;- 0\n\n# Perform the simulations\nfor (i in 1:num_simulations) {\n  # Create three doors with one car and two goats\n  doors &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n\n  # Player's initial choice\n  player_choice &lt;- sample(1:3, 1)\n\n  # Monty Hall reveals a goat behind one of the unchosen doors\n  monty_reveals &lt;- which(doors[-player_choice] == \"goat\")\n  monty_reveals &lt;- monty_reveals[1]  # Monty reveals the first goat he encounters\n\n  # Determine the other unchosen door\n  other_door &lt;- setdiff(1:3, c(player_choice, monty_reveals))\n\n  # Simulate switching doors\n  switch_choice &lt;- other_door[1]\n\n  # Check if the player wins when switching\n  if (doors[switch_choice] == \"car\") {\n    switch_wins &lt;- switch_wins + 1\n  }\n\n  # Check if the player wins when staying\n  if (doors[player_choice] == \"car\") {\n    stay_wins &lt;- stay_wins + 1\n  }\n}\n\n# Calculate win percentages\nswitch_win_percent &lt;- (switch_wins / num_simulations) * 100\nstay_win_percent &lt;- (stay_wins / num_simulations) * 100\n\nNow, let’s create a more visually appealing plot to display the results:\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\nlibrary(ggsci)\np &lt;- ggplot(data.frame(Strategy = c(\"Switch\", \"Stay\"), Win_Percentage = c(switch_win_percent, stay_win_percent)), aes(x = Strategy, y = Win_Percentage)) +\n  geom_bar(stat = \"identity\", fill = c(\"lightblue\", \"papayawhip\")) +\n  labs(title = \"Win Percentage in Monty Hall Problem\",\n       x = \"Strategy\",\n       y = \"Win Percentage (%)\") +\n  theme_minimal()\nprint(p)\n\n\n\n\n\n\n\n\nIn this simulation, we ran 10,000 scenarios of the Monty Hall problem and recorded the results. As the bar plot illustrates, the win percentage is typically higher when switching doors compared to staying with the initial choice.\n(Dickey et al. 1975)\n\n\n\n\nReferences\n\nDickey, James, N. T. Gridgeman, M. C. S. Kingsley, I. J. Good, James E. Carlson, Daniel Gianola, Michael H. Kutner, and Steve Selvin. 1975. “Letters to the Editor.” The American Statistician 29 (3): 131–34. http://www.jstor.org/stable/2683443."
  },
  {
    "objectID": "posts/007_Illusion_Of_Luck/index.html",
    "href": "posts/007_Illusion_Of_Luck/index.html",
    "title": "The Illusion of Luck: How the Number Zero Always Wins in the Casino",
    "section": "",
    "text": "Have you ever wondered why casinos seem to have a mysterious edge, making them consistently profitable? Let’s explore a paradox in the world of gambling, where the number zero takes center stage and helps the house always come out on top.\nTo illustrate this phenomenon, let’s simulate the game of roulette using the R programming language. We’ll focus on a simple bet: predicting whether the ball will land on red or black. In a fair game, the odds of winning such a bet would be 1 to 1. However, the introduction of the number zero alters the dynamics.\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\nplay_roulette &lt;- function(bet, chosen_number, num_spins) {\n  roulette_numbers &lt;- 1:36\n  results &lt;- sample(roulette_numbers, num_spins, replace = TRUE)\n  winnings &lt;- ifelse(results == chosen_number, 36, 0)  # 35 to 1 payout\n  winnings[bet == \"red\" & results %% 2 == 0] &lt;- 2  # win if the color is red\n  winnings[bet == \"black\" & results %% 2 != 0] &lt;- 2  # win if the color is black\n  total_winnings &lt;- sum(winnings)\n  return(total_winnings)\n}\n\n# Simulating plays with and without the house advantage\nset.seed(123)  # Set the seed for reproducibility\nwithout_advantage &lt;- play_roulette(\"red\", 17, 5000)\n\nset.seed(123)  # Reset the same seed for comparison\nwith_advantage &lt;- play_roulette(\"red\", 17, 5000) - 0.05 * 1000  # 5% house advantage\n\n# Comparing results\ncat(\"Without the house advantage: \", without_advantage, \"\\n\")\n\nWithout the house advantage:  10538 \n\ncat(\"With the house advantage: \", with_advantage, \"\\n\")\n\nWith the house advantage:  10488 \n\n# Data for plotting\ndata &lt;- data.frame(\n  Scenario = c(\"Without Advantage\", \"With Advantage\"),\n  Total_Winnings = c(without_advantage, with_advantage)\n)\n\n# Create a bar plot\nggplot(data, aes(x = Scenario, y = Total_Winnings, fill = Scenario)) +\n  geom_bar(stat = \"identity\", position = \"dodge\",fill = c(\"lightblue\", \"papayawhip\")) +\n  labs(title = \"Roulette Simulation: Comparing Outcomes\",\n       x = \"Scenario\",\n       y = \"Total Winnings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n-   This function simulates playing roulette based on specified parameters.\n\n-   It generates random roulette numbers for a given number of spins.\n\n-   Calculates winnings based on the chosen number and adjusts for bets on red or black, considering a 5% house advantage.\n\n\n-   The code uses **`set.seed(123)`** to ensure reproducibility in random number generation.\n\n-   It simulates roulette plays both with and without a house advantage (5% reduction in winnings for the scenario with the house advantage).\n\n\n-   The **`cat`** statements print the total winnings for each scenario, allowing a direct comparison of outcomes.\n\n\n-   Creates a data frame (**`data`**) containing scenarios (\"Without Advantage\" and \"With Advantage\") and their corresponding total winnings.\n\n\n-   Utilizes ggplot2 to generate a bar plot.\n\n-   **`geom_bar`** represents the data as bars, positioned side by side (**`position = \"dodge\"`**).\n\n-   The plot is customized with a minimal theme, a title (\"Roulette Simulation: Comparing Outcomes\"), and axis labels (\"Scenario\" and \"Total Winnings\").\n\n-   Different colors (\"lightblue\" and \"papayawhip\") are assigned to each scenario for clarity.\nIn our simulation, we introduced a slight modification to the payouts, reducing them just enough to create a 5% advantage for the house. This mirrors the real-world scenario where the presence of zero in roulette gives the casino an edge.\nRunning our simulation both with and without the house advantage reveals a stark contrast. The casino consistently comes out ahead in the long run, showcasing how the number zero plays a crucial role in tipping the odds in favor of the house.\nThe illusion of luck in casinos often stems from subtle yet significant factors, such as the presence of zero in games like roulette. Understanding these nuances can provide valuable insights into the mechanics of gambling and why, ultimately, the house always wins.\nAs you ponder the next spin of the roulette wheel, remember that behind the excitement lies a carefully crafted system where even the seemingly neutral zero plays a pivotal role in ensuring the casino’s enduring success."
  },
  {
    "objectID": "posts/008_ggplot_loops/index.html",
    "href": "posts/008_ggplot_loops/index.html",
    "title": "Snippet #1: ggplot loops",
    "section": "",
    "text": "Create an empty list\nPopulate your list with objects (ggplots)\nCreate iteratively names for the objects\nRename the objects inside the list using the name list generated previously\nShow all plots using wrap_plots\n\nInstead of using boring plots we will use our private art collections and items.\nOne great package to create your art in R is aRtsy Let’s fire it up\n\nrequire(aRtsy)\n\nLoading required package: aRtsy\n\nrequire(patchwork)\n\nLoading required package: patchwork\n\n\n\n#before starting for having a look at the palette \n?colorPalette\n\nstarting httpd help server ... done\n\n\nCreate a Mondrian and save it\n\nset.seed(34)\nComposition_10 <- canvas_squares(colors = colorPalette(\"boogy2\"))\nsaveCanvas(Composition_10 , filename = \"Mondrian.png\")\nComposition_10 \n\n\n\n\nand another one\n\nset.seed(1)\naspect_ratio <- 1\nheight <- 2\nComposition_1 = canvas_segments(colors = colorPalette(\"blackwhite\"))\nComposition_1 \n\n\n\n\nor if you want to create a lots of them, create names automatically and then take a look at just one of your artistic composition in your collection use the following code:\n\nn_items <- 3\ncollection <- list()\nname_of_Composition  <- list()\nfor (i in 1:n_items) {\n  seed <-  (sample(1:100000,1)) + 1\n  name_of_Composition[[i]] <- paste0(\"Composition_\", i)\n  collection[[i]] <- canvas_squares(colors = colorPalette(\"boogy2\"))\n  \n}\nnames(collection) <- name_of_Composition\n\ncollection\n\n$Composition_1\n\n\n\n\n\n\n$Composition_2\n\n\n\n\n\n\n$Composition_3\n\n\n\n\n\n\n#as you can notice the setting for figure output in this chunk was changed in order to showplots with a rato of 3:1\nwrap_plots(collection)\n\n\n\n\n[Wickham (2016)](Derks 2022)(Pedersen 2022)\n\n\n\n\nReferences\n\nDerks, Koen. 2022. “aRtsy: Generative Art with ’Ggplot2’.” https://CRAN.R-project.org/package=aRtsy.\n\n\nPedersen, Thomas Lin. 2022. “Patchwork: The Composer of Plots.” https://CRAN.R-project.org/package=patchwork.\n\n\nWickham, Hadley. 2016. “Ggplot2: Elegant Graphics for Data Analysis.” https://ggplot2.tidyverse.org."
  },
  {
    "objectID": "posts/009_Dice_rolls/index.html",
    "href": "posts/009_Dice_rolls/index.html",
    "title": "Central Limit Theorem with Dice Rolls",
    "section": "",
    "text": "Dice rolling is a pastime enjoyed by many, whether in board games or games of chance. But have you ever wondered how the sum of multiple dice rolls behaves when you roll them repeatedly? In this blog post, we embark on a journey into the world of dice rolls and the fascinating Central Limit Theorem. We’ll uncover how the sum of dice rolls can transform into a Gaussian distribution as the number of rolls increases.\nTo fully grasp how the sum of dice rolls approaches a Gaussian distribution, let’s start with an overview of dice rolls. Imagine rolling a standard six-sided die, the kind commonly found in board games. Each roll yields a number between 1 and 6, with each outcome equally likely at 1/6.\nNow, what happens when we roll two dice and sum the results? In this scenario, the range of possible sums expands. You could obtain a sum of 2 (when both dice show 1) or a sum of 12 (when both dice display 6), with all possible combinations in between. For instance, you could achieve a sum of 7 when one die shows 3 and the other shows 4, or a sum of 4 when one die shows 2 and the other shows 2.\nVisualizing the sum of two dice rolls can be done using a bar chart. Notably, there are many more possible combinations resulting in a sum of 7 compared to combinations yielding sums of 2 or 12. This creates a triangular distribution, with a central peak at the most likely sum (in the case of two dice, that’s 7) and tails tapering off as you move away from the peak.\nHowever, this triangular distribution will change as we increase the number of dice rolled and the total number of rolls. By continuing with our simulation, we will witness how this distribution transforms into a Gaussian distribution as we increase the value of n, as predicted by the Central Limit Theorem.\nLet’s see the theory in action. We’ll use R to simulate multiple dice rolls and visualize how the distribution changes with an increasing value of n.\n\n# Load the necessary library\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\n# Number of dice rolls and number of dice\nn_rolls &lt;- 10000\nn_dice &lt;- c(2, 3, 4, 5, 6, 7,8,9, 10, 20, 50, 100)  # You can add more values\n\n# Function to simulate dice rolls and calculate the sum\nsimulate_dice_rolls &lt;- function(n_rolls, n_dice) {\n  results &lt;- replicate(n_rolls, sum(sample(1:6, n_dice, replace = TRUE)))\n  return(results)\n}\n\n# Simulate dice rolls for various values of n_dice\nresults_list &lt;- lapply(n_dice, function(n) simulate_dice_rolls(n_rolls, n))\n\n# Create a data frame to store the results\nresults_df &lt;- data.frame(N_Dice = rep(n_dice, each = n_rolls), Sum = unlist(results_list))\n\n# Create a histogram to visualize the distributions\np &lt;- ggplot(results_df, aes(x = Sum)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  facet_wrap(~N_Dice, scales = \"free\") +\n  labs(title = \"Sum of Dice Rolls vs. Number of Dice\",\n       x = \"Sum of Dice Rolls\",\n       y = \"Frequency\")\n\nprint(p)\n\n\n\n\n\n\n\n\nIn conclusion, the Central Limit Theorem offers an intriguing insight into the behavior of dice rolls. As we roll dice more and more times and sum the results, the distribution of the sums converges towards a Gaussian distribution, defying the original distribution of the dice rolls. This exemplifies the power and predictability of probability theory."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html",
    "href": "posts/010_Data_Visualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is the crucial first step in the data analysis process. Before applying complex statistical models or machine learning algorithms, it is essential to understand the structure, trends, and peculiarities of the data you are working with. In this introductory section, we will explore the fundamental concepts of EDA and its role in data analysis.\n\n\nEDA serves several purposes:\n\nUnderstanding Data: EDA helps us become familiar with the dataset, identify the available variables, and understand their nature (numeric, categorical, temporal, etc.).\nDetecting Patterns: EDA allows us to detect patterns, relationships, and potential outliers within the data. This is critical for making informed decisions during the analysis.\nData Cleaning: Through EDA, we can identify missing values, outliers, or data inconsistencies that require cleaning and preprocessing.\nFeature Engineering: EDA may suggest feature engineering opportunities, such as creating new variables or transforming existing ones to better represent the underlying data.\nHypothesis Generation: EDA often leads to the generation of hypotheses or research questions, guiding further investigation.\nCommunicating Insights: EDA produces visualizations and summaries that facilitate the communication of insights to stakeholders or team members.\n\nIn the following sections, we will delve into the practical aspects of EDA, starting with data simulation and visualization techniques.\n\n\n\nBefore diving into Exploratory Data Analysis (EDA) on real datasets, it’s helpful to begin with the generation of simulated data. This allows us to have full control over the data and create example scenarios to understand key EDA concepts. In this section, we will learn how to generate simulated datasets using R.\n\n\nTo start, let’s define some basic parameters that we’ll use to generate simulated data:\n\nx_min: The minimum value for the variable x.\nx_max: The maximum value for the variable x.\nx_step: The increment between successive x values.\ny_mean: The mean value for the dependent variable y.\ny_sd: The standard deviation for the dependent variable y.\ny_min: The minimum possible value for y.\ny_max: The maximum possible value for y.\n\nWe will use these parameters to generate sample data.\nNow, let’s proceed to generate sample data based on the defined parameters. In this example, we’ll create a simple dataset with the following variables:\n\nx: A sequence of values ranging from x_min to x_max with an increment of x_step.\nvar_random: A random variable with values uniformly distributed between y_min and y_max.\nvar_norm: A variable with values generated from a normal distribution with mean y_mean and standard deviation y_sd.\nvar_sin: A variable with values generated as the sine function of x.\n\nHere’s the R code to create the sample dataset:\n\nlibrary(data.table)\n\n# Parameters\nx_min   &lt;- 0\nx_max   &lt;- 10   \nx_step  &lt;- 0.01\n\ny_mean  &lt;- 0.5\ny_sd    &lt;- 0.25\ny_min   &lt;- -1\ny_max   &lt;- 1     \n\nx       &lt;- seq(x_min, x_max, x_step)\n\n# Variables\nvar_random  &lt;- runif(length(x), y_min, y_max)\nvar_norm    &lt;- rnorm(length(x), y_mean, y_sd) \nvar_sin     &lt;- sin(x)\n\n# Data.frame \ndf  &lt;- data.frame(x, var_random, var_norm, var_sin)\ndt  &lt;- data.table(df)\n\n# Melt \ndtm &lt;- melt(dt, id.vars=\"x\")\n\nThis code creates a dataset df and a data.table dt containing the generated variables. The melt function from the data.table library is used to reshape the data for visualization purposes.\nWith our simulated data ready, we can now move on to creating various plots and performing EDA.\nIn this section, we will explore various visualization techniques that play a crucial role in Exploratory Data Analysis (EDA). Visualizations help us gain insights into the data’s distribution, patterns, and relationships between variables. We will use the simulated dataset generated in the previous section to illustrate these techniques.\n\n\n\n\nThe choice of visualization depends on the nature of your data and the specific aspects you want to highlight. Generally, in EDA, we often need to:\n\nExamine Changes Over Time: Use time series plots when you want to assess changes in one or more variables over time.\nCheck for Data Distribution: Create distribution plots, such as histograms and density plots, to understand how data points are distributed.\nExplore Variable Relationships: Employ correlation plots and scatter plots to identify linear relationships between variables.\n\nLet’s start by examining these aspects one by one using our simulated dataset.\n\n\nTo explore changes over time, we’ll create a time series plot for the var_sin variable. This variable represents a sine wave and is well-suited for a time series representation. Here’s the R code to create a time series plot:\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\np &lt;- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\n\n\n\n\nIn this code, we use ggplot2 to create a line plot for the var_sin variable.\n\n\n\nTo check the data distribution, we’ll create histogram plots for each of the variables: var_random, var_norm, and var_sin. Histograms provide a visual representation of the frequency distribution of data values. Here’s the R code:\n\np3 &lt;- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 &lt;- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\n\n\n\n\np4\n\n\n\n\n\n\n\np5\n\n\n\n\n\n\n\n\nThese plots will help us understand the distribution characteristics of our variables.\n\n\n\nCorrelation plots allow us to examine relationships between variables. We’ll create scatter plots for pairs of variables to assess their linear correlation. Here’s an example for var_sin and var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  &lt;- runif(x,y_min,y_max)\nvar_norm2    &lt;- rnorm(x,y_mean,y_sd) \nvar_sin2     &lt;- sin(x) + rnorm(x,0,0.01) \n\ndf2&lt;- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 &lt;- data.table(df2)\n\np10 &lt;- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\n\n\n\n\nThese scatter plots help us identify whether variables exhibit linear correlation.\nIn the following sections, we’ll delve deeper into each of these plot types, interpret the results, and explore additional visualization techniques for EDA.\n\n\nBox plots, also known as box-and-whisker plots, provide a summary of the data’s distribution, including median, quartiles, and potential outliers. They are particularly useful for comparing the distributions of different variables or groups. Here’s an example of creating box plots for var_random, var_norm, and var_sin:\n\np6 &lt;- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\n\n\n\n\nBox plots can reveal variations and central tendencies of the variables.\n\n\n\nPair plots, or scatterplot matrices, allow us to visualize pairwise relationships between multiple variables in a dataset. They are helpful for identifying correlations and patterns among variables simultaneously. Here’s how to create a pair plot for our dataset:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot &lt;- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\nWarning in geom_point(): All aesthetics have length 1, but the data has 16 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nPair plots provide a comprehensive view of variable interactions.\n\n\n\nTime series data often contain underlying components such as trends and seasonality that can be crucial for understanding the data’s behavior. Time series decomposition is a technique used in Exploratory Data Analysis (EDA) to separate these components. In this section, we’ll demonstrate how to perform time series decomposition using our simulated var_sin data.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp &lt;- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\n\n\n\n\nThe code above performs the following:\n\nDecomposes the var_sin time series using the decompose function. We specify a frequency of 365 since the data represents daily observations.\nPlots the decomposed components, including the original time series, trend, seasonal component, and residual.\n\nThe resulting plot will show the individual components of the time series, allowing us to gain insights into its underlying patterns.\n\n\n\nInteractive plots, created using libraries like plotly or shiny, allow users to explore data interactively. You can create interactive scatter plots, line plots, or heatmaps, enhancing the user’s ability to dig deeper into the data.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nCaricamento pacchetto: 'plotly'\n\n\nIl seguente oggetto è mascherato da 'package:ggplot2':\n\n    last_plot\n\n\nIl seguente oggetto è mascherato da 'package:stats':\n\n    filter\n\n\nIl seguente oggetto è mascherato da 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot &lt;- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"&lt;br&gt;var_random:\", var_random, \"&lt;br&gt;var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %&gt;%\n  add_markers() %&gt;%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn this initial section, we’ve introduced the fundamental concepts of exploratory data analysis (EDA) and the importance of data visualization in gaining insights from complex datasets. We’ve explored various types of plots and their applications in EDA.\nNow, let’s dive deeper and enhance our understanding by demonstrating practical examples of EDA using real-world datasets. We’ll showcase how different types of plots and interactive visualizations can provide valuable insights and drive data-driven decisions.\nLet’s embark on this EDA journey and uncover the hidden stories within our data through hands-on examples.\n\n\n\n\n\n\n\n\n\n\nIn this section, we’ll dive into a meaningful example of time series decomposition to demonstrate its practical utility in Exploratory Data Analysis (EDA). Time series decomposition allows us to extract valuable insights from time-dependent data. We’ll use our simulated var_sin time series to illustrate its significance.\n\n\n\nImagine we have daily temperature data for a city over several years. We want to understand the underlying patterns in temperature variations, including trends and seasonality, to make informed decisions related to weather forecasts, climate monitoring, or energy management.\nLet’s create the enhanced dataset with temperature data for multiple cities. We’ll use the data.table library to manage the dataset efficiently:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities &lt;- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date &lt;- as.Date(\"2010-01-01\")\nend_date &lt;- as.Date(\"2019-12-31\")\ndate_seq &lt;- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data &lt;- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature &lt;- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp &lt;- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\n\n\n\n\nWe’ve generated temperature data for each city over the span of ten years, resulting in a diverse and complex dataset.\n\n\n\nNow that we have our multi-city temperature dataset, let’s apply time series decomposition to analyze temperature trends and seasonality for one of the cities, such as New York (see plot)\n\n\n\nThe plot will display the components of the time series for New York, including the original time series, trend, seasonal component, and residual. Similar analyses can be performed for other cities to identify regional temperature patterns.\n\n\n\nWith our enhanced multi-city temperature dataset and time series decomposition, we can:\n\nRegional Analysis: Compare temperature patterns across different cities to identify regional variations.\nSeasonal Insights: Understand how temperature seasonality differs between cities and regions.\nLong-Term Trends: Analyze temperature trends for each city over the ten-year period.\n\nThis advanced analysis helps us make informed decisions related to climate monitoring, urban planning, and resource management.\n\n\n\n\n\n\nIn this section, we’ll illustrate the significance of distribution plots in Exploratory Data Analysis (EDA) by considering a practical scenario. Distribution plots help us understand how data points are distributed and can reveal insights about the underlying data characteristics. We’ll use our simulated dataset and focus on the var_random variable.\n\n\n\nImagine we have a dataset containing exam scores of students in a class. We want to gain insights into the distribution of exam scores to answer questions like:\n\nWhat is the typical exam score?\nAre the exam scores normally distributed?\nAre there any outliers or unusual patterns in the scores?\n\n\n\n\nLet’s create a histogram to visualize the distribution of exam scores using the var_random variable. This will help us answer the questions posed above.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\n\n\n\n\n\nThe resulting histogram will display the distribution of exam scores. Here’s what we can interpret:\n\nTypical Exam Score: The histogram will show where the majority of exam scores lie, indicating the typical or central value.\nDistribution Shape: We can assess whether the scores follow a normal distribution, are skewed, or have other unique characteristics.\nOutliers: Outliers, if present, will appear as data points far from the central part of the distribution.\n\n\n\n\nBy analyzing the distribution of exam scores, we can:\n\nIdentify Central Tendency: Determine the typical exam score, which can be useful for setting benchmarks or evaluating student performance.\nUnderstand Data Characteristics: Gain insights into the shape of the distribution, which informs us about the data’s characteristics.\nDetect Outliers: Identify outliers or unusual scores that may require further investigation.\n\n\n\n\n\nIn this section, we’ll explore advanced correlation analysis using more complex datasets. We’ll create two datasets: one representing students’ academic performance, and the other containing information about their study habits and extracurricular activities. We’ll investigate correlations between various factors to gain deeper insights.\n\n\nLet’s create the two complex datasets for our correlation analysis:\nAcademic Performance Dataset:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students &lt;- 500\n\nacademic_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\n\nNow that we have our complex datasets, let’s perform advanced correlation analysis to explore relationships between academic performance, study habits, and extracurricular activities. We’ll calculate correlations and visualize them using a heatmap:\n\n# Calculate correlations between variables\ncorrelation_matrix &lt;- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nThe resulting heatmap visually represents the correlations between academic performance and study-related factors. Here’s what we can interpret:\n\nColor Intensity: The color intensity indicates the strength and direction of the correlation. Positive correlations are shown in blue, while negative correlations are in red. The darker the color, the stronger the correlation.\nCorrelation Coefficients: The heatmap displays the actual correlation coefficients as labels in the lower triangle. These values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n\n\n\n\nBy conducting advanced correlation analysis, we can:\n\nUnderstand Complex Relationships: Explore intricate correlations between academic performance, study hours, extracurricular activities, and stress levels.\nIdentify Key Factors: Determine which factors have the most significant impact on academic performance.\nOptimize Student Support: Use insights to provide targeted support and interventions for students.\n\nAdvanced correlation analysis helps us uncover nuanced relationships within complex datasets.\n\n\n\n\nIn this section, we’ll explore the versatility of box plots by working with diverse and complex datasets. We’ll create two datasets: one representing the distribution of monthly sales for multiple product categories, and the other containing information about customer demographics. These datasets will allow us to visualize various types of distributions and identify outliers.\n\n\nLet’s create the two complex datasets for our box plot analysis:\nSales Dataset and Customer Demographics Dataset:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months &lt;- 24\nproduct_categories &lt;- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data &lt;- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers &lt;- 300\n\ndemographics_data &lt;- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 &lt;- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 &lt;- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\n\n\n\n\n\nThese box plots help us gain insights into diverse distributions:\n\nSales Distribution: We can observe how sales are distributed across different product categories, identifying variations and potential outliers.\nCustomer Age Distribution: The box plot displays the spread of customer ages, highlighting the central tendency and any potential outliers.\n\n\n\n\nBy using box plots with complex datasets, we can:\n\nAnalyze Diverse Distributions: Visualize and compare distributions of sales for multiple product categories and customer age distributions.\nOutlier Detection: Identify potential outliers in both sales data and customer demographics.\nSegmentation Insights: Understand how sales vary across product categories and the age distribution of customers.\n\nBox plots are versatile tools for exploring various types of data distributions and making data-driven decisions.\n\n\n\n\n\n\nSuppose we have a dataset containing monthly stock prices for three companies: Company A, Company B, and Company C. We want to create an interactive time series plot that allows users to:\n\nSelect the company they want to visualize.\nZoom in and out to explore specific time periods.\nHover over data points to view detailed information.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months &lt;- 24\n\ntime_series_data &lt;- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot &lt;- plot_ly(data = time_series_data, x = ~Date) %&gt;%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %&gt;%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\nInteractive data visualizations are a powerful tool for EDA and data presentation. In the next section, we’ll explore another advanced visualization technique: time series decomposition.\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\n[Wickham (2016)](Schloerke et al. 2021)"
  },
  {
    "objectID": "posts/011_Data_Simulation/index.html",
    "href": "posts/011_Data_Simulation/index.html",
    "title": "Snippet #3: Functions for simulating data",
    "section": "",
    "text": "Example of creating variables using runif and rnorm\nWriting a function that wraps all\n\nFirst of all we use the runif and rnorm to have a look how they work.\n\nlibrary(data.table)\nx_min   &lt;- 0\nx_max   &lt;- 10   \nx_step  &lt;- 0.01\n\ny_mean  &lt;- 0.5\ny_sd    &lt;- 0.25\ny_min   &lt;- -1\ny_max   &lt;- 1   \n\nx       &lt;- seq(x_min,x_max,x_step)\nvar_random  &lt;- runif(x,y_min,y_max)\nvar_norm    &lt;- rnorm(x,y_mean,y_sd) \n\ndf  &lt;- data.frame (x,var_random,var_norm)\ndt  &lt;- data.table(df)\n\n\nsimpleDataset &lt;- function(number_of_rows,means,sds)\n{\nl &lt;- length(means)\nres &lt;- lapply(seq(1:l),function(x) \n       eval(\n       parse(\n       text=paste(\"rnorm(\",number_of_rows,\",\",means[x],\",\",sds[x],\")\",sep=\"\"))\n       )\n       ) \ndat &lt;- data.frame((sapply(res,c)))\nid &lt;- rownames(dat)\ndat &lt;-  cbind(id=id,dat)\ndt &lt;- data.table(dat)\nreturn(dt)\n}\n\nExample 1: We simulate the values of the LDL cholesterol of 2 patients in 3 different times. The first one patient (X1) has an average value of 200 of LDL with a standard variation of 2 while the second (X2) has an average of 150 with a standard deviation of 10. Note: All values are expressed in mg/dL\n\ndataset1 &lt;- simpleDataset(3,c(200,180),c(2,10))\ndataset1\n\n   id       X1       X2\n1:  1 200.1841 167.1265\n2:  2 199.9952 200.8787\n3:  3 201.2486 187.5768\n\n\nExample 2: this time we combine runif and simpleDataset. We simulate the values of the LDL cholesterol of 5 patients in 7 different times. The values for each patient are between a min = 100 and a max = 150 with a standard deviation between a min sd = 10 and max sd = 40. We also simulate two time that presents outliers values between a min = 180 and max = 200 and an min sd = 10 and max sd = 40 . We merge the values for each patient (7 times + 2 outliers times) and finally we use the function melt to reshape the dataset.\n\ndat1 &lt;- simpleDataset(number_of_rows=7,\n                      means=runif(5,100,150),\n                      sds=runif(5,10,40))\noutliers &lt;- simpleDataset(number_of_rows=2,\n                      means=runif(5,180,200),\n                      sds=runif(5,10,40))                 \n\ndat1\n\n   id        X1       X2        X3        X4        X5\n1:  1 153.59476 150.6474 119.10800 169.21507 111.45022\n2:  2 136.25580 197.2160 109.72680 103.96251 103.52508\n3:  3  88.40632 127.6134  94.28173  96.85567 152.28825\n4:  4 114.22804 144.0820 169.41607 132.85758 124.59658\n5:  5 147.04467 140.8414  75.96130  92.81705 107.48489\n6:  6 130.92207 137.8937 130.39466  83.80787 119.87301\n7:  7 125.79370 114.3462  56.15695  97.22265  62.44163\n\noutliers\n\n   id       X1       X2       X3       X4       X5\n1:  1 192.4697 146.3098 155.0556 179.5488 151.9553\n2:  2 195.1163 163.9017 183.5246 189.1330 163.9440\n\ndato     &lt;-rbind(dat1,outliers) \ndt.melt &lt;- melt(dat1, id.vars=\"id\")\ncolnames(dt.melt) &lt;- c(\"id\",\"category\",\"var1\")\ndt.melt$ncat &lt;- as.numeric(dt.melt$category)\n\ndt.melt\n\n    id category      var1 ncat\n 1:  1       X1 153.59476    1\n 2:  2       X1 136.25580    1\n 3:  3       X1  88.40632    1\n 4:  4       X1 114.22804    1\n 5:  5       X1 147.04467    1\n 6:  6       X1 130.92207    1\n 7:  7       X1 125.79370    1\n 8:  1       X2 150.64741    2\n 9:  2       X2 197.21595    2\n10:  3       X2 127.61337    2\n11:  4       X2 144.08198    2\n12:  5       X2 140.84145    2\n13:  6       X2 137.89369    2\n14:  7       X2 114.34618    2\n15:  1       X3 119.10800    3\n16:  2       X3 109.72680    3\n17:  3       X3  94.28173    3\n18:  4       X3 169.41607    3\n19:  5       X3  75.96130    3\n20:  6       X3 130.39466    3\n21:  7       X3  56.15695    3\n22:  1       X4 169.21507    4\n23:  2       X4 103.96251    4\n24:  3       X4  96.85567    4\n25:  4       X4 132.85758    4\n26:  5       X4  92.81705    4\n27:  6       X4  83.80787    4\n28:  7       X4  97.22265    4\n29:  1       X5 111.45022    5\n30:  2       X5 103.52508    5\n31:  3       X5 152.28825    5\n32:  4       X5 124.59658    5\n33:  5       X5 107.48489    5\n34:  6       X5 119.87301    5\n35:  7       X5  62.44163    5\n    id category      var1 ncat\n\nstr(dt.melt)\n\nClasses 'data.table' and 'data.frame':  35 obs. of  4 variables:\n $ id      : chr  \"1\" \"2\" \"3\" \"4\" ...\n $ category: Factor w/ 5 levels \"X1\",\"X2\",\"X3\",..: 1 1 1 1 1 1 1 2 2 2 ...\n $ var1    : num  153.6 136.3 88.4 114.2 147 ...\n $ ncat    : num  1 1 1 1 1 1 1 2 2 2 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "posts/012_Coin_Toss/index.html",
    "href": "posts/012_Coin_Toss/index.html",
    "title": "Exploring Coin Flip Sequences with Simulation in R",
    "section": "",
    "text": "In this blog post, we will dive into the fascinating world of coin flip sequences. Leveraging the capabilities of R, we will conduct simulations to gain insights into the probabilities of consecutive sequences of heads or tails.\nLet’s kick things off by simulating coin flips using R. We’ll conduct experiments with varying numbers of flips to observe how the results evolve.\n\n# Function to simulate coin tosses\nsimulate_tosses &lt;- function(n) {\n  set.seed(42)  # Set seed for reproducibility\n  flips &lt;- sample(c(\"Heads\", \"Tails\"), n, replace = TRUE)\n  return(flips)\n}\n\n# Simulate coin flips for different scenarios\nflips_100 &lt;- simulate_tosses(100)\nflips_1000 &lt;- simulate_tosses(1000)\nflips_10000 &lt;- simulate_tosses(10000)\nflips_100000 &lt;- simulate_tosses(100000)\n\nMoving on, we’ll analyze the consecutive sequences of heads or tails in each simulation. We’ll count the length of these sequences to gain insights into their distribution.\n\n# Function to count consecutive sequences\ncount_consecutive_sequences &lt;- function(flips) {\n  count &lt;- rep(0, length(flips))\n  counter &lt;- 1\n\n  for (i in 2:length(flips)) {\n    if (flips[i] == flips[i-1]) {\n      counter &lt;- counter + 1\n    } else {\n      counter &lt;- 1\n    }\n    count[i] &lt;- counter\n  }\n\n  return(count)\n}\n\n# Function to create a plot for consecutive sequences\ncreate_plot &lt;- function(consecutive_counts, n) {\n  data &lt;- data.frame(Launch = 1:length(consecutive_counts), Consecutive = consecutive_counts)\n  \n  library(ggplot2)\n  \n  plot &lt;- ggplot(data, aes(x = Launch, y = Consecutive)) +\n    geom_line() +\n    labs(title = paste(\"Consecutive Sequences -\", n, \"Tosses\"),\n         x = \"Toss Number\",\n         y = \"Consecutive Count\") +\n    theme_minimal()\n  \n  return(plot)\n}\n\n# Analyze consecutive sequences for different scenarios\nconsecutive_counts_100 &lt;- count_consecutive_sequences(flips_100)\nconsecutive_counts_1000 &lt;- count_consecutive_sequences(flips_1000)\nconsecutive_counts_10000 &lt;- count_consecutive_sequences(flips_10000)\nconsecutive_counts_100000 &lt;- count_consecutive_sequences(flips_100000)\n\nNow, let’s create visualizations to better understand the distribution of consecutive sequences. We’ll use the ggplot2 library to craft insightful graphs and gridExtra to arrange them together.\n\n# Create plots\nplot_100 &lt;- create_plot(consecutive_counts_100, 100)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\nplot_1000 &lt;- create_plot(consecutive_counts_1000, 1000)\nplot_10000 &lt;- create_plot(consecutive_counts_10000, 10000)\nplot_100000 &lt;- create_plot(consecutive_counts_100000, 100000)\n\n# Use gridExtra to arrange the plots\nlibrary(gridExtra)\ngrid.arrange(plot_100, plot_1000, plot_10000, plot_100000, ncol = 2)\n\n\n\n\n\n\n\n\n\n# Function to simulate coin tosses and plot the maximum consecutive sequence\nsimulate_and_plot_max_sequence &lt;- function(n) {\n  # Simulate coin tosses\n  flips &lt;- simulate_tosses(n)\n  \n  # Analyze consecutive sequences\n  consecutive_counts &lt;- count_consecutive_sequences(flips)\n  \n  # Return the maximum consecutive sequence\n  return(max(consecutive_counts))\n}\n\n# Vector to store results\nmax_sequences &lt;- c()\n\n# Perform simulations with increasing numbers of tosses\nfor (n_tosses in c(100, 2000, 10000, 100000)) {\n  max_sequence &lt;- simulate_and_plot_max_sequence(n_tosses)\n  max_sequences &lt;- c(max_sequences, max_sequence)\n}\n\n# Plot the results\nlibrary(ggplot2)\n\ndata &lt;- data.frame(Num_Tosses = c(100, 2000, 10000, 100000), Max_Sequence = max_sequences)\n\nplot &lt;- ggplot(data, aes(x = Num_Tosses, y = Max_Sequence)) +\n  geom_line() +\n  labs(title = \"Maximum Consecutive Sequence vs. Number of Tosses\",\n       x = \"Number of Tosses\",\n       y = \"Max Consecutive Sequence\") +\n  theme_minimal()\n\nprint(plot)\n\n\n\n\n\n\n\n\nAfter simulating coin flips and analyzing consecutive sequences, we’ve gained valuable insights into the probability of encountering various sequences. As expected, with a larger number of flips, the likelihood of encountering longer consecutive sequences increases.\nConclusion Exploring probability through simulation is a powerful way to grasp the nuances of random processes. Whether you’re a statistician, data scientist, or simply curious, these simulations provide a fascinating glimpse into the world of chance.\nFeel free to experiment with different parameters, such as the number of flips or the probability of getting heads or tails, and observe how the results change. Happy exploring!"
  },
  {
    "objectID": "posts/013_Clean_csv/index.html",
    "href": "posts/013_Clean_csv/index.html",
    "title": "Snippet #2: Cleaning column names of an imported csv",
    "section": "",
    "text": "Import data from a csv file\nUse the function clean_names from (Firke 2023)j R function\nWrite a function in base using gsub and regex to tackle specific issues\nYou’re done\n\nFirst of all we import the csv using the library (Müller 2020)here\n\nlibrary(here)\n\nhere() starts at I:/giorgioluciano.github.io/Blog\n\nfile_in <- \"FakeData.csv\"\npath_in <- \"posts/013_Clean_csv/\"\ndata <- read.csv(here(path_in,file_in), head=T, check.names=F, encoding=\"latin1\")\n\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ndata_fixed <- clean_names(data)\n\nAnd now the function written by William Doane\n\nclinical_names <- function(.data, unique = FALSE) {\n  n <- if (is.data.frame(.data)) colnames(.data) else .data\n  n <- gsub(\"cvrisk\", \"CVrisk\", n , ignore.case=T)\n  n <- gsub(\"hbo\", \"HBO\", n , ignore.case=T)\n  n <- gsub(\"ft4\", \"fT4\", n , ignore.case=T)\n  n <- gsub(\"f_t4\", \"fT4\", n , ignore.case=T)\n  n <- gsub(\"ft3\", \"fT3\", n , ignore.case=T)\n  n <- gsub(\"f_t3\", \"fT3\", n , ignore.case=T)\n  n <- gsub(\"ldl\", \"LDL\", n , ignore.case=T)\n  n <- gsub(\"hdl\", \"HDL\", n , ignore.case=T)\n  n <- gsub(\"hba1c\", \"HbA1C\", n, ignore.case=T)\n  n <- gsub(\"hbac1\", \"HbA1C\", n, ignore.case=T)\n  n <- gsub(\"hb_ac1\", \"HbA1C\",n,ignore.case=T)\n  n <- gsub(\"\\\\igf\\\\b\", \"IGF\", n , ignore.case=T)\n  n <- gsub(\"tsh\", \"TSH\", n , ignore.case=T)\n  n <- gsub(\"acth\", \"ACTH\", n, ignore.case=T)\n  n <- gsub(\"\\\\Na\\\\b\", \"Sodio\", n)\n  n <- gsub(\"\\\\K\\\\b\",  \"Potassio\", n)\n  n <- gsub(\"\\\\P\\\\b\",  \"Fosforo\", n)\n  n <- gsub(\"\\\\pas\\\\b\", \"PAS\", n, ignore.case=T)\n  n <- gsub(\"\\\\pad\\\\b\", \"PAD\", n, ignore.case=T)\n  n <- gsub(\"\\\\pth\\\\b\", \"PTH\", n, ignore.case=T)\n  n <- gsub(\"\\\\clu\\\\b\", \"CLU\", n, ignore.case=T)\n  n <- gsub(\"\\\\tg\\\\b\", \"TG\", n, ignore.case=T)\n  n <- gsub(\"\\\\glic\\\\b\", \"glicemia\", n, ignore.case=T)\n  if (unique) n <- make.unique(n, sep = \"_\")\n  if (is.data.frame(.data)) {\n    colnames(.data) <- n\n    .data\n  } else {\n    n\n  }\n}\n\n\ndata_clean <- clinical_names(data_fixed)\n\ncomparison <- cbind(data.frame((colnames(data))),\n                        data.frame((colnames(data_fixed))),\n                        data.frame((colnames(data_clean))))\n\ncolnames(comparison) <- c(\"original\",\"fixed\",\"clean\") \n\ncomparison\n\n           original             fixed             clean\n1          paziente          paziente          paziente\n2               età               eta               eta\n3               SEX               sex               sex\n4          diagnosi          diagnosi          diagnosi\n5           terapia           terapia           terapia\n6             tempo             tempo             tempo\n7            Cvrisk            cvrisk            CVrisk\n8              peso              peso              peso\n9        delta Peso        delta_peso        delta_peso\n10              BMI               bmi               bmi\n11         deltaBMI         delta_bmi         delta_bmi\n12              PAS               pas               PAS\n13         deltaPas         delta_pas         delta_PAS\n14              pad               pad               PAD\n15         deltaPad         delta_pad         delta_PAD\n16              HBO               hbo               HBO\n17           neutro            neutro            neutro\n18            linfo             linfo             linfo\n19             glic              glic          glicemia\n20    deltaglicemia     deltaglicemia     deltaglicemia\n21            HBAC1             hbac1             HbA1C\n22       deltaHbAc1      delta_hb_ac1       delta_HbA1C\n23            sodio             sodio             sodio\n24         potassio          potassio          potassio\n25           calcio            calcio            calcio\n26          fosforo           fosforo           fosforo\n27      colesterolo       colesterolo       colesterolo\n28 deltaColesterolo delta_colesterolo delta_colesterolo\n29              HDL               hdl               HDL\n30         deltaHDL         delta_hdl         delta_HDL\n31              ldl               ldl               LDL\n32         deltaLDL         delta_ldl         delta_LDL\n33               TG                tg                tg\n34          deltaTG          delta_tg          delta_tg\n35             ACTH              acth              ACTH\n36        cortisolo         cortisolo         cortisolo\n37              CLU               clu               CLU\n38              IGF               igf               IGF\n39              TSH               tsh               TSH\n40              fT4              f_t4               fT4\n41              PTH               pth               PTH\n42       Vitamina D        vitamina_d        vitamina_d\n43          dose_CA           dose_ca           dose_ca\n44          dose_HC           dose_hc           dose_hc\n45          dose_PL           dose_pl           dose_pl\n46 dose equivalente  dose_equivalente  dose_equivalente\n\n\n\n\n\n\nReferences\n\nFirke, Sam. 2023. “Janitor: Simple Tools for Examining and Cleaning Dirty Data.” https://CRAN.R-project.org/package=janitor.\n\n\nMüller, Kirill. 2020. “Here: A Simpler Way to Find Your Files.” https://CRAN.R-project.org/package=here."
  },
  {
    "objectID": "posts/014_Card_Shuffling/index.html",
    "href": "posts/014_Card_Shuffling/index.html",
    "title": "Exploring Card Shuffling: A Visual Journey",
    "section": "",
    "text": "Today, we embark on a visual exploration of card shuffling, delving into a captivating technique known as the Faro shuffle. Unlike conventional shuffling, the Faro shuffle promises not just randomness but a mathematical symphony that unfolds card by card, a dance that echoes with the precision of numbers.\nIn the quest to unravel the intricacies of card shuffling, we turn to the Faro shuffle, a technique that mimics the graceful dance of cards. The accompanying R code brings this technique to life, allowing us to simulate the position of the ace of hearts after multiple Faro shuffles.\n\n# Function to perform a faro shuffle on a deck of cards\nfaro &lt;- function(deck, num_shuffles) {\n  for (i in 1:num_shuffles) {\n    half1 &lt;- deck[1:(length(deck)/2)]\n    half2 &lt;- deck[(length(deck)/2 + 1):length(deck)]\n    deck &lt;- c()\n    for (j in 1:length(half1)) {\n      deck &lt;- c(deck, half1[j], half2[j])\n    }\n  }\n  return(deck)\n}\n\nHere, the faro function is defined to simulate the Faro shuffle. It takes a deck of cards (deck) and the number of shuffles to perform (num_shuffles). The function iterates through the halves of the deck, interleaving the cards to simulate the Faro shuffle. This process is repeated for the specified number of shuffles.\n\n# Install the required packages if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\n# Load the necessary packages\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\n# Function to simulate the position of the ace of hearts after shuffling\nsimulate_ace_of_hearts_position &lt;- function(num_shuffles, num_simulations) {\n  original_deck &lt;- 1:52\n  ace_position &lt;- numeric(num_simulations)\n  \n  for (i in 1:num_simulations) {\n    shuffled_deck &lt;- faro(original_deck, num_shuffles)\n    ace_position[i] &lt;- which(shuffled_deck == 12)\n  }\n  \n  return(ace_position)\n}\n\n# Simulation parameters\nnum_shuffles_list &lt;- seq(1, 30, by = 1)\nnum_simulations &lt;- 100\n\n# Run the simulation\nace_positions &lt;- lapply(num_shuffles_list, function(num_shuffles) {\n  simulate_ace_of_hearts_position(num_shuffles, num_simulations)\n})\n\n# Prepare data for the plot\ndf &lt;- data.frame(\n  Num_Shuffles = rep(num_shuffles_list, each = num_simulations),\n  Ace_Position = unlist(ace_positions)\n)\n\n# Create a line plot with a ribbon in ggplot2\nggplot(df, aes(x = Num_Shuffles, y = Ace_Position)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Ace_Position - 5, ymax = Ace_Position + 5), fill = \"lightblue\", alpha = 0.3) +\n  geom_text(aes(label = Ace_Position), vjust = -0.5, hjust = 0.5, size = 3) +\n  labs(title = \"Ace of Hearts Position relative to Initial Position\",\n       x = \"Number of Shuffles\",\n       y = \"Ace of Hearts Position\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the simulation parameters section, we define the number of Faro shuffles to explore (num_shuffles_list) and the number of simulations to run for each shuffle scenario (num_simulations). The resulting dataframe (’df’) contains the number of shuffles and the corresponding positions of the ace of hearts after simulation.Finallym the code utilizes ggplot2 to create a line plot with a ribbon that represents the range of possible ace of hearts positions after each Faro shuffle. The resulting visualization allows us to witness the mesmerizing journey of the ace of hearts through the rhytmic Faro shuffle.\nThrough this visual exploration, we gain insights into the inherent randomness of card shuffling. The fluctuating position of the ace of hearts highlights the complex interplay of probability and chance in card games.\nFeel free to experiment with the provided R code, adjusting parameters and exploring different aspects of card shuffling. The visual representation serves as a captivating way to understand the nuances of this seemingly simple yet intriguing process."
  },
  {
    "objectID": "posts/015_Birthday_Paradox/index.html",
    "href": "posts/015_Birthday_Paradox/index.html",
    "title": "The Birthday Paradox: When Probability Plays Tricks",
    "section": "",
    "text": "The Birthday Paradox is a probabilistic problem concerning the likelihood that two people in a group share the same birthday. At first glance, it might seem like the probability is very low, but in reality, it’s higher than you might think.\nThe paradox is based on the fact that there are many possible combinations of people’s birthdays within a group. While it might appear that there is only a small chance that two people share a birthday, things change when we consider the entire group.\nLet’s dive into action and use the R programming language to simulate the Birthday Paradox. We will see how the probabilities change as the group size increases.\n\n# Number of simulations\nnum_simulations &lt;- 10000\n\n# Function to check if there are shared birthdays in a group\ncheck_shared_birthday &lt;- function(group_size) {\n  birthdays &lt;- sample(1:365, group_size, replace = TRUE)\n  if (length(birthdays) == length(unique(birthdays))) {\n    return(FALSE)  # No shared birthdays\n  } else {\n    return(TRUE)   # Shared birthdays\n  }\n}\n\n# Simulate the Birthday Paradox\nsimulate_birthday_paradox &lt;- function(group_size) {\n  shared_birthday_count &lt;- 0\n  for (i in 1:num_simulations) {\n    if (check_shared_birthday(group_size)) {\n      shared_birthday_count &lt;- shared_birthday_count + 1\n    }\n  }\n  return(shared_birthday_count / num_simulations)\n}\n\n# Test the Birthday Paradox simulation for different group sizes\ngroup_sizes &lt;- 2:100\nresults &lt;- numeric(length(group_sizes))\n\nfor (i in 1:length(group_sizes)) {\n  results[i] &lt;- simulate_birthday_paradox(group_sizes[i])\n}\n\n# Plot the results\nplot(group_sizes, results, type = \"l\", xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\", main = \"Birthday Paradox Simulation\")\nabline(h = 0.5, col = \"red\", lty = 2)  # Add a line at 0.5 for reference\n\n\n\n\n\n\n\n\nThe Birthday Paradox is a captivating example of how probability can be counterintuitive. Despite our intuitions, the probability of finding two people with the same birthday is higher than it seems. This paradox is an opportunity to explore the laws of probability and how they can surprise us."
  },
  {
    "objectID": "posts/016_Anscombe/index.html",
    "href": "posts/016_Anscombe/index.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Anscombe’s Quartet, known as the “Anscombe’s Test,” consists of four datasets with very similar descriptive statistics but visually distinct characteristics. These quartets serve as an enlightening example of the importance of visualizing data before drawing conclusions.\nIn this post, we will delve into how to calculate and visualize Anscombe’s Quartet using R and the powerful ggplot2 library. We’ll also use custom functions to generate these quartets and analyze them.\nAnscombe’s Quartet was created by the statistician Francis Anscombe in 1973 to underscore the importance of data visualization before analysis. Despite having similar statistics, these datasets exhibit significantly different visual behaviors. Let’s see how R and ggplot2 help us explore them.\nTo get started, we need to load some libraries:\n\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nAnscombe’s Quartet comprises four datasets, each with 11 data points. Here’s a brief overview of the quartet:\n\nDataset 1: A straightforward linear relationship between X and Y.\nDataset 2: A linear relationship with an outlier.\nDataset 3: A linear relationship with one point substantially different from the others.\nDataset 4: A non-linear relationship.\n\n(see Rpubs page)\n\nlibrary(datasets)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\ndatasets::anscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nanscombe_tidy <- anscombe %>%\n    mutate(observation = seq_len(n())) %>%\n    gather(key, value, -observation) %>%\n    separate(key, c(\"variable\", \"set\"), 1, convert = TRUE) %>%\n    mutate(set = c(\"I\", \"II\", \"III\", \"IV\")[set]) %>%\n    spread(variable, value)\n\nhead(anscombe_tidy)\n\n  observation set  x    y\n1           1   I 10 8.04\n2           1  II 10 9.14\n3           1 III 10 7.46\n4           1  IV  8 6.58\n5           2   I  8 6.95\n6           2  II  8 8.14\n\nggplot(anscombe_tidy, aes(x, y)) +\n    geom_point() +\n    facet_wrap(~ set) +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNow, let’s integrate your custom R code and examples to generate and visualize Anscombe’s Quartet:\n\nlibrary(vtable)\n\nLoading required package: kableExtra\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(kableExtra)\nlibrary(patchwork)\n\n\n# Note: Function to generate Anscombe's Quartet datasets for x2 we need a trick that can be also improved but for now as a brutal approx works\n\n\nplotreg <- function(df) {\n  formula <- y ~ x\n  \n  ggplot(df, aes(x = x, y = y)) +\n    geom_point(aes(size = 1), alpha = 0.3) +\n    geom_smooth(method = \"lm\", formula = formula, se = TRUE) +\n    coord_cartesian(xlim = c(4, 19), ylim = c(4, 14)) +  # Imposta i limiti di x e y\n    theme_light(base_size = 10) +\n    theme(legend.position = \"none\")\n}\n\ngenerate_noisy_points <- function(x, y, noise_level = 0.1) {\n  n <- length(x)\n  \n  # Generate random noise for x and y separately\n  noise_x <- rnorm(n, mean = 0, sd = noise_level)\n  noise_y <- rnorm(n, mean = 0, sd = noise_level)\n  \n  # Ensure that the sum of noise on x and y is approximately zero\n  noise_x <- noise_x - mean(noise_x)\n  noise_y <- noise_y - mean(noise_y)\n  \n  # Add noise to the original data\n  x_noisy <- x + noise_x\n  y_noisy <- y + noise_y\n  \n  return(data.frame(x = x_noisy, y = y_noisy))\n}\n\n# Function to generate approximated points with an option to add noise\ngenerate_approximated_points <- function(n, x, y, noise_level = 0) {\n  # Create a new interpolation based on the original data\n  interpolated_values <- approx(x, y, xout = seq(min(x), max(x), length.out = n))\n  \n  # Extract the interpolated points\n  x_interp <- interpolated_values$x\n  y_interp <- interpolated_values$y\n  \n  # Add noise if needed\n  if (noise_level > 0) {\n    noise <- rnorm(n, mean = 0, sd = noise_level)\n    x_interp <- x_interp + noise\n    y_interp <- y_interp + noise\n  }\n  \n  # Return the approximated points\n  return(data.frame(x = x_interp, y = y_interp))\n}\n\n\nmultians <- function(npoints = 11, anscombe) {\n  x1 <- anscombe$x1\n  x2 <- anscombe$x2\n  x3 <- anscombe$x3\n  x4 <- anscombe$x4\n  y1 <- anscombe$y1\n  y2 <- anscombe$y2\n  y3 <- anscombe$y3\n  y4 <- anscombe$y4\n\n  ## Generate Quartet 1 ##\n  x_selected <- c(x1[2], x1[4], x1[11])\n  y_selected <- c(y1[2], y1[4], y1[11])\n\n  # Calculate the linear regression\n  linear_model <- lm(y_selected ~ x_selected)\n\n  # Extract coefficients of the line\n  intercept <- coef(linear_model)[1]\n  slope <- coef(linear_model)[2]\n\n  # Create a sinusoidal curve above or below the line\n  x_sin <- seq(min(x1), max(x1), length.out = npoints)  # x range for the sinusoid\n  amplitude <- 1  # Amplitude of the sinusoid\n  frequency <- 4  # Frequency of the sinusoid\n  phase <- pi / 2  # Phase of the sinusoid (for rotation)\n  sinusoid <- amplitude * sin(2 * pi * frequency * (x_sin - min(x1)) / (max(x1) - min(x1)) + phase)\n\n  # Generate points above or below the line\n  y_sin <- slope * x_sin + intercept + sinusoid\n  df1 <- data.frame(x = x_sin, y = y_sin)\n\n  ## Generate Quartet 2 ##\n  n_points_approximated <- npoints\n  noise_level <- 0.1\n\n  # Generate approximated points\n  approximated_points <- generate_approximated_points(n_points_approximated, x2, y2, noise_level = 0.1)\n\n  # Add noise to the approximated points\n  noisy_approximated_points <- generate_noisy_points(approximated_points$x, approximated_points$y, noise_level)\n\n  # Now, you have noisy approximated points in df2\n  df2 <- data.frame(x = noisy_approximated_points$x, y = noisy_approximated_points$y)\n\n  ## Generate Quartet 3 ##\n  lm_model <- lm(y3 ~ x3, subset = -c(3))\n  x_generated <- seq(min(x3), max(x3), length.out = npoints)\n  y_generated <- predict(lm_model, newdata = data.frame(x3 = x_generated))\n\n  x_outlier <- 13\n  y_outlier <- 12.74\n  x_generated <- c(x_generated, x_outlier)\n  y_generated <- c(y_generated, y_outlier)\n\n  df3 <- data.frame(x = x_generated, y = y_generated)\n\n  ## Generate Quartet 4 ##\n  \n  y4[9]\n  x <- c(rep(min(x4),npoints))\n  y <- c(seq(min(y4[-8]), max(y4[-8]), length.out = (npoints)))\n  x_new = c(x,x4[8])\n  y_new = c(y,y4[8])\n  df4 <- data.frame(x = x_new, y = y_new)\n\n  return(list(df1 = df1, df2 = df2, df3 = df3, df4 = df4))\n}\n\n\n\n# Generate and plot Quartet 1\nt1 <- multians(33,anscombe)\n\np1 <- plotreg(t1$df1)\np3 <- plotreg(t1$df3)\np4 <- plotreg(t1$df4)\np2 <- plotreg(t1$df2)\n\n(p1 | p2) / (p3 | p4)\n\n\n\n# Example of eight summaries (replace them with your own)\nsummary1 <- st(t1$df1)\nsummary5 <- st(data.frame(anscombe$x1,anscombe$y1))\n\nsummary2 <- st(t1$df2)\nsummary6 <- st(data.frame(anscombe$x2,anscombe$y2))\n\nsummary3 <- st(t1$df3)\nsummary7 <- st(data.frame(anscombe$x3,anscombe$y3))\n\nsummary4 <- st(t1$df4)\nsummary8 <- st(data.frame(anscombe$x4,anscombe$y4))\n\n\nsummary1 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    x \n    33 \n    9 \n    3 \n    4 \n    6.5 \n    12 \n    14 \n  \n  \n    y \n    33 \n    8.3 \n    2.2 \n    4.7 \n    6.5 \n    10 \n    13 \n  \n\n\n\n\nsummary5 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    anscombe.x1 \n    11 \n    9 \n    3.3 \n    4 \n    6.5 \n    12 \n    14 \n  \n  \n    anscombe.y1 \n    11 \n    7.5 \n    2 \n    4.3 \n    6.3 \n    8.6 \n    11 \n  \n\n\n\n\nsummary2 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    x \n    33 \n    9 \n    3 \n    4 \n    6.5 \n    11 \n    14 \n  \n  \n    y \n    33 \n    7.6 \n    1.8 \n    3 \n    6.6 \n    8.9 \n    9.3 \n  \n\n\n\n\nsummary6 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    anscombe.x2 \n    11 \n    9 \n    3.3 \n    4 \n    6.5 \n    12 \n    14 \n  \n  \n    anscombe.y2 \n    11 \n    7.5 \n    2 \n    3.1 \n    6.7 \n    8.9 \n    9.3 \n  \n\n\n\n\nsummary3\n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    x \n    34 \n    9.1 \n    3.1 \n    4 \n    6.6 \n    12 \n    14 \n  \n  \n    y \n    34 \n    7.3 \n    1.4 \n    5.4 \n    6.3 \n    8.1 \n    13 \n  \n\n\n\n\nsummary7 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    anscombe.x3 \n    11 \n    9 \n    3.3 \n    4 \n    6.5 \n    12 \n    14 \n  \n  \n    anscombe.y3 \n    11 \n    7.5 \n    2 \n    5.4 \n    6.2 \n    8 \n    13 \n  \n\n\n\n\nsummary4 \n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    x \n    34 \n    8.3 \n    1.9 \n    8 \n    8 \n    8 \n    19 \n  \n  \n    y \n    34 \n    7.2 \n    1.4 \n    5.2 \n    6.2 \n    8 \n    12 \n  \n\n\n\n\nsummary8\n\n\n\nSummary Statistics\n \n  \n    Variable \n    N \n    Mean \n    Std. Dev. \n    Min \n    Pctl. 25 \n    Pctl. 75 \n    Max \n  \n \n\n  \n    anscombe.x4 \n    11 \n    9 \n    3.3 \n    8 \n    8 \n    8 \n    19 \n  \n  \n    anscombe.y4 \n    11 \n    7.5 \n    2 \n    5.2 \n    6.2 \n    8.2 \n    12 \n  \n\n\n\n\n\nAnscombe’s Quartet is a powerful reminder that descriptive statistics alone may not reveal the complete story of your data. Visualization is a crucial tool in data analysis, helping you uncover patterns, outliers, and unexpected relationships that numbers alone might miss.\n(Szafir 2018; Anscombe 1973)\n\n\n\n\nReferences\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21. http://www.jstor.org/stable/2682899.\n\n\nSzafir, Danielle Albers. 2018. “The Good, the Bad, and the Biased: Five Ways Visualizations Can Mislead (and How to Fix Them).” Interactions 25 (4): 26–33. https://doi.org/10.1145/3231772."
  },
  {
    "objectID": "posts/018_Excel_Ita/index.html",
    "href": "posts/018_Excel_Ita/index.html",
    "title": "Criteri Essenziali per La Gestione Dati",
    "section": "",
    "text": "Excel non è un Database: Linee Guida Essenziali per una Corretta Gestione dei Dati\nÈ fondamentale comprendere la differenza tra un foglio di calcolo e un database, nonché l’importanza di una corretta gestione dei dati. Di seguito sono riportati motivi per cui Excel non dovrebbe essere utilizzato come database, insieme a criteri essenziali per gestire e analizzare i dati in modo efficace.\n\nPerché Excel non è un Database\n1. Limitata integrità e validazione dei dati\nExcel permette di inserire qualsiasi tipo di dato in qualsiasi cella, senza un controllo rigido sulla coerenza. I database, al contrario, richiedono la definizione di tipi di dati e limitano gli inserimenti in base a queste regole, garantendo maggiore integrità.\n2. Mancanza di robuste funzionalità di sicurezza\nI database offrono un controllo più sofisticato sugli accessi, con ruoli, permessi e log di attività. In Excel, chiunque con accesso al file può modificarlo senza lasciare traccia, mettendo a rischio la sicurezza dei dati.\n3. Difficoltà nella gestione di grandi set di dati\nQuando i dati superano certe dimensioni (milioni di righe o centinaia di colonne), Excel diventa inefficiente e può bloccarsi. I database sono progettati per gestire grandi volumi di dati in modo efficiente.\n4. Limitate capacità di collaborazione multiutente\nExcel ha funzionalità di condivisione, ma i conflitti tra versioni possono facilmente emergere. I database, invece, sono progettati per la collaborazione simultanea senza perdita di dati.\n5. Assenza di strumenti complessi di interrogazione e reporting\nLe query SQL nei database permettono analisi molto più complesse rispetto a quanto sia possibile in Excel. I database offrono anche reportistica avanzata con strumenti di Business Intelligence (BI).\nClicca qui per ulteriori info\n\n\nCriteri Essenziali per la Gestione dei Dati\nQuando si gestiscono dati per l’analisi, è importante rispettare determinati criteri per garantire accuratezza ed efficienza.\n\n1. Convenzioni di Denominazione dei File\nIl nome di un file deve essere chiaro, descrittivo e privo di caratteri speciali. Utilizzare nomi significativi è essenziale per garantire che chiunque possa capire il contenuto del file senza doverlo aprire. Evitare nomi vaghi come “database_ultima_versione” o “file_finale”, che non offrono informazioni utili.\nEsempi corretti:\n\n“2024924_nome_trial_autore_versione.csv”\n“2023_report_emoglobina_livelli.xls”\n“esame_glucosio_pazienti_clinica_X_v3.xlsx”\n\nEsempi da evitare:\n\n“file_versione_definitiva_finale.xlsx”\n“dati123.xls”\n\nEvitare caratteri speciali:\nEvita l’uso di caratteri come:\n/ \\ * & # % @ ( ) [ ] { }\nEsempi corretti:\n\n“dati_pazienti_2023.xlsx”\n“analisi_vitaminaD_v2.csv”\n\nEsempi da evitare:\n\n“dati/pazienti(2023).xlsx”\n“report&finale#versione.xls”\n\n\n\n2. Convenzioni di Denominazione delle Colonne e delle Righe\nI nomi di colonne e righe devono essere chiari e coerenti. Un nome significativo aiuta a capire immediatamente cosa rappresentano i dati senza necessità di ulteriori spiegazioni.\nNomi significativi:\nUtilizzare nomi che descrivano esattamente i dati contenuti. Ad esempio, invece di usare nomi come “var1” o “colonna1”, usare nomi più specifici come “eta_paziente” o “Vitamina_D”.\nEsempi corretti:\n\n“nome_paziente”\n“eta”\n“altezza”\n“peso”\n“BMI”\n“Vitamina_D”\n\nEsempi da evitare:\n\n“colonna1”\n“dato_A”\n“x1”\n\nEvitare caratteri speciali:\nCome per i nomi dei file, evitare caratteri come spazi, trattini e simboli speciali (“/”, “&”, “#”, etc.) e lettere accentate. Usare l’underscore (_) o il camelCase (es. “nomeCliente”) per separare le parole.\n\n\n3. Coerenza nei Nomi\nEvitare l’uso misto di maiuscole e minuscole, inglese e italiano, spazi e underscore. I nomi devono seguire un formato coerente per evitare confusione.\nEsempi da evitare:\n\n“NomePaziente” (uso misto di maiuscole e minuscole)\n“pressione_Sanguigna” (uso misto di lingue)\n“data nascita” (uso di spazi)\n\nEsempi corretti:\n\n“nome_paziente”\n“pressione_sistolica”\n“data_nascita”\n\n\n\n4. Sensibilità alle Maiuscole\nUtilizzare lettere maiuscole e minuscole in modo coerente e intenzionale. La distinzione tra maiuscole e minuscole può essere significativa, soprattutto nei database case-sensitive.\n\n\n5. Celle Vuote\nUna cella vuota può avere diversi significati. Assicurarsi di chiarire sempre cosa indica una cella vuota (dati non misurati, fuori scala, mancanti, etc.) per evitare ambiguità.\n\n\n6. Zero vs. Dati Mancanti\nAssicurarsi di distinguere tra uno zero e un dato mancante. Utilizzare valori specifici come “NA” o “null” per indicare dati mancanti e non confonderli con zeri.\n\n\n7. Notazione Numerica Coerente\nLe virgole e i punti hanno significati diversi nella notazione numerica a seconda del paese. Assicurarsi di utilizzare una notazione numerica coerente per evitare ambiguità nei valori.\n\n\n8. Evitare Duplicati\nNon ci dovrebbero essere righe o colonne duplicate, a meno che non ci sia una ragione specifica e giustificata. I duplicati possono falsare i risultati dell’analisi.\nSeguendo queste linee guida, i vostri dati saranno puliti, coerenti e pronti per un’analisi accurata\n\n\n\nUlteriori Linee Guida per Dati Clinici\nQuando si gestiscono dati clinici o sensibili, sono necessarie ulteriori precauzioni:\n\nIdentificazione del Paziente\nAssicurarsi che i nomi dei pazienti (o gli ID se i nomi non possono essere riportati) siano coerenti e corretti. Errori nei dati identificativi possono portare a gravi conseguenze cliniche.\nControllo di Errori Macroscopici\nControllare errori macroscopici come altezze di 170 metri, pazienti che pesano 8 kg o con BMI irrealistici (&gt;30 o &lt;1). Questi errori evidenti possono influire negativamente sull’analisi.\nFormattazione Condizionale\nApplicare una formattazione condizionale ai dati quando possibile (es. evidenziare automaticamente le celle con valori fuori dal range atteso). Questo può aiutare a individuare rapidamente e visivamente gli errori nei dati. Clicca qui per una guida alla formattazione condizionale\n\n\n\nFAQ: Domande Frequenti su Excel e la Gestione dei Dati\n1. Posso utilizzare Excel come database per piccoli set di dati?\nSì, Excel può essere utilizzato per piccoli set di dati, soprattutto se si tratta di progetti temporanei o analisi rapide. Tuttavia, anche per set di dati piccoli, un database garantisce maggiore integrità, sicurezza e scalabilità, evitando potenziali problemi quando il volume dei dati cresce.\n2. Perché Excel non è consigliato per dati sensibili come quelli clinici?\nExcel non offre controlli di accesso avanzati o log di modifica, esponendo i dati a errori accidentali o manipolazioni non tracciate. Inoltre, non è possibile crittografare o proteggere adeguatamente i dati sensibili in un foglio di calcolo, aumentando il rischio di violazioni della privacy.\n3. Quali sono i principali rischi di sicurezza nell’utilizzare Excel per la gestione dei dati clinici?\nI principali rischi includono:\n\nModifiche accidentali o intenzionali non tracciate.\nAccesso non autorizzato ai dati.\nMancanza di crittografia e protezione dei dati.\nDifficoltà nel mantenere la coerenza e l’integrità dei dati, specialmente in ambienti multiutente.\n\n4. È possibile limitare chi può accedere e modificare i dati in un file Excel?\nExcel offre alcune funzionalità di protezione, come la protezione delle celle e l’impostazione di password per l’accesso ai fogli. Tuttavia, queste misure sono deboli rispetto ai sistemi di sicurezza avanzati disponibili in un database relazionale, che permettono controlli granulari su chi può visualizzare o modificare i dati.\n5. Come posso gestire i duplicati in Excel?\nExcel fornisce strumenti per individuare e rimuovere duplicati, ma il controllo deve essere eseguito manualmente o tramite formule. Nei database, le chiavi primarie possono essere utilizzate per garantire che non ci siano duplicati nei record, offrendo una maggiore automazione e affidabilità.\n6. Cosa devo fare se il mio file Excel diventa troppo grande e lento da usare?\nSe un file Excel diventa troppo grande, è probabile che tu abbia superato i limiti operativi del foglio di calcolo. In questi casi, è consigliabile migrare i dati in un database che può gestire volumi molto più grandi di righe e colonne in modo efficiente, senza rallentamenti.\n7. Come posso evitare di commettere errori di formattazione numerica (es. virgola e punto decimale)?\nAssicurati di impostare correttamente le preferenze regionali nel software che utilizzi per gestire i dati. Nei database, puoi specificare in modo preciso il formato numerico accettato per evitare ambiguità, cosa che Excel non può garantire automaticamente senza un’attenta configurazione."
  },
  {
    "objectID": "posts/019_Excel_Eng/index.html",
    "href": "posts/019_Excel_Eng/index.html",
    "title": "Non-Negotiables for Your Data",
    "section": "",
    "text": "Non-Negotiables for Your Data\nIn today’s data-driven world, it is essential to understand the difference between a spreadsheet and a database, as well as the importance of proper data management. This post will explain why Excel should not be used as a database and provide key guidelines for effectively managing and analyzing data.\n\nWhy Excel Is Not a Database\nWhile Excel is a powerful tool for many tasks, it’s important to recognize its limitations when it comes to data management. Here are some key reasons why:\n\nLimited Data Integrity and Validation\nExcel allows users to input any type of data in any cell without strict consistency checks. Databases, on the other hand, enforce data types and restrict entries based on predefined rules.\nLack of Robust Security Features\nDatabases provide more advanced access control, including roles, permissions, and activity logs. In Excel, anyone with file access can modify the data without any trace.\nDifficulty Handling Large Datasets\nAs data grows (millions of rows or hundreds of columns), Excel becomes inefficient and prone to crashing. Databases are designed to handle large datasets efficiently.\nLimited Multi-user Collaboration\nAlthough Excel offers sharing features, version conflicts can easily arise. Databases are built for simultaneous collaboration without data loss.\nLack of Advanced Querying and Reporting Tools\nSQL queries in databases allow for far more complex analyses than Excel can handle. Additionally, databases offer advanced reporting features with Business Intelligence (BI) tools.\n\nFor serious data management needs, it’s advisable to use a dedicated database system like MySQL, PostgreSQL, or MongoDB.\n\n\nKey Guidelines for Data Management\nWhen someone provides data for analysis, it’s important to adhere to certain standards to ensure accurate and efficient analysis.\n\n1. File Naming Conventions\nFile names should be clear, descriptive, and free of special characters. Using meaningful names is essential to ensure that anyone can understand the content of the file without having to open it. Avoid vague or generic names like “database_latest_version” or “final_file”, which provide little context.\nHere are some best practices for naming files:\n\nGood examples:\n\n“yyyymmdd_trial_name_author_revision”\n“hospitalXYZ_patient_data.csv”\n\nExamples to avoid:\n\n“final_final_version_file.xlsx”\n“data123.xls”*\n\n\nAvoid special characters like spaces or symbols that can cause issues in different systems and software. Characters to avoid include:\n\n/, \\, *, &, #, %, @\n\nInstead, use underscores (_) to separate words (e.g., “correct_file_name.xlsx”).\n\n\n2. Column and Row Naming Conventions\nJust like file names, column and row names should follow the same rules of clarity and consistency. Meaningful column names help everyone immediately understand the data without needing extra explanations.\nHere are some guidelines for naming columns and rows:\n\nUse meaningful names:\nColumns should clearly describe the data they contain. Instead of using generic names like “var1” or “column1”, use more specific names like “patient_age” or “quantity_sold”.\nGood examples:\n\n“patient_name”, “Vitamin D” , “Cholesterol”\n\nExamples to avoid:\n\n“column1”, “data_A”, “x1”\n\nAvoid special characters:\nLike file names, avoid characters like spaces, dashes, or special symbols (“/”, “&”, “#”, etc.). Use underscores (_) or camelCase (e.g., “customerName”) to separate words in column names.\n\n\nCase Sensitivity\nBe intentional with the use of uppercase and lowercase letters to differentiate categories, data types, or names. Case sensitivity can be important, especially when working with case-sensitive databases or programming languages.\nEmpty Cells\nAn empty cell has meaning. Is it an unmeasured value? An out-of-scale value? A missing value? Or data that cannot be retrieved? Always clarify the meaning of empty cells to avoid ambiguity.\nZero vs. Missing Data\nZero is different from missing data. Ensure that this distinction is clear in the dataset, perhaps by using a specific value (e.g., “NA” or “null”) to indicate missing data.\nConsistent Numeric Notation\nCommas and periods have different meanings in numerical notation depending on the region (e.g., 1,000 in the US means “one thousand,” whereas in Europe it may mean “one”). Use consistent and clear numeric notation throughout the document.\nAvoid Duplicates\nThere should be no duplicate rows or columns unless there is a specific and justified reason for their presence. Duplicates can skew analysis results.\n\nFollowing these guidelines will ensure your data is clean, consistent, and ready for accurate analysis. Remember, good data management practices are the foundation for reliable research and decision-making."
  },
  {
    "objectID": "posts/020_Formattazione_condizionale/index.html",
    "href": "posts/020_Formattazione_condizionale/index.html",
    "title": "Formattazione condizionale in Excel",
    "section": "",
    "text": "Formattazione Condizionale\nLa formattazione condizionale in Excel consente di applicare automaticamente stili di formattazione (come colori delle celle, stili di carattere o bordi) alle celle in base a determinate condizioni o criteri specifici. Ecco i passaggi per eseguire una formattazione condizionale in Excel:\n\nPassaggi per eseguire la formattazione condizionale in Excel\n\nSeleziona le celle da formattare:\n\nEvidenzia l’intervallo di celle su cui vuoi applicare la formattazione condizionale. Ad esempio, se vuoi applicarla a un elenco di valori numerici, seleziona l’intervallo come A1\n\nApri il menu di formattazione condizionale:\n\nVai alla scheda Home della barra multifunzione.\nNel gruppo Stili, clicca su Formattazione condizionale.\n\nScegli il tipo di regola: Dal menu a discesa della formattazione condizionale, puoi scegliere diversi tipi di regole in base alle tue esigenze:\n\nRegole predefinite:\n\nEvidenzia regole celle: Usa questa opzione per applicare la formattazione a celle che soddisfano una condizione come “maggiore di”, “minore di”, “tra”, “testo che contiene”, ecc.\nBarre dati: Applica barre colorate all’interno delle celle per rappresentare i valori numerici.\nScale di colori: Applica diverse sfumature di colori in base ai valori delle celle.\nSet di icone: Applica icone (frecce, segni di spunta, semafori) per rappresentare i valori delle celle rispetto a una scala di valori.\n\nNuova regola: Clicca su Nuova regola per creare condizioni personalizzate. Ad esempio, puoi applicare una formattazione basata su formule specifiche.\n\n\nDefinisci il criterio di formattazione: Selezionando una delle opzioni predefinite, potrai impostare le condizioni. Ad esempio, se scegli “Maggiore di”, inserisci il valore di riferimento nella casella, ad esempio 50, per formattare tutte le celle che contengono un valore superiore a 50.\nEsempio: Se scegli Evidenzia regole celle &gt; Maggiore di…, puoi inserire il numero 50 e poi scegliere uno stile di formattazione come il riempimento rosso chiaro con testo rosso scuro.\n\nScegli uno stile di formattazione:\n\nSeleziona lo stile che desideri applicare alle celle che soddisfano la condizione. Puoi scegliere tra colori di riempimento, colori del testo o bordi, oppure personalizzare lo stile cliccando su Formato personalizzato.\n\n\nApplica la regola:\n\nDopo aver configurato la regola, clicca su OK per applicare la formattazione condizionale alle celle selezionate.\n\n\nEcco un esempio di come usare la formattazione condizionale in Excel per un’analisi del sangue. Supponiamo che tu abbia un foglio con i risultati di vari esami del sangue per diversi pazienti, e vuoi evidenziare i risultati che sono al di fuori dei range normali per facilitare l’identificazione di valori critici.\n\n\nEsempio di analisi del sangue\n\n\n\n\n\n\n\n\n\nPaziente\nEmoglobina (g/dL)\nColesterolo (mg/dL)\nGlucosio (mg/dL)\n\n\n\n\nMario Rossi\n12.5\n180\n95\n\n\nAnna Verdi\n14.8\n240\n110\n\n\nLuca Blu\n11.2\n170\n160\n\n\nGiulia Gialli\n13.6\n220\n85\n\n\n\n\n\nObiettivo:\nEvidenziare i valori:\n\nEmoglobina al di fuori del range normale (tra 12 e 16 g/dL).\nColesterolo superiore a 200 mg/dL (indica colesterolo alto).\nGlucosio superiore a 100 mg/dL (indica glicemia alta).\n\n\n\nPassaggi per applicare la formattazione condizionale:\n\n1. Formattazione per l’emoglobina fuori dal range normale\nVogliamo evidenziare in rosso i valori di Emoglobina che sono al di sotto di 12 g/dL o al di sopra di 16 g/dL.\n\nSeleziona l’intervallo delle celle che contiene i valori dell’emoglobina (es. B2\n).\nVai su Home &gt; Formattazione condizionale &gt; Nuova regola.\nClicca su Utilizza una formula per determinare le celle da formattare.\nInserisci la seguente formula per evidenziare i valori inferiori a 12 o superiori a 16:\n\n=O(B2&lt;12, B2&gt;16)\n\nClicca su Formato… e scegli un riempimento rosso o un colore di testo rosso per evidenziare i valori fuori range.\nClicca su OK per applicare la regola.\n\n\n\n2. Formattazione per il colesterolo alto\nVogliamo evidenziare in giallo i valori di Colesterolo superiori a 200 mg/dL.\n\nSeleziona l’intervallo delle celle con i valori del colesterolo (es. C2).\nVai su Home &gt; Formattazione condizionale &gt; Evidenzia regole celle &gt; Maggiore di….\nNella finestra che si apre, inserisci 200 come valore.\nSeleziona uno stile di formattazione predefinito o clicca su Formato personalizzato e scegli un riempimento giallo.\nClicca su OK.\n\n\n\n3. Formattazione per il glucosio alto\nVogliamo evidenziare in arancione i valori di Glucosio superiori a 100 mg/dL.\n\nSeleziona l’intervallo delle celle con i valori del glucosio (es. D2).\nVai su Home &gt; Formattazione condizionale &gt; Evidenzia regole celle &gt; Maggiore di….\nInserisci 100 come valore.\nSeleziona uno stile di formattazione con riempimento arancione per i valori che superano il limite.\nClicca su OK.\n\n\n\n\nRisultato finale:\nDopo aver applicato queste regole, la tabella potrebbe apparire così:\n\n\n\nInterpretazione:\n\nI valori di Anna Verdi e Luca Blu sono evidenziati per il colesterolo e il glucosio elevati, rispettivamente in ciano e arancione.\nIl valore dell’emoglobina di Luca Blu è evidenziato in rosso poiché è inferiore al valore normale di 12 g/dL.\nIn questo modo, è facile individuare rapidamente i pazienti con valori critici, consentendo un’analisi più efficiente e una migliore gestione clinica."
  },
  {
    "objectID": "posts/021_confronto_xls_database/confronto_xls_database.html",
    "href": "posts/021_confronto_xls_database/confronto_xls_database.html",
    "title": "Confronto tra Excel e Database nella Gestione Clinica dei Dati: Errori E Sicurezza",
    "section": "",
    "text": "Confronto tra Excel e Database nella Gestione Clinica dei Dati: Errori e Sicurezza\n\nCaso Excel: Errore nel Valore di una Glicemia\nUn errore comune in Excel è l’inserimento di valori non corretti a causa di formati non standardizzati. Supponiamo di avere una tabella in cui si registrano i valori di glicemia dei pazienti. Il formato corretto richiede l’uso del punto come separatore decimale (ad esempio, 5.8 mmol/L), ma un utente inserisce erroneamente un valore con la virgola (5,8).\nEsempio di tabella:\n\n\n\nPaziente\nValore di glicemia (mmol/L)\n\n\n\n\nPaziente A\n5.6\n\n\nPaziente B\n6.2\n\n\nPaziente C\n5,8\n\n\nPaziente D\n7.1\n\n\n\nIn questo caso, Excel potrebbe trattare 5,8 come testo, non come numero, perché la virgola è interpretata come un separatore non numerico. Di conseguenza, il valore 5,8 verrebbe ignorato durante i calcoli. Per esempio, se si tenta di calcolare la media dei valori con la formula =MEDIA(B2:B5), Excel considererà solo i valori numerici (5.6, 6.2 e 7.1), restituendo una media non corretta.\n\nMedia calcolata da Excel (ignorando 5,8):\n(5.6+6.2+7.1)/3=6.3(5.6 + 6.2 + 7.1) / 3 = 6.3(5.6+6.2+7.1)/3=6.3\nMedia corretta (includendo 5.8):\n(5.6+6.2+5.8+7.1)/4=6.175(5.6 + 6.2 + 5.8 + 7.1) / 4 = 6.175(5.6+6.2+5.8+7.1)/4=6.175\n\nQuesto errore può facilmente passare inosservato, portando a conclusioni cliniche sbagliate.\n\n\nCaso Database: Prevenzione dell’Errore\nIn un database, l’inserimento di dati con formato errato viene automaticamente bloccato. Supponiamo che la colonna dei valori glicemici sia definita come DECIMAL o FLOAT, dove è richiesto l’uso del punto decimale. Se si tenta di inserire un valore con la virgola, il database restituirà un errore, impedendo l’inserimento errato.\nCREATE TABLE glicemia (\n      paziente VARCHAR(50), \n      valore_glicemia DECIMAL(4,1) );\n\nINSERT INTO glicemia (paziente, valore_glicemia) VALUES ('Paziente C', '5,8');\nQuesto comando genererebbe un errore simile a:\nERROR: invalid input syntax for type numeric: \"5,8\"\nIl database obbliga l’utente a correggere l’errore prima di continuare, prevenendo errori nei calcoli successivi.\n\n\nCaso Excel: Mancanza di Controllo sugli Accessi\nExcel non offre un sistema avanzato di controllo sugli accessi e tracciamento delle modifiche. Supponiamo che un file Excel venga utilizzato per gestire i dati clinici di pazienti, con il rischio che qualsiasi utente con accesso possa modificare i dati (intenzionalmente o accidentalmente) senza lasciare traccia.\nEsempio:\n\n\n\n\n\n\n\n\n\n\nID Paziente\nNome\nDiagnosi\nTerapia\nMedico\n\n\n\n\n001\nMario Rossi\nIpertensione\nACE Inibitori\nDott. Bianchi\n\n\n002\nAnna Verdi\nDiabete Tipo 2\nInsulina\nDott. Neri\n\n\n003\nLuca Blu\nInsufficienza cardiaca\nBetabloccanti\nDott. Verdi\n\n\n\nUn operatore sanitario potrebbe per errore modificare la terapia di un paziente, sostituendo “Insulina” con “Metformina” per Anna Verdi. In questo scenario, non ci sarebbe alcun modo di risalire a chi ha apportato la modifica, poiché Excel non tiene traccia delle modifiche, esponendo i dati clinici a gravi rischi.\n\n\nCaso Database: Robustezza nella Gestione della Sicurezza\nUn database offre robusti meccanismi di controllo degli accessi, con permessi specifici per ogni ruolo. Ad esempio, solo determinati utenti possono modificare i dati sensibili, mentre altri hanno accesso in sola lettura. Inoltre, ogni modifica viene tracciata automaticamente, registrando chi ha fatto la modifica e quando.\nEsempio di gestione degli accessi:\nGRANT SELECT ON cartella_clinica TO infermiera;\nGRANT UPDATE ON terapia TO medico;\nOgni modifica viene registrata, e se qualcuno cambia la terapia, è possibile risalire all’operatore e correggere l’errore.\nEsempio di log delle modifiche:\n\n\n\n\n\n\n\n\n\n\n\nID Paziente\nOperatore\nCampo Modificato\nValore Precedente\nValore Nuovo\nData Modifica\n\n\n\n\n002\ninfermiera1\nTerapia\nInsulina\nMetformina\n2024-09-24 10:30\n\n\n\n\n\nCaso Excel: Duplicazione dei Dati\nUn altro rischio comune in Excel è la duplicazione accidentale dei dati. Supponiamo che durante l’inserimento di nuovi pazienti, un utente copi e incolli una riga esistente invece di crearne una nuova, modificando solo alcune informazioni e lasciando altri campi inalterati.\nEsempio di tabella:\n\n\n\n\n\n\n\n\n\n\nID Paziente\nNome\nDiagnosi\nTerapia\nMedico\n\n\n\n\n001\nMario Rossi\nIpertensione\nACE Inibitori\nDott. Bianchi\n\n\n002\nAnna Verdi\nDiabete Tipo 2\nInsulina\nDott. Neri\n\n\n003\nMario Rossi\nInsufficienza Cardiaca\nBetabloccanti\nDott. Bianchi\n\n\n\nIn questo caso, abbiamo due pazienti con lo stesso nome, ma terapie e diagnosi diverse. Questa duplicazione potrebbe non essere immediatamente evidente e potrebbe causare confusione clinica o errori nella gestione del paziente. Excel non dispone di controlli per rilevare automaticamente questi errori.\n\n\nCaso Database: Eliminazione delle Duplicazioni\nUn database può evitare questo tipo di errore utilizzando vincoli di unicità. Se la colonna “ID Paziente” è definita come chiave primaria, nessun duplicato sarà consentito.\nSebbene Excel sia uno strumento potente e molto utilizzato per la gestione dei dati, può presentare alcune limitazioni, specialmente quando si tratta di gestire dati sensibili come quelli clinici. Un database strutturato offre un livello superiore di controllo e sicurezza, prevenendo errori comuni come quelli dovuti a formati non standardizzati o duplicazioni. La scelta dello strumento giusto dipende dalle esigenze specifiche e dalla complessità dei dati da gestire, con l’obiettivo di garantire sempre accuratezza e sicurezza.\nEsempio:\nCREATE TABLE cartella_clinica (\n    id_paziente INT PRIMARY KEY,\n    nome VARCHAR(100),\n    diagnosi VARCHAR(100),\n    terapia VARCHAR(100),\n    medico VARCHAR(100)\n);\nSe si tenta di inserire un nuovo paziente con un ID già esistente:\nINSERT INTO cartella_clinica (id_paziente, nome, diagnosi, terapia, medico)\nVALUES (003, 'Mario Rossi', 'Ipertensione', 'ACE Inibitori', 'Dott. Bianchi');\nIl database restituirebbe un errore simile a:\nERROR: duplicate key value violates unique constraint \"cartella_clinica_pkey\"\nQuesto impedisce l’inserimento accidentale di dati duplicati, preservando l’integrità dei dati clinici.\n\nL’utilizzo di Excel per la gestione di dati clinici comporta rischi significativi di errori dovuti a formati incoerenti, duplicazioni e mancanza di tracciabilità. Al contrario, un database offre strumenti avanzati per prevenire questi problemi, garantendo una maggiore sicurezza, precisione e affidabilità nella gestione dei dati clinici."
  },
  {
    "objectID": "posts/022_pitfall_curves/pitfall_curves.html",
    "href": "posts/022_pitfall_curves/pitfall_curves.html",
    "title": "The Pitfall of Arbitrary Curve Deconvolution: A Cautionary Tale for Scientists and Students",
    "section": "",
    "text": "As scientists and data analysts, we often encounter complex curves in our work. These could be spectroscopic data, chromatograms, or any other type of signal that appears as a single peak but might be composed of multiple underlying components. The process of breaking down such a curve into its constituent parts is known as curve deconvolution or peak fitting.\nIt’s a common scenario: a researcher or student successfully decomposes a curve into two or three components, and they’re thrilled with the result. It seems to fit well, and they’re ready to draw conclusions based on these components. But there’s a crucial point that’s often overlooked: the number of components used in the deconvolution is often arbitrary.\nIn this post, we’ll explore why this is problematic and demonstrate how we can fit a curve with a varying number of components using R. This exercise will highlight why we should be cautious about interpreting the results of curve deconvolution without careful consideration.\nLet’s start with some R code that generates a simple curve and then fits it with a varying number of Gaussian components:\n\n# Load necessary libraries\nlibrary(minpack.lm)\nlibrary(ggplot2)\n\n# Function to generate a Gaussian curve\ngaussian <- function(x, amp, cen, wid) {\n  amp * exp(-((x - cen)^2) / (2 * wid^2))\n}\n\n# Generate sample data (a curve with a single peak)\nset.seed(123)\nx <- seq(0, 10, length.out = 100)\ny <- gaussian(x, 1, 5, 1) + rnorm(100, sd = 0.05)\ndata <- data.frame(x = x, y = y)\n\n# Function to create a model with n Gaussians\ncreate_n_gaussian_model <- function(n) {\n  components <- paste0(\"gaussian(x, amp\", 1:n, \", cen\", 1:n, \", wid\", 1:n, \")\", collapse = \" + \")\n  as.formula(paste(\"y ~\", components))\n}\n\n# Function to fit n Gaussians\nfit_n_gaussian <- function(data, n) {\n  model <- create_n_gaussian_model(n)\n  start_list <- as.list(rep(c(0.5, 5, 1), n))\n  names(start_list) <- c(rbind(paste0(\"amp\", 1:n), paste0(\"cen\", 1:n), paste0(\"wid\", 1:n)))\n  \n  fit <- nlsLM(model, data = data, start = start_list)\n  return(fit)\n}\n\n# Fit with varying number of Gaussians\nfits <- lapply(1:5, function(n) fit_n_gaussian(data, n))\n\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n\n# Calculate AIC for each fit\naics <- sapply(fits, AIC)\n\n# Create predictions and components for each fit\nfor (i in 1:length(fits)) {\n  data[paste0(\"fit\", i)] <- predict(fits[[i]], newdata = data)\n  params <- coef(fits[[i]])\n  for (j in 1:i) {\n    data[paste0(\"comp\", j, \"_\", i)] <- gaussian(data$x, params[paste0(\"amp\", j)], \n                                                params[paste0(\"cen\", j)], \n                                                params[paste0(\"wid\", j)])\n  }\n}\n\n# Plot the results\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit1, color = \"1 Gaussian\")) +\n  geom_line(aes(y = fit2, color = \"2 Gaussians\")) +\n  geom_line(aes(y = fit3, color = \"3 Gaussians\")) +\n  geom_line(aes(y = fit4, color = \"4 Gaussians\")) +\n  geom_line(aes(y = fit5, color = \"5 Gaussians\")) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\", \"orange\")) +\n  labs(title = \"Curve Deconvolution with Varying Number of Gaussians\",\n       subtitle = paste(\"AIC values:\", paste(round(aics, 2), collapse = \", \")),\n       x = \"X\", y = \"Y\", color = \"Fit\") +\n  theme_minimal()\n\n\n\n# Plot individual components for the 3-Gaussian fit\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit3, color = \"Total Fit\")) +\n  geom_line(aes(y = comp1_3, color = \"Component 1\")) +\n  geom_line(aes(y = comp2_3, color = \"Component 2\")) +\n  geom_line(aes(y = comp3_3, color = \"Component 3\")) +\n  scale_color_manual(values = c(\"black\", \"red\", \"blue\", \"green\")) +\n  labs(title = \"Deconvolution into 3 Gaussians\",\n       x = \"X\", y = \"Y\", color = \"Component\") +\n  theme_minimal()\n\n\n\n\nThis code generates a single peak and then fits it with 1 to 5 Gaussian components. It then plots the results and calculates the Akaike Information Criterion (AIC) for each fit.\nWhat we see from this exercise is striking:\n\nAll fits, from 1 to 5 components, appear to fit the data reasonably well visually.\nThe AIC values generally decrease as we add more components, suggesting that more complex models fit the data better.\n\nHowever, here’s the crucial point: our original data was generated from a single Gaussian curve with added noise. Despite this, we can “successfully” decompose it into 2, 3, 4, or even 5 components, each of which might seem meaningful if we didn’t know the true origin of the data.\nThis demonstrates a fundamental issue in curve deconvolution: without additional information or constraints, we can often fit a curve with an arbitrary number of components, and purely statistical measures might even suggest that more components provide a better fit.\nSo, what should we do? Here are some guidelines:\n\nAlways consider the physical or chemical meaning behind the components. Does it make sense in your specific context to have 2, 3, or more components?\nUse multiple criteria for model selection, not just visual fit or a single statistical measure. AIC, BIC, cross-validation, and other techniques can provide different perspectives.\nBe cautious about over-interpretation. Just because you can fit a curve with multiple components doesn’t necessarily mean those components represent real, distinct physical or chemical entities.\nWhen possible, use additional experimental techniques to validate your deconvolution. For example, in spectroscopy, you might use different types of spectroscopy or chemical separation techniques to confirm the presence of multiple components.\nAlways report the uncertainty in your fits and be transparent about the assumptions made in your analysis.\n\nIn conclusion, while curve deconvolution can be a powerful tool, it’s crucial to approach it with caution and skepticism. The ability to fit a curve with multiple components doesn’t always reflect the underlying reality of the system you’re studying. As scientists and data analysts, it’s our responsibility to critically evaluate our methods and results, always keeping in mind the limitations and potential pitfalls of our analytical techniques."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html",
    "href": "posts/Packages Review 2022/index.html",
    "title": "R Packages 2022 list",
    "section": "",
    "text": "List of packages I’ve found useful in my workflow during 2022 (in no particular order)"
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#plot",
    "href": "posts/Packages Review 2022/index.html#plot",
    "title": "R Packages 2022 list",
    "section": "Plot",
    "text": "Plot\n\nggvoronoi: Voronoi Diagrams and Heatmaps with ‘ggplot2’\ntags: #ggplot #tidyverse #voronoi\n[cran package link] https://CRAN.R-project.org/package=ggvoronoi\n\ndescription from the author/vignette\n\n\nEasy creation and manipulation of Voronoi diagrams using ‘deldir’ with visualization in ‘ggplot2’. Convenient functions are provided to create nearest neighbor diagrams and heatmaps. Diagrams are computed with ‘deldir’ and processed to work with the ‘sp’ framework. Results are provided in a convenient spatial data structure and displayed with ‘ggplot2’. An outline can be provided by the user to specify the spatial domain of interest.\n\n\n\nggh4x: Hacks for ‘ggplot2’\ntags: #ggplot #tidyverse\n[cran package link] https://CRAN.R-project.org/package=ggh4x\n\ndescription from the author/vignette\n\n\nA ‘ggplot2’ extension that does a variety of little helpful things. The package extends ‘ggplot2’ facets through customisation, by setting individual scales per panel, resizing panels and providing nested facets. Also allows multiple colour and fill scales per plot. Also hosts a smaller collection of stats, geoms and axis guides.\n\n\n\ngdiff: Graphical Difference Testing\ntags: #plot #ggplot #data analysis #comparison\n[cran package link] https://CRAN.R-project.org/package=gdiff\ndescription from the author/vignette\n\n\nFunctions for performing graphical difference testing. Differences are generated between raster images. Comparisons can be performed between different package versions and between different R versions.\n\n\n\naplot: Decorate a ‘ggplot’ with Associated Information\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=aplot\ndescription from the author/vignette \nFor many times, we are not just aligning plots as what ‘cowplot’ and ‘patchwork’ did. Users would like to align associated information that requires axes to be exactly matched in subplots, e.g. hierarchical clustering with a heatmap. This package provides utilities to aligns associated subplots to a main plot at different sides (left, right, top and bottom) with axes exactly matched.\n\n\ngghighlight: Highlight Lines and Points in ‘ggplot2’\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=gghighlight\ndescription from the author/vignette\n\nMake it easier to explore data with highlights.\n\n\n\npdp: Partial Dependence Plots\ntags: #plot #data analysis #ggplot [cran package link] https://CRAN.R-project.org/package=pdp\ndescription from the author/vignette\n\nA general framework for constructing partial dependence (i.e., marginal effect) plots from various types machine learning models in R.\n\n\n\nvcd: Visualizing Categorical Data\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=testDriveR\ndescription from the author/vignette\n\nVisualization techniques, data sets, summary and inference procedures aimed particularly at categorical data. Special emphasis is given to highly extensible grid graphics. The package was package was originally inspired by the book “Visualizing Categorical Data” by Michael Friendly and is now the main support package for a new book, “Discrete Data Analysis with R” by Michael Friendly and David Meyer (2015).\n\n\n\ngTestsMulti: New Graph-Based Multi-Sample Tests\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=gTestsMulti\ndescription from the author/vignette\n\nNew multi-sample tests for testing whether multiple samples are from the same distribution. They work well particularly for high-dimensional data. Song, H. and Chen, H. (2022) <arXiv:2205.13787>.\n\n\n\nspiralize: Visualize Data on Spirals\ntags: #statistic #visualization #plot\n[cran package link] https://CRAN.R-project.org/package=spiralize\ndescription from the author/vignette\n\nIt visualizes data along an Archimedean spiral https://en.wikipedia.org/wiki/Archimedean_spiral, makes so-called spiral graph or spiral chart. It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns.\n\n\n\nvaluemap: Making Choropleth Map\ntags: #plot #valuemap\n[cran package link] https://CRAN.R-project.org/package=valuemap\ndescription from the author/vignette\n\nYou can easily visualize your ‘sf’ polygons or data.frame with h3 address. While ‘leaflet’ package is too raw for data analysis, this package can save data analysts’ efforts & time with pre-set visualize options.\n\n\n\ntessellation: Delaunay and Voronoï Tessellations\ntags: #tesselation #voronoi #delaunay\n[cran package link] https://cran.r-project.org/package=tessellation\ndescription from the author/vignette\n\nDelaunay and Voronoï tessellations, with emphasis on the two-dimensional and the three-dimensional cases (the package provides functions to plot the tessellations for these cases). Delaunay tessellations are computed in C with the help of the ‘Qhull’ library http://www.qhull.org/.\n\n\n\nggdist: Visualizations of Distributions and Uncertainty\ntags: #ggplot #Distributions #Uncertainty\n[cran package link] https://cran.r-project.org/package=ggdist\ndescription from the author/vignette\n\nProvides primitives for visualizing distributions using ‘ggplot2’ that are particularly tuned for visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as bootstrap distributions or Bayesian posterior samples) are easily visualized. Visualization primitives include but are not limited to: points with multiple uncertainty intervals, eye plots (Spiegelhalter D., 1999) https://ideas.repec.org/a/bla/jorssa/v162y1999i1p45-58.html, density plots, gradient plots, dot plots (Wilkinson L., 1999) doi:10.1080/00031305.1999.10474474, quantile dot plots (Kay M., Kola T., Hullman J., Munson S., 2016) doi:10.1145/2858036.2858558, complementary cumulative distribution function barplots (Fernandes M., Walls L., Munson S., Hullman J., Kay M., 2018) doi:10.1145/3173574.3173718, and fit curves with multiple uncertainty ribbons.\n\n\n\ngrafify: Easy Graphs for Data Visualisation and Linear Models for ANOVA\ntags: #multivariate #Inference #tests #statistics\n[cran package link] https://cran.r-project.org//package=energy\ndescription from the author/vignette\n\nE-statistics (energy) tests and statistics for multivariate and univariate inference, including distance correlation, one-sample, two-sample, and multi-sample tests for comparing multivariate distributions, are implemented. Measuring and testing multivariate independence based on distance correlation, partial distance correlation, multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy distance, testing for multivariate normality, distance components (disco) for non-parametric analysis of structured data, and other energy statistics/methods are implemented.\n\n\n\nDiagrammeR: Graph/Network Visualization\ntags: #graph #networks\n[cran package link] https://cran.r-project.org/package=DiagrammeR\ndescription from the author/vignette\n\nBuild graph/network structures using functions for stepwise addition and deletion of nodes and edges. Work with data available in tables for bulk addition of nodes, edges, and associated metadata. Use graph selections and traversals to apply changes to specific nodes or edges. A wide selection of graph algorithms allow for the analysis of graphs. Visualize the graphs and take advantage of any aesthetic properties assigned to nodes and edges.\n\n\n\nnetplot: Beautiful Graph Drawing\ntags: #plot #graph\n[cran package link] https://cran.r-project.org/package=netplot\ndescription from the author/vignette\n\nA graph visualization engine that puts an emphasis on aesthetics at the same time of providing default parameters that yield out-of-the-box-nice visualizations. The package is built on top of ‘The Grid Graphics Package’ and seamlessly work with ‘igraph’ and ‘network’ objects.\n\n\n\nvivid: variable importance and variable interaction displays\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=vivid\ndescription from the author/vignette\n\nVariable importance, interaction measures and partial dependence plots are important summaries in the interpretation of statistical and machine learning models. In our R package vivid (variable importance and variable interaction displays) we create new visualisation techniques for exploring these model summaries. We construct heatmap and graph-based displays showing variable importance and interaction jointly, which are carefully designed to highlight important aspects of the fit. We also construct a new matrix-type layout showing all single and bivariate partial dependence plots, and an alternative layout based on graph Eulerians focusing on key subsets. Our new visualisations are model-agnostic and are applicable to regression and classification supervised learning settings. They enhance interpretation even in situations where the number of variables is large and the interaction structure complex.\n\n\n\ngridpattern: ‘grid’ Pattern Grobs\ntags: #database #relational #data\n[cran package link] https://cran.r-project.org/package=gridpattern\ndescription from the author/vignette\n\nProvides ‘grid’ grobs that fill in a user-defined area with various patterns. Includes enhanced versions of the geometric and image-based patterns originally contained in the ‘ggpattern’ package as well as original ‘pch’, ‘polygon_tiling’, ‘regular_polygon’, ‘rose’, ‘text’, ‘wave’, and ‘weave’ patterns plus support for custom user-defined patterns.\n\n\n\nPairViz: Visualization using Graph Traversal\ntags: #graphs #visualization [cran package link] https://cran.r-project.org/package=PairViz\ndescription from the author/vignette\n\nImproving graphics by ameliorating order effects, using Eulerian tours and Hamiltonian decompositions of graphs. References for the methods presented here are C.B. Hurley and R.W. Oldford (2010) doi:10.1198/jcgs.2010.09136 and C.B. Hurley and R.W. Oldford (2011) doi:10.1007/s00180-011-0229-5.\n\n\n\nDiagrammeR: Graph/Network Visualization\ntags: #graph #networks\n[cran package link] https://cran.r-project.org/package=DiagrammeR\ndescription from the author/vignette\n\nBuild graph/network structures using functions for stepwise addition and deletion of nodes and edges. Work with data available in tables for bulk addition of nodes, edges, and associated metadata. Use graph selections and traversals to apply changes to specific nodes or edges. A wide selection of graph algorithms allow for the analysis of graphs. Visualize the graphs and take advantage of any aesthetic properties assigned to nodes and edges.\n\n\n\nggimage: Use Image in ‘ggplot2’\ntags: #ggplot [cran package link] https://cran.r-project.org/package=ggimage\ndescription from the author/vignette\n\nSupports image files and graphic objects to be visualized in ‘ggplot2’ graphic system.\n\n\n\nsuperb: Summary Plots with Adjusted Error Bars\ntags: #ggplot #summary #plots\n[cran package link] https://cran.r-project.org//package=superb\ndescription from the author/vignette\n\nComputes standard error and confidence interval of various descriptive statistics under various designs and sampling schemes. The main function, superbPlot(), can either return a plot or a dataframe with the statistic and its precision interval so that other plotting package can be used. See Cousineau and colleagues (2021) doi:10.1177/25152459211035109 or Cousineau (2017) doi:10.5709/acp-0214-z for a review as well as Cousineau (2005) doi:10.20982/tqmp.01.1.p042, Morey (2008) doi:10.20982/tqmp.04.2.p061, Baguley (2012) doi:10.3758/s13428-011-0123-7, Cousineau & Laurencelle (2016) doi:10.1037/met0000055, Cousineau & O’Brien (2014) doi:10.3758/s13428-013-0441-z, Calderini & Harding doi:10.20982/tqmp.15.1.p001 for specific references.\n\n\n\nkhroma: Colour Schemes for Scientific Data Visualization\ntags: #plot #colors\n[cran package link] https://cran.r-project.org/web/package=khroma\n\nColour schemes ready for each type of data (qualitative, diverging or sequential), with colours that are distinct for all people, including colour-blind readers. This package provides an implementation of Paul Tol (2018) and Fabio Crameri (2018) doi:10.5194/gmd-11-2541-2018 colour schemes for use with ‘graphics’ or ‘ggplot2’. It provides tools to simulate colour-blindness and to test how well the colours of any palette are identifiable. Several scientific thematic schemes (geologic timescale, land cover, FAO soils, etc.) are also implemented\n\n\n\nggside: Side Grammar Graphics\ntags: #plot #ggplot\n[cran package link] https://cran.r-project.org//package=ggside\ndescription from the author/vignette\n\nThe grammar of graphics as shown in ‘ggplot2’ has provided an expressive API for users to build plots. ‘ggside’ extends ‘ggplot2’ by allowing users to add graphical information about one of the main panel’s axis using a familiar ‘ggplot2’ style API with tidy data. This package is particularly useful for visualizing metadata on a discrete axis, or summary graphics on a continuous axis such as a boxplot or a density distribution.\n\n\n\nggquiver: Quiver Plots for ‘ggplot2’\ntags: #plot #ggplot\n[cran package link] https://cran.r-project.org/package=ggquiver\ndescription from the author/vignette\n\nAn extension of ‘ggplot2’ to provide quiver plots to visualise vector fields. This functionality is implemented using a geom to produce a new graphical layer, which allows aesthetic options. This layer can be overlaid on a map to improve visualisation of mapped data.\n\n\n\ntimevis: Create Interactive Timeline Visualizations in R\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/package=timevis\ndescription from the author/vignette\n\nCreate rich and fully interactive timeline visualizations. Timelines can be included in Shiny apps and R markdown documents, or viewed from the R console and ‘RStudio’ Viewer. ‘timevis’ includes an extensive API to manipulate a timeline after creation, and supports getting data out of the visualization into R. Based on the ‘vis.js’ Timeline module and the ‘htmlwidgets’ R package.\n\n\n\nmisc3d: Miscellaneous 3D Plots\ntags: #plot #misc\n[cran package link] https://cran.r-project.org//package=misc3d\ndescription from the author/vignette\n\nA collection of miscellaneous 3d plots, including isosurfaces.."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#math",
    "href": "posts/Packages Review 2022/index.html#math",
    "title": "R Packages 2022 list",
    "section": "Math",
    "text": "Math\n\nlmtest: Testing Linear Regression Models\ntags: #linear regression #testing\n[cran package link] https://cran.r-project.org/package=lmtest\ndescription from the author/vignette\n\nA collection of tests, data sets, and examples for diagnostic checking in linear regression models. Furthermore, some generic tools for inference in parametric models are provided.\n\n\n\noptimx: Expanded Replacement and Extension of the ‘optim’ Function\ntags: #optim\n[cran package link] https://cran.r-project.org/packages=optimx\ndescription from the author/vignette\n\nProvides a replacement and extension of the optim() function to call to several function minimization codes in R in a single statement. These methods handle smooth, possibly box constrained functions of several or many parameters. Note that function ‘optimr()’ was prepared to simplify the incorporation of minimization codes going forward. Also implements some utility codes and some extra solvers, including safeguarded Newton methods. Many methods previously separate are now included here."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#statistics",
    "href": "posts/Packages Review 2022/index.html#statistics",
    "title": "R Packages 2022 list",
    "section": "Statistics",
    "text": "Statistics\n\ncorrr: Correlations in R\ntags: #statistics #data #correlation #calculus\n[cran package link] https://CRAN.R-project.org/package=corrr%5D\ndescription from the author/vignette\n\nA ‘ggplot2’ extension that does a variety of little helpful things. The package extends ‘ggplot2’ facets through customisation, by setting individual scales per panel, resizing panels and providing nested facets. Also allows multiple colour and fill scales per plot. Also hosts a smaller collection of stats, geoms and axis guides.\n\n\n\nFactoMineR: Multivariate Exploratory Data Analysis and Data Mining\ntags: #statistics #pca #clustering #multivariate #data\n[cran package link]https://CRAN.R-project.org/package=FactoMineR\ndescription from the author/vignette\n\nExploratory data analysis methods to summarize, visualize and describe datasets. The main principal component methods are available, those with the largest potential in terms of applications: principal component analysis (PCA) when variables are quantitative, correspondence analysis (CA) and multiple correspondence analysis (MCA) when variables are categorical, Multiple Factor Analysis when variables are structured in groups, etc. and hierarchical cluster analysis. F. Husson, S. Le and J. Pages (2017).\n\n\n\nVIM: Visualization and Imputation of Missing Values\ntags: #data #missing values\n[cran package link] https://CRAN.R-project.org/package=VIM\ndescription from the author/vignette\n\nNew tools for the visualization of missing and/or imputed values are introduced, which can be used for exploring the data and the structure of the missing and/or imputed values. Depending on this structure of the missing values, the corresponding methods may help to identify the mechanism generating the missing values and allows to explore the data including missing values. In addition, the quality of imputation can be visually explored using various univariate, bivariate, multiple and multivariate plot methods. A graphical user interface available in the separate package VIMGUI allows an easy handling of the implemented plot methods.\n\n\n\ncfda: Categorical Functional Data Analysis\ntags: #data #categorical\n[cran package link] https://CRAN.R-project.org/package=cfda\ndescription from the author/vignette\n\nPackage for the analysis of categorical functional data. The main purpose is to compute an encoding (real functional variable) for each state doi:10.3390/math9233074. It also provides functions to perform basic statistical analysis on categorical functional data.\n\n\n\nSHT: Statistical Hypothesis Testing Toolbox\ntags: #statistics #data analysis #comparison\n[cran package link] https://CRAN.R-project.org/package=SHT\ndescription from the author/vignette\n\nWe provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) doi:10.1007/0-387-27605-X.\n\n\n\ncontingencytables: Statistical Analysis of Contingency Tables\ntags: #plot #data analysis #ggplot\n[cran package link] <https://contingencytables.com/\ndescription from the author/vignette\n\nProvides functions to perform statistical inference of data organized in contingency tables. This package is a companion to the “Statistical Analysis of Contingency Tables” book by Fagerland et al. <ISBN 9781466588172>.\n\n\n\nMorphoTools2: Multivariate Morphometric Analysis\ntags: #statistics #multivatiate [cran package link] https://CRAN.R-project.org/package=MorphoTools2\ndescription from the author/vignette\n\nTools for multivariate analyses of morphological data, wrapped in one package, to make the workflow convenient and fast. Statistical and graphical tools provide a comprehensive framework for checking and manipulating input data, statistical analyses, and visualization of results. Several methods are provided for the analysis of raw data, to make the dataset ready for downstream analyses. Integrated statistical methods include hierarchical classification, principal component analysis, principal coordinates analysis, non-metric multidimensional scaling, and multiple discriminant analyses: canonical, stepwise, and classificatory (linear, quadratic, and the non-parametric k nearest neighbours). The philosophy of the package will be described in Šlenker et al. (in prep).\n\n\n\nautostats: Auto Stats\ntags: #statistic #reports #exploration\n[cran package link]https://CRAN.R-project.org/package=autostats\ndescription from the author/vignette\n\nAutomatically do statistical exploration. Create formulas using ‘tidyselect’ syntax, and then determine cross-validated model accuracy and variable contributions using ‘glm’ and ‘xgboost’. Contains additional helper functions to create and modify formulas. Has a flagship function to quickly determine relationships between categorical and continuous variables in the data set.\n\n\n\nflexclust: Flexible Cluster Algorithms\ntags: #classificatrion #clusters #multivariate\n[cran package link] https://cran.r-project.org/package=flexclust\ndescription from the author/vignette\n\nThe main function kcca implements a general framework for k-centroids cluster analysis supporting arbitrary distance measures and centroid computation. Further cluster methods include hard competitive learning, neural gas, and QT clustering. There are numerous visualization methods for cluster results (neighborhood graphs, convex cluster hulls, barcharts of centroids, …), and bootstrap methods for the analysis of cluster stability.\n\n\n\nffmanova: Fifty-Fifty MANOVA\ntags: #MANOVA #MANCONVA\n[cran package link] https://cran.r-project.org/package=ffmanova\ndescription from the author/vignette\n\nGeneral linear modeling with multiple responses (MANCOVA). An overall p-value for each model term is calculated by the 50-50 MANOVA method by Langsrud (2002) doi:10.1111/1467-9884.00320, which handles collinear responses. Rotation testing, described by Langsrud (2005) doi:10.1007/s11222-005-4789-5, is used to compute adjusted single response p-values according to familywise error rates and false discovery rates (FDR). The approach to FDR is described in the appendix of Moen et al. (2005) doi:10.1128/AEM.71.4.2086-2094.2005. Unbalanced designs are handled by Type II sums of squares as argued in Langsrud (2003) doi:10.1023/A:1023260610025. Furthermore, the Type II philosophy is extended to continuous design variables as described in Langsrud et al. (2007) doi:10.1080/02664760701594246. This means that the method is invariant to scale changes and that common pitfalls are avoided.\n\n\n\ncompareGroups 4.0: Descriptives by groups\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=compareGroups\ndescription from the author/vignette\n\ncompareGroups is an R package available on CRAN which performs descriptive tables displaying means, standard deviation, quantiles or frequencies of several variables. Also, p-value to test equality between groups is computed using the appropiate test. With a very simple code, nice, compact and ready-to-publish descriptives table are displayed on R console. They can also be exported to different formats, such as Word, Excel, PDF or inserted in a R-Sweave or R-markdown d\n\n\n\nDescTools: Tools for Descriptive Statistics\ntags: #statistics\n[cran package link] https://cran.r-project.org//package=DescTools\ndescription from the author/vignette\n\nA collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author’s intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The ‘BigCamelCase’ style was consequently applied to functions borrowed from contributed R packages as well.\n\n\n\noutForest: Multivariate Outlier Detection and Replacement\ntags: #random forest #outliers\n[cran package link] https://cran.r-project.org/package=outForest\ndescription from the author/vignette\n\nProvides a random forest based implementation of the method described in Chapter 7.1.2 (Regression model based anomaly detection) of Chandola et al. (2009) doi:10.1145/1541880.1541882. It works as follows: Each numeric variable is regressed onto all other variables by a random forest. If the scaled absolute difference between observed value and out-of-bag prediction of the corresponding random forest is suspiciously large, then a value is considered an outlier. The package offers different options to replace such outliers, e.g. by realistic values found via predictive mean matching. Once the method is trained on a reference data, it can be applied to new data.\n\n\n\nmultid: Multivariate Difference Between Two Groups\ntags: #multivariate #test\n[cran package link] https://cran.r-project.org/package=multid\ndescription from the author/vignette\n\nEstimation of multivariate differences between two groups (e.g., multivariate sex differences) with regularized regression methods and predictive approach. See Lönnqvist & Ilmarinen (2021) doi:10.1007/s11109-021-09681-2 and Ilmarinen et al. (2021) doi:10.31234/osf.io/j59bs.\n\n\n\nsimpr: Flexible ‘Tidyverse’-Friendly Simulations\ntags: #simulation #tidyverse\n[cran package link] https://cran.r-project.org/package=simpr\ndescription from the author/vignette\n\nA general, ‘tidyverse’-friendly framework for simulation studies, design analysis, and power analysis. Specify data generation, define varying parameters, generate data, fit models, and tidy model results in a single pipeline, without needing loops or custom functions.\n\n\n\nggfortify: Data Visualization Tools for Statistical Analysis Results\ntags: #statistics #datavis\n[cran package link] https://cran.r-project.org/web/packages/ggfortify/index.html\ndescription from the author/vignette\n\nUnified plotting tools for statistics commonly used, such as GLM, time series, PCA families, clustering and survival analysis. The package offers a single plotting interface for these analysis results and plots in a unified style using ‘ggplot2’.\n\n\n\nMVTests: Multivariate Hypothesis Tests\ntags: #statistics #test #multivariate\n[cran package link] https://cran.r-project.org/package=MVTests\ndescription from the author/vignette\n\nMultivariate hypothesis tests and the confidence intervals. It can be used to test the hypothesizes about mean vector or vectors (one-sample, two independent samples, paired samples), covariance matrix (one or more matrices), and the correlation matrix. Moreover, it can be used for robust Hotelling T^2 test at one sample case in high dimensional data. For this package, we have benefited from the studies Rencher (2003), Nel and Merwe (1986) doi:10.1080/03610928608829342, Tatlidil (1996), Tsagris (2014), Villasenor Alva and Estrada (2009) doi:10.1080/03610920802474465.\n\n\n\nplsVarSel: Variable Selection in Partial Least Squares\ntags: #pls #chemometrics\n[cran package link] https://cran.r-project.org/packages=plsVarSel\ndescription from the author/vignette\n\nInterfaces and methods for variable selection in Partial Least Squares. The methods include filter methods, wrapper methods and embedded methods. Both regression and classification is supported.\n\n\n\nRcmdrPlugin.EZR: R Commander Plug-in for the EZR (Easy R) Package\ntags: #statistics #ROC\n[cran package link] https://cran.r-project.org/package=RcmdrPlugin.EZR/index.html\ndescription from the author/vignette\n\nEZR (Easy R) adds a variety of statistical functions, including survival analyses, ROC analyses, metaanalyses, sample size calculation, and so on, to the R commander. EZR enables point-and-click easy access to statistical functions, especially for medical statistics. EZR is platform-independent and runs on Windows, Mac OS X, and UNIX. Its complete manual is available only in Japanese (Chugai Igakusha, ISBN: 978-4-498-10918-6, Nankodo, ISBN: 978-4-524-26158-1, Ohmsha, ISBN: 978-4-274-22632-8), but an report that introduced the investigation of EZR was published in Bone Marrow Transplantation (Nature Publishing Group) as an Open article. This report can be used as a simple manual. It can be freely downloaded from the journal website as shown below. This report has been cited in more than 3,000 scientific articles.\n\n\n\nRcmdrPlugin.NMBU: R Commander Plug-in for University Level Applied Statistics\ntags: #PLS #LDA #QDA\n[cran package link] https://cran.r-project.org/package=RcmdrPlugin.NMBU\ndescription from the author/vignette\n\nAn R Commander “plug-in” extending functionality of linear models and providing an interface to Partial Least Squares Regression and Linear and Quadratic Discriminant analysis. Several statistical summaries are extended, predictions are offered for additional types of analyses, and extra plots, tests and mixed models are available.\n\n\n\nDataEditR: An Interactive Editor for Viewing, Entering, Filtering & Editing Data\ntags: #tables #editor\n[cran package link] https://cran.r-project.org/package=DataEditR\ndescription from the author/vignette\n\nAn interactive editor built on ‘rhandsontable’ to allow the interactive viewing, entering, filtering and editing of data in R https://dillonhammill.github.io/DataEditR/.\n\n\n\nfICA: Classical, Reloaded and Adaptive FastICA Algorithms\ntags: #ggplot #summary #plots\n[cran package link] https://cran.r-project.org/package=fICA\ndescription from the author/vignette\n\nAlgorithms for classical symmetric and deflation-based FastICA, reloaded deflation-based FastICA algorithm and an algorithm for adaptive deflation-based FastICA using multiple nonlinearities. For details, see Miettinen et al. (2014) doi:10.1109/TSP.2014.2356442 and Miettinen et al. (2017) doi:10.1016/j.sigpro.2016.08.028. The package is described in Miettinen, Nordhausen and Taskinen (2018) doi:10.32614/RJ-2018-046.\n\n\n\nJFE: Tools and GUI for Analyzing Time Series Data of Just Finance and Econometrics\ntags: #econometrics #finance\n[cran package link] https://cran.r-project.org/web/packages/JFE/index.html\ndescription from the author/vignette\n\nSupport the analysis of financial and econometric time series, including recursive forecasts for machine learning.\n\n\n\nanscombiser: Create Datasets with Identical Summary Statistics\ntags: #statistics #anscombe\n[cran package link] https://cran.r-project.org/package=anscombiser\ndescription from the author/vignette\n\nThe anscombiser package takes a simpler and quicker approach to the same problem, using Anscombe’s statistics. It uses shifting, scaling and rotating to transform the observations in an input dataset to achieve a target set of Anscombe’s statistics”\n\n\n\nrandtoolbox: Toolbox for Pseudo and Quasi Random Number Generation and Random Generator Tests\ntags: #distributions\n[cran package link] https://cran.r-project.org/package=randtoolbox\ndescription from the author/vignette\n\nProvides (1) pseudo random generators - general linear congruential generators, multiple recursive generators and generalized feedback shift register (SF-Mersenne Twister algorithm and WELL generators); (2) quasi random generators - the Torus algorithm, the Sobol sequence, the Halton sequence (including the Van der Corput sequence) and (3) some generator tests - the gap test, the serial test, the poker test. See e.g. Gentle (2003) doi:10.1007/b97336. The package can be provided without the rngWELL dependency on demand. Take a look at the Distribution task view of types and tests of random number generators. Version in Memoriam of Diethelm and Barbara Wu"
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#medicine",
    "href": "posts/Packages Review 2022/index.html#medicine",
    "title": "R Packages 2022 list",
    "section": "Medicine",
    "text": "Medicine\n\nvisR: Clinical Graphs and Tables Adhering to Graphical Principles\ntags: #data #clinical\n[cran package link] https://CRAN.R-project.org/package=cfda\ndescription from the author/vignette\n\nTo enable fit-for-purpose, reusable clinical and medical research focused visualizations and tables with sensible defaults and based on graphical principles as described in: “Vandemeulebroecke et al. (2018)” doi:10.1002/pst.1912, “Vandemeulebroecke et al. (2019)” doi:10.1002/psp4.12455, and “Morris et al. (2019)” doi:10.1136/bmjopen-2019-030215.\n\n\n\nvbp: Blood Pressure Analysis in R\ntags: #statistics #clinical data\n[cran package link] https://cran.r-project.org/package=bp\ndescription from the author/vignette\n\nCardiovascular disease (CVD) is the leading cause of death worldwide with Hypertension, specifically, affecting over 1.1 billion people annually. The goal of the package is to provide a comprehensive toolbox for analyzing blood pressure data using a variety of statistical metrics and visualizations to bring more clarity to CVD."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#teaching",
    "href": "posts/Packages Review 2022/index.html#teaching",
    "title": "R Packages 2022 list",
    "section": "Teaching",
    "text": "Teaching\n\ntestDriveR: Teaching Data for Statistics and Data Science\ntags: #plot #data analysis #ggplot\n[cran package link] https://CRAN.R-project.org/package=testDriveR\ndescription from the author/vignette\n\nProvides data sets for teaching statistics and data science courses. It includes a sample of data from John Edmund Kerrich’s famous coinflip experiment. These are data that I used for teaching SOC 4015 / SOC 5050 at Saint Louis University (SLU). The package also contains an R Markdown template with the required formatting for assignments in my courses SOC 4015, SOC 4650, SOC 5050, and SOC 5650 at SLU."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#chemistry",
    "href": "posts/Packages Review 2022/index.html#chemistry",
    "title": "R Packages 2022 list",
    "section": "Chemistry",
    "text": "Chemistry\n\nstoichcalc: R Functions for Solving Stoichiometric Equations\ntags: #chemistry #stoichiometry [cran package link] https://CRAN.R-project.org/package=stoichcalc\ndescription from the author/vignette\n\nGiven a list of substance compositions, a list of substances involved in a process, and a list of constraints in addition to mass conservation of elementary constituents, the package contains functions to build the substance composition matrix, to analyze the uniqueness of process stoichiometry, and to calculate stoichiometric coefficients if process stoichiometry is unique. (See Reichert, P. and Schuwirth, N., A generic framework for deriving process stoichiometry in enviromental models, Environmental Modelling and Software 25, 1241-1251, 2010 for more details.)\n\n\n\ninters: Flexible Tools for Estimating Interactions\ntags: #statistics #interactions [cran package link] https://CRAN.R-project.org/package=inters\ndescription from the author/vignette\n\nA set of functions to estimate interactions flexibly in the face of possibly many controls. Implements the procedures described in Blackwell and Olson (2022) doi:10.1093/restud/rdt044.\n\n\n\nwaves: Vis-NIR Spectral Analysis Wrapper\ntags: #spectroscopy #preprocessing #filtering #model training\n[cran package link] https://cran.r-project.org/package=waves\ndescription from the author/vignette\n\nOriginally designed application in the context of resource-limited plant research and breeding programs, ‘waves’ provides an open-source solution to spectral data processing and model development by bringing useful packages together into a streamlined pipeline. This package is wrapper for functions related to the analysis of point visible and near-infrared reflectance measurements. It includes visualization, filtering, aggregation, preprocessing, cross-validation set formation, model training, and prediction functions to enable open-source association of spectral and reference data. This package is documented in a peer-reviewed manuscript in the Plant Phenome Journal doi:10.1002/ppj2.20012. Specialized cross-validation schemes are described in detail in Jarquín et al. (2017) doi:10.3835/plantgenome2016.12.0130. Example data is from Ikeogu et al. (2017) doi:10.1371/journal.pone.0188918.\n\n\n\nMSclassifR: Automated Classification of Mass Spectra\ntags: #Classification #Mass-Spectra\n[cran package link] https://cran.r-project.org//package=MSclassifR\ndescription from the author/vignette\n\nFunctions to classify mass spectra in known categories, and to determine discriminant mass-over-charge values. It includes easy-to-use functions for pre-processing mass spectra, functions to determine discriminant mass-over-charge values (m/z) from a library of mass spectra corresponding to different categories, and functions to predict the category (species, phenotypes, etc.) associated to a mass spectrum from a list of selected mass-over-charge values. Two vignettes illustrating how to use the functions of this package from real data sets are also available online to help users: https://agodmer.github.io/MSclassifR_examples/Vignettes/Vignettemsclassifr_Ecrobia.html and https://agodmer.github.io/MSclassifR_examples/Vignettes/Vignettemsclassifr_Klebsiella.html.\n\n\n\nNGLVieweR: load a PDB in R in order to view it\ntags: #chemistrys #visualization #molecular\n[cran package link] https://cran.r-project.org/packages=NGLVieweR\ndescription from the author/vignette\n\nProvides an ‘htmlwidgets’ https://www.htmlwidgets.org/ interface to ‘NGL.js’ http://nglviewer.org/ngl/api/. ‘NGLvieweR’ can be used to visualize and interact with protein databank (‘PDB’) and structural files in R and Shiny applications. It includes a set of API functions to manipulate the viewer after creation in Shiny."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#writing-reports-and-articles",
    "href": "posts/Packages Review 2022/index.html#writing-reports-and-articles",
    "title": "R Packages 2022 list",
    "section": "Writing reports and articles",
    "text": "Writing reports and articles\n\nutile.tools: Summarize Data for Publication\ntags: #statistic #reports #exploration\n[cran package link] [https://CRAN.R-project.org/package=utile.tools]\ndescription from the author/vignette\n\nA set of tools for preparing and summarizing data for publication purposes. Includes functions for tabulating models, means to produce human-readable summary statistics from raw data, macros for calculating duration of time, and simplistic hypothesis testing tools.\n\n\n\nrrtable: Reproducible Research with a Table of R Codes\ntags: #tables #reproducible research\n[cran package link] https://cran.r-project.org/package=rrtable\ndescription from the author/vignette\n\nMakes documents containing plots and tables from a table of R codes. Can make “HTML”, “pdf(‘LaTex’)”, “docx(‘MS Word’)” and “pptx(‘MS Powerpoint’)” documents with or without R code. In the package, modularized ‘shiny’ app codes are provided. These modules are intended for reuse across applications.\n\n\n\nreporter: Creates Statistical Reports\ntags: #statistics #report\n[cran package link] https://CRAN.R-project.org/package=reporter\ndescription from the author/vignette\n\nContains functions to create regulatory-style statistical reports. Originally designed to create tables, listings, and figures for the pharmaceutical, biotechnology, and medical device industries, these reports are generalized enough that they could be used in any industry. Generates text, rich-text, PDF, HTML, and Microsoft Word file formats. The package specializes in printing wide and long tables with automatic page wrapping and splitting. Reports can be produced with a minimum of function calls, and without relying on other table packages. The package supports titles, footnotes, page header, page footers, spanning headers, page by variables, and automatic page numbering.\n\n\n\nPDE: Extract Tables and Sentences from PDFs with User Interface\ntags: #pdf #scraping\n[cran package link] https://cran.r-project.org/packages=PDE\ndescription from the author/vignette\n\nPDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\n\nTplyr: A Grammar of Clinical Data Summary\ntags: #clinical #medical\n[cran package link] https://cran.r-project.org/package=Tplyr\ndescription from the author/vignette\n\nA tool created to simplify the data manipulation necessary to create clinical reports."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#coding",
    "href": "posts/Packages Review 2022/index.html#coding",
    "title": "R Packages 2022 list",
    "section": "Coding",
    "text": "Coding\n\nmockr: Mocking in R\ntags: #testing\n[cran package link] https://cran.r-project.org/package=mockr\ndescription from the author/vignette\n\nProvides a means to mock a package function, i.e., temporarily substitute it for testing. Designed as a drop-in replacement for the now deprecated ‘testthat::with_mock()’ and ‘testthat::local_mock()’.\n\n\n\ncOde: Automated C Code Generation for ‘deSolve’, ‘bvpSolve’\ntags: #C #Jacobians\n[cran package link] https://cran.r-project.org/package=cOde\ndescription from the author/vignette\n\nGenerates all necessary C functions allowing the user to work with the compiled-code interface of ode() and bvptwp(). The implementation supports “forcings” and “events”. Also provides functions to symbolically compute Jacobians, sensitivity equations and adjoint sensitivities being the basis for sensitivity analysis.\n\n\n\nmatlab2r: Translation Layer from MATLAB to R\ntags: #R #Matlab\n[cran package link] https://cran.r-project.org/package=matlab2r\ndescription from the author/vignette\n\nAllows users familiar with MATLAB to use MATLAB-named functions in R. Several basic MATLAB functions are written in this package to mimic the behavior of their original counterparts, with more to come as this package grows.\n\n\n\nlessR: Less Code, More Results\ntags: #coding\n[cran package link] https://cran.r-project.org//package=lessR\ndescription from the author/vignette\n\nEach function accomplishes the work of several or more standard R functions. For example, two function calls, Read() and CountAll(), read the data and generate summary statistics for all variables in the data frame, plus histograms and bar charts as appropriate. Other functions provide for descriptive statistics, a comprehensive regression analysis, analysis of variance and t-test, plotting including the introduced here Violin/Box/Scatter plot for a numerical variable, bar chart, histogram, box plot, density curves, calibrated power curve, reading multiple data formats with the same function call, variable labels, color themes, Trellis graphics and a built-in help system. Also includes a confirmatory factor analysis of multiple indicator measurement models, pedagogical routines for data simulation such as for the Central Limit Theorem, and generation and rendering of R markdown instructions for interpretative output.\n\n\n\nGroundhog: Addressing The Threat That R Poses To Reproducible Research\ntags: #reproducibility\n[cran package link] https://cran.r-project.org/package=groundhog\ndescription from the author/vignette\n\nMake R scripts that rely on packages reproducible, by ensuring that every time a given script is run, the same version of the used packages are loaded (instead of whichever version the user running the script happens to have installed). This is achieved by using the new command groundhog.library() instead of the base command library(), and including a date in the call. The date is used to call on the same version of the package every time (the most recent version available on CRAN at that date)."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#graphics",
    "href": "posts/Packages Review 2022/index.html#graphics",
    "title": "R Packages 2022 list",
    "section": "Graphics",
    "text": "Graphics\n\nraymolecule: Parse and Render Molecular Structures in 3D\ntags: #chemistry #rendering\n[cran package link] https://cran.r-project.org/package=raymolecule\ndescription from the author/vignette\n\nDownloads and parses ‘SDF’ (Structural Description Format) and ‘PDB’ (Protein Database) files for 3D rendering."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#regression",
    "href": "posts/Packages Review 2022/index.html#regression",
    "title": "R Packages 2022 list",
    "section": "Regression",
    "text": "Regression\n\nspeedglm: Fitting Linear and Generalized Linear Models to Large Data Sets\ntags: #fitting #GLM\n[cran package link] https://cran.r-project.org//package=speedglm\ndescription from the author/vignette\n\nFitting linear models and generalized linear models to large data sets by updating algorithms.\n\n\n\nmodelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready\ntags: #models\n[cran package link] https://cran.r-project.org/package=modelsummary\ndescription from the author/vignette\n\nCreate beautiful and customizable tables to summarize several statistical models side-by-side. Draw coefficient plots, multi-level cross-tabs, dataset summaries, balance tables (a.k.a. “Table 1s”), and correlation matrices. This package supports dozens of statistical models, and it can produce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint, Excel, RTF, JPG, or PNG. Tables can easily be embedded in ‘Rmarkdown’ or ‘knitr’ dynamic documents."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#signal-processing",
    "href": "posts/Packages Review 2022/index.html#signal-processing",
    "title": "R Packages 2022 list",
    "section": "Signal Processing",
    "text": "Signal Processing\n\ngsignal: Signal Processing in R\ntags: #signal processing #filters #boxcar\n[cran package link] https://cran.r-project.org/packages=gsignal\ndescription from the author/vignette\n\nR implementation of the ‘Octave’ package ‘signal’, containing a variety of signal processing tools, such as signal generation and measurement, correlation and convolution, filtering, filter design, filter analysis and conversion, power spectrum analysis, system identification, decimation and sample rate change, and windowing."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#data-analysis",
    "href": "posts/Packages Review 2022/index.html#data-analysis",
    "title": "R Packages 2022 list",
    "section": "Data analysis",
    "text": "Data analysis\n\nsplitTools: Tools for Data Splitting\ntags: #data splitting\n[cran package link] https://cran.r-project.org/package=splitTools\ndescription from the author/vignette\n\nFast, lightweight toolkit for data splitting. Data sets can be partitioned into disjoint groups (e.g. into training, validation, and test) or into (repeated) k-folds for subsequent cross-validation. Besides basic splits, the package supports stratified, grouped as well as blocked splitting. Furthermore, cross-validation folds for time series data can be created. See e.g. Hastie et al. (2001) doi:10.1007/978-0-387-84858-7 for the basic background on data partitioning and cross-validation.\n\n\n\noptedr: Calculating Optimal and D-Augmented Designs\ntags: #DoE #Chemometrics #optimal-design\n[cran package link] https://cran.r-project.org//package=optedr\ndescription from the author/vignette\n\nCalculates D-, Ds-, A- and I-optimal designs for non-linear models, via an implementation of the cocktail algorithm (Yu, 2011, doi:10.1007/s11222-010-9183-2). Compares designs via their efficiency, and D-augments any design with a controlled efficiency. An efficient rounding function has been provided to transform approximate designs to exact designs. mynotes\n\n\n\nReDaMoR: Relational Data Modeler\ntags: #database #relational #data\n[cran package link] https://cran.r-project.org/package=ReDaMoR\ndescription from the author/vignette\n\nThe aim of this package is to manipulate relational data models in R. It provides functions to create, modify and export data models in json format. It also allows importing models created with ‘MySQL Workbench’ (https://www.mysql.com/products/workbench/). These functions are accessible through a graphical user interface made with ‘shiny’. Constraints such as types, keys, uniqueness and mandatory fields are automatically checked and corrected when editing a model. Finally, real data can be confronted to a model to check their compatibility.\n\n\n\nexplore: Simplifies Exploratory Data Analysis\ntags: #graphs #visualization\n[cran package link] https://cran.r-project.org/package=gridpattern\ndescription from the author/vignette\n\nInteractive data exploration with one line of code or use an easy to remember set of tidy functions for exploratory data analysis. Introduces three main verbs. explore() to graphically explore a variable or table, describe() to describe a variable or table and report() to create an automated report.\n\n\n\nesquisse: Explore and Visualize Your Data Interactivelly\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/package=esquisse\ndescription from the author/vignette\n\nA ‘shiny’ gadget to create ‘ggplot2’ figures interactively with drag-and-drop to map your variables to different aesthetics. You can quickly visualize your data accordingly to their type, export in various formats, and retrieve the code to reproduce the plot.\n\n\n\nplfMA: A GUI to View, Design and Export Various Graphs of Data\ntags: #visualization #GUI\n[cran package link] http://cran.stat.unipd.it/package=plfMA\ndescription from the author/vignette\n\nProvides a graphical user interface for viewing and designing various types of graphs of the data. The graphs can be saved in different formats of an image.\n\n\n\ndatasets.load: Interfaces for Loading Datasets\ntags: #visualization #interactive\n[cran package link] https://cran.r-project.org/packages/datasets.load/index.html\ndescription from the author/vignette\n\nVisual interface for loading datasets in RStudio from all installed (including unloaded) packages, also includes command line interfaces.\n\n\n\nloon.shiny: Automatically Create a ‘Shiny’ App Based on Interactive ‘Loon’ Widgets\ntags: #data analysis\n[cran package link] https://cran.r-project.org/package=loon.shiny\ndescription from the author/vignette\n\nPackage ‘shiny’ provides interactive web applications in R. Package ‘loon’ is an interactive toolkit engaged in open-ended, creative and unscripted data exploration. The ‘loon.shiny’ package can take ‘loon’ widgets and display a selfsame ‘shiny’ app.\n\n\n\nloon: Interactive Statistical Data Visualization\ntags: #plot #data analysis\n[cran package link] https://cran.r-project.org//package=loon\ndescription from the author/vignette\n\nAn extendable toolkit for interactive data visualization and exploration.\n\n\n\nrio: A Swiss-Army Knife for Data I/O\ntags: #data input\n[cran package link] https://cran.r-project.org/package=rio\ndescription from the author/vignette\n\nStreamlined data import and export by making assumptions that the user is probably willing to make: ‘import()’ and ‘export()’ determine the data structure from the file extension, reasonable defaults are used for data import and export (e.g., ‘stringsAsFactors=FALSE’), web-based import is natively supported (including from SSL/HTTPS), compressed files can be read directly without explicit decompression, and fast import packages are used where appropriate. An additional convenience function, ‘convert()’, provides a simple method for converting between file types.\n\n\n\ntabxplor: User-Friendly Tables with Color Helpers for Data Exploration\ntags: #plots #tables\n[cran package link] https://cran.r-project.org/package=tabxplor\ndescription from the author/vignette\n\nMake it easy to deal with multiple cross-tables in data exploration, by creating them, manipulating them, and adding color helpers to highlight important informations. All functions are “tidy”, pipe-friendly, and render data frames which can be easily manipulated. Tables can be exported to Excel and in html with formats and colors.\n\n\n\ngroupdata2: Creating Groups from Data\ntags: #tables #data\n[cran package link] https://cran.r-project.org//package=groupdata2\ndescription from the author/vignette\n\nhods for dividing data into groups. Create balanced partitions and cross-validation folds. Perform time series windowing and general grouping and splitting of data. Balance existing groups with up- and downsampling or collapse them to fewer groups.\n\n\n\nconjurer: A Parametric Method for Generating Synthetic Data\ntags: #plot #colors\n[cran package link] https://cran.r-project.org//package=conjurer\ndescription from the author/vignette\n\nBuilds synthetic data applicable across multiple domains. This package also provides flexibility to control data distribution to make it relevant to many industry examples\n\n\n\nowidR: A Package for Importing Data from Our World in Data\ntags: #data #statistics\n[cran package link] https://cran.r-project.org/package=owidR\ndescription from the author/vignette\n\nScrapes data from the Our World in Data website to offer easy to use functions for searching for datasets and downloading them into R.\n\n\n\ntidycharts: Generate Tidy Charts Inspired by ‘IBCS’\ntags: #plots\n[cran package link] https://cran.r-project.org/package=tidycharts\ndescription from the author/vignette\n\nThere is a wide range of R packages created for data visualization, but still, there was no simple and easily accessible way to create clean and transparent charts - up to now. The ‘tidycharts’ package enables the user to generate charts compliant with International Business Communication Standards (‘IBCS’). It means unified bar widths, colors, chart sizes, etc. Creating homogeneous reports has never been that easy! Additionally, users can apply semantic notation to indicate different data scenarios (plan, budget, forecast). What’s more, it is possible to customize the charts by creating a personal color pallet with the possibility of switching to default options after the experiments. We wanted the package to be helpful in writing reports, so we also made joining charts in a one, clear image possible. All charts are generated in SVG format and can be shown in the ‘RStudio’ viewer pane or exported to HTML output of ‘knitr’/‘markdown’."
  },
  {
    "objectID": "posts/Packages Review 2022/index.html#extra",
    "href": "posts/Packages Review 2022/index.html#extra",
    "title": "R Packages 2022 list",
    "section": "EXTRA",
    "text": "EXTRA\n\ntidydice: simulates rolling a dice and flipping a coin\ntags: #teaching #fun\n[cran package link] https://cran.r-project.org//package=tidydice\ndescription from the author/vignette\n\nThis package simulates rolling a dice and flipping a coin. Each experiment generates a tibble. Dice rolls and coin flips are simulated using sample(). The properties of the dice can be changed, like the number of sides. A coin flip is simulated using a two sided dice. Experiments can be combined with the pipe-operator.\n\n\n\ntiling:Polygon Tiling Examples\ntags: #arts #fun\n[cran package link] https://cran.rstudio.com/web/package=gridpattern\ndescription from the author/vignette\n\nSeveral uniform regular polygon tiling patterns can be achieved by use of grid.pattern_regular_polygon() plus occasionally grid.polygon() to set a background color. This vignette highlights several such tiling patterns plus a couple notable non-uniform tiling patterns.\n\n\n\nlingtypology: Linguistic Typology and Mapping\ntags: #linguistic mapping\n[cran package link] https://cran.r-project.org/package=lingtypology\ndescription from the author/vignette\n\nProvides R with the Glottolog database https://glottolog.org/ and some more abilities for purposes of linguistic mapping. The Glottolog database contains the catalogue of languages of the world. This package helps researchers to make a linguistic maps, using philosophy of the Cross-Linguistic Linked Data project https://clld.org/, which allows for while at the same time facilitating uniform access to the data across publications. A tutorial for this package is available on GitHub pages https://docs.ropensci.org/lingtypology/ and package vignette. Maps created by this package can be used both for the investigation and linguistic teaching. In addition, package provides an ability to download data from typological databases such as WALS, AUTOTYP and some others and to create your own database website."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html",
    "href": "posts/Packages Review 2024/index.html",
    "title": "R Packages 2024 list so far",
    "section": "",
    "text": "List of packages I’ve found useful in my workflow during 2024 (so far)"
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#plot",
    "href": "posts/Packages Review 2024/index.html#plot",
    "title": "R Packages 2024 list so far",
    "section": "Plot",
    "text": "Plot\n\nspiralize: Visualize Data on Spirals\ntags: #plot\n[cran package link] https://CRAN.R-project.org/package=spiralize\ndescription from the author/vignette\n\n\nIt visualizes data along an Archimedean spiral https://en.wikipedia.org/wiki/Archimedean_spiral, makes so-called spiral graph or spiral chart. It has two major advantages for visualization: 1. It is able to >visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns.\n\n\n\npanelView: Visualizing Panel Data\ntags: #plot\n[cran package link] https://CRAN.R-project.org/package=panelView\ndescription from the author/vignette\n\n\nVisualizes panel data. It has three main functionalities: (1) it plots the treatment status and missing values in a panel dataset; (2) it visualizes the temporal dynamics of a main variable of interest; (3) it depicts the bivariate relationships between a treatment variable and an outcome variable either by unit or in aggregate. For details, see doi:10.18637/jss.v107.i07."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#spectroscopy",
    "href": "posts/Packages Review 2024/index.html#spectroscopy",
    "title": "R Packages 2024 list so far",
    "section": "Spectroscopy",
    "text": "Spectroscopy\n\nOpenSpecy: Analyze, Process, Identify, and Share Raman and (FT)IR Spectra\ntags: #spectroscopy\n[cran package link] https://CRAN.R-project.org/package=OpenSpecy\ndescription from the author/vignette\n\n\nRaman and (FT)IR spectral analysis tool for plastic particles and other environmental samples (Cowger et al. 2021, doi:10.1021/acs.analchem.1c00123). With read_any(), Open Specy provides a single function for reading individual, batch, or map spectral data files like .asp, .csv, .jdx, .spc, .spa, .0, and .zip. process_spec() simplifies processing spectra, including smoothing, baseline correction, range restriction and flattening, intensity conversions, wavenumber alignment, and min-max normalization. Spectra can be identified in batch using an onboard reference library (Cowger et al. 2020, doi:10.1177/0003702820929064) using match_spec(). A Shiny app is available via run_app() or online at> https://openanalysis.org/openspecy/.\n\n\n\nplsVarSel: Variable Selection in Partial Least Squares\ntags: #pls #partial least squares #regression\n[cran package link] https://CRAN.R-project.org/package=plsVarSel\ndescription from the author/vignette\n\n\nInterfaces and methods for variable selection in Partial Least Squares. The methods include filter methods, wrapper methods and embedded methods. Both regression and classification is supported."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#statistics",
    "href": "posts/Packages Review 2024/index.html#statistics",
    "title": "R Packages 2024 list so far",
    "section": "Statistics",
    "text": "Statistics\n\nqreport: Statistical Reporting with ‘Quarto’\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=qreport\ndescription from the author/vignette\n\n\nProvides statistical components, tables, and graphs that are useful in ‘Quarto’ and ‘RMarkdown’ reports and that produce ‘Quarto’ elements for special formatting such as tabs and marginal notes and graphs. Some of the functions produce entire report sections with tabs, e.g., the missing data report created by missChk(). Functions for inserting variables and tables inside ‘graphviz’ and ‘mermaid’ diagrams are included, and so are special clinical trial graphics for adverse event reporting.\n\n\n\nsjPlot: Data Visualization for Statistics in Social Science\ntags: #statistics #social science [cran package link] https://CRAN.R-project.org/package=sjPlot\ndescription from the author/vignette\n\n\nCollection of plotting and table output functions for data visualization. Results of various statistical analyses (that are commonly used in social sciences) can be visualized using this package, including simple and cross tabulated frequencies, histograms, box plots, (generalized) linear models, mixed effects models, principal component analysis and correlation matrices, cluster analyses, scatter plots, stacked scales, effects plots of regression models (including interaction terms) and much more. This package supports labelled data.\n\n\n\nMVET: Multivariate Estimates and Tests\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=MVET\ndescription from the author/vignette\n\n\nMultivariate estimation and testing, currently a package for testing parametric data. To deal with parametric data, various multivariate normality tests and outlier detection are performed and visualized using the ‘ggplot2’ package. Homogeneity tests for covariance matrices are also possible, as well as the Hotelling’s T-square test and the multivariate analysis of variance test. We are exploring additional tests and visualization techniques, such as profile analysis and randomized complete block design, to be made available in the future and making them easily accessible to users.\n\n\n\npbox: Exploring Multivariate Spaces with Probability Boxes\ntags: #statistics\n[cran package link] https://CRAN.R-project.org/package=pbox\ndescription from the author/vignette\n\n\nAdvanced statistical library offering a method to encapsulate and query the probability space of a dataset effortlessly using Probability Boxes (p-boxes). Its distinctive feature lies in the ease with which users can navigate and analyze marginal, joint, and conditional probabilities while taking into account the underlying correlation structure inherent in the data using copula theory and models. A comprehensive explanation is available in the paper “pbox: Exploring Multivariate Spaces with Probability Boxes” to be published in the Journal of Statistical Software.\n\n\n\nequatiomatic: Transform Models into ‘LaTeX’ Equations\ntags: #statistics #latex #regression #models\n[cran package link] https://CRAN.R-project.org/package=equatiomatic\ndescription from the author/vignette\n\n\nThe goal of ‘equatiomatic’ is to reduce the pain associated with writing ‘LaTeX’ formulas from fitted models. The primary function of the package, extract_eq(), takes a fitted model object as its input and returns the corresponding ‘LaTeX’ code for the model.\n\n\n\nbulkreadr: The Ultimate Tool for Reading Data in Bulk\ntags: #bulk import\n[cran package link] https://CRAN.R-project.org/package=bulkreadr\ndescription from the author/vignette\n\n\nDesigned to simplify and streamline the process of reading and processing large volumes of data in R, this package offers a collection of functions tailored for bulk data operations. It enables users to efficiently read multiple sheets from Microsoft Excel and Google Sheets workbooks, as well as various CSV files from a directory. The data is returned as organized data frames, facilitating further analysis and manipulation. Ideal for handling extensive data sets or batch processing tasks, bulkreadr empowers users to manage data in bulk effortlessly, saving time and effort in data preparation workflows. Additionally, the package seamlessly works with labelled data from SPSS and Stata."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#simulated-data",
    "href": "posts/Packages Review 2024/index.html#simulated-data",
    "title": "R Packages 2024 list so far",
    "section": "Simulated data",
    "text": "Simulated data\n\nrsurv: Random Generation of Survival Data\ntags: #rsurv\n[cran package link] https://CRAN.R-project.org/package=rsurv\ndescription from the author/vignette\n\n\nRandom generation of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package ‘rsurv’ also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package ‘rsurv’ lies in the fact that linear predictors are specified via a formula-based approach, facilitating the inclusion of categorical variables and interaction terms. The functions implemented in the package ‘rsurv’ can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivariate survival data, and competing risks survival data. Details about the R package ‘rsurv’ can be found in Demarqui (2024) doi:10.48550/arXiv.2406.01750."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#reporting-and-formatting",
    "href": "posts/Packages Review 2024/index.html#reporting-and-formatting",
    "title": "R Packages 2024 list so far",
    "section": "Reporting and Formatting",
    "text": "Reporting and Formatting\n\nftExtra: Extensions for ‘Flextable’\ntags: #tables #flextables\n[cran package link] https://CRAN.R-project.org/package=ftExtra\ndescription from the author/vignette\n\n\nBuild display tables easily by extending the functionality of the ‘flextable’ package. Features include spanning header, grouping rows, parsing markdown and so on."
  },
  {
    "objectID": "posts/Packages Review 2024/index.html#fun",
    "href": "posts/Packages Review 2024/index.html#fun",
    "title": "R Packages 2024 list so far",
    "section": "Fun",
    "text": "Fun\n\nPlayerChart: Generate Pizza Chart: Player Stats 0-100\ntags: #statistics  [cran package link] https://CRAN.R-project.org/package=PlayerChart\ndescription from the author/vignette\n\n\nCreate an interactive pizza chart visualizing a specific player’s statistics across various attributes in a sports dataset. The chart is constructed based on input parameters: ‘data’, a dataframe containing player data for any sports; ‘player_stats_col’, a vector specifying the names of the columns from the dataframe that will be used to create slices in the pizza chart, with statistics ranging between 0 and 100; ‘name_col’, specifying the name of the column in the dataframe that contains the player names; and ‘player_name’, representing the specific player whose statistics will be visualized in the chart, serving as the chart title.\n\n\n\ngameR: Color Palettes Inspired by Video Games\ntags: #statistics  [cran package link] https://CRAN.R-project.org/package=gameR\ndescription from the author/vignette\n\n\nPalettes based on video games."
  },
  {
    "objectID": "posts/023_rss_quarto/index.html",
    "href": "posts/023_rss_quarto/index.html",
    "title": "Configuring Your RSS Feed in Quarto: A Crucial Naming Tip",
    "section": "",
    "text": "Creating an RSS feed for your Quarto blog can be tricky, but there’s one essential detail that can save you a lot of frustration: matching your file names correctly."
  },
  {
    "objectID": "posts/023_rss_quarto/index.html#the-key-to-success-matching-names",
    "href": "posts/023_rss_quarto/index.html#the-key-to-success-matching-names",
    "title": "Configuring Your RSS Feed in Quarto: A Crucial Naming Tip",
    "section": "The Key to Success: Matching Names",
    "text": "The Key to Success: Matching Names\nWhen configuring your RSS feed in the _quarto.yml file, ensure that the name you use for the RSS icon matches the name of your listing file. This might seem minor, but it’s crucial for your RSS feed to work properly.\n\nExample:\nIf your blog’s main listing file is named blog.qmd (instead of the default index.qmd), your _quarto.yml configuration should reflect this:\nwebsite:\n  navbar:\n    right:\n      - icon: rss\n        href: blog.xml  # Note: This matches the 'blog.qmd' file name\n\n\nCommon Mistake:\nA frequent error is using index.xml when your listing file is actually named blog.qmd. This mismatch can prevent your RSS feed from functioning correctly."
  },
  {
    "objectID": "posts/023_rss_quarto/index.html#why-this-matters",
    "href": "posts/023_rss_quarto/index.html#why-this-matters",
    "title": "Configuring Your RSS Feed in Quarto: A Crucial Naming Tip",
    "section": "Why This Matters",
    "text": "Why This Matters\nQuarto uses these file names to generate the correct RSS feed file. When the names don’t match, the RSS feed might not be created or updated as expected, leaving your subscribers without new content."
  },
  {
    "objectID": "posts/023_rss_quarto/index.html#quick-checklist",
    "href": "posts/023_rss_quarto/index.html#quick-checklist",
    "title": "Configuring Your RSS Feed in Quarto: A Crucial Naming Tip",
    "section": "Quick Checklist:",
    "text": "Quick Checklist:\n\nIdentify your main listing file name (e.g., blog.qmd, index.qmd)\nEnsure your _quarto.yml RSS configuration uses the same name (e.g., blog.xml, index.xml)\nDouble-check for any typos or discrepancies\n\nBy paying attention to this naming convention, you’ll ensure your Quarto RSS feed works smoothly, keeping your readers up-to-date with your latest posts.\nRemember, in the world of web development and blogging, sometimes the smallest details make the biggest difference!"
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#importance-of-eda",
    "href": "posts/010_Data_Visualization/index.html#importance-of-eda",
    "title": "Data Visualization",
    "section": "",
    "text": "EDA serves several purposes:\n\nUnderstanding Data: EDA helps us become familiar with the dataset, identify the available variables, and understand their nature (numeric, categorical, temporal, etc.).\nDetecting Patterns: EDA allows us to detect patterns, relationships, and potential outliers within the data. This is critical for making informed decisions during the analysis.\nData Cleaning: Through EDA, we can identify missing values, outliers, or data inconsistencies that require cleaning and preprocessing.\nFeature Engineering: EDA may suggest feature engineering opportunities, such as creating new variables or transforming existing ones to better represent the underlying data.\nHypothesis Generation: EDA often leads to the generation of hypotheses or research questions, guiding further investigation.\nCommunicating Insights: EDA produces visualizations and summaries that facilitate the communication of insights to stakeholders or team members.\n\nIn the following sections, we will delve into the practical aspects of EDA, starting with data simulation and visualization techniques."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#generating-simulated-data",
    "href": "posts/010_Data_Visualization/index.html#generating-simulated-data",
    "title": "Data Visualization",
    "section": "",
    "text": "Before diving into Exploratory Data Analysis (EDA) on real datasets, it’s helpful to begin with the generation of simulated data. This allows us to have full control over the data and create example scenarios to understand key EDA concepts. In this section, we will learn how to generate simulated datasets using R.\n\n\nTo start, let’s define some basic parameters that we’ll use to generate simulated data:\n\nx_min: The minimum value for the variable x.\nx_max: The maximum value for the variable x.\nx_step: The increment between successive x values.\ny_mean: The mean value for the dependent variable y.\ny_sd: The standard deviation for the dependent variable y.\ny_min: The minimum possible value for y.\ny_max: The maximum possible value for y.\n\nWe will use these parameters to generate sample data.\nNow, let’s proceed to generate sample data based on the defined parameters. In this example, we’ll create a simple dataset with the following variables:\n\nx: A sequence of values ranging from x_min to x_max with an increment of x_step.\nvar_random: A random variable with values uniformly distributed between y_min and y_max.\nvar_norm: A variable with values generated from a normal distribution with mean y_mean and standard deviation y_sd.\nvar_sin: A variable with values generated as the sine function of x.\n\nHere’s the R code to create the sample dataset:\n\nlibrary(data.table)\n\n# Parameters\nx_min   &lt;- 0\nx_max   &lt;- 10   \nx_step  &lt;- 0.01\n\ny_mean  &lt;- 0.5\ny_sd    &lt;- 0.25\ny_min   &lt;- -1\ny_max   &lt;- 1     \n\nx       &lt;- seq(x_min, x_max, x_step)\n\n# Variables\nvar_random  &lt;- runif(length(x), y_min, y_max)\nvar_norm    &lt;- rnorm(length(x), y_mean, y_sd) \nvar_sin     &lt;- sin(x)\n\n# Data.frame \ndf  &lt;- data.frame(x, var_random, var_norm, var_sin)\ndt  &lt;- data.table(df)\n\n# Melt \ndtm &lt;- melt(dt, id.vars=\"x\")\n\nThis code creates a dataset df and a data.table dt containing the generated variables. The melt function from the data.table library is used to reshape the data for visualization purposes.\nWith our simulated data ready, we can now move on to creating various plots and performing EDA.\nIn this section, we will explore various visualization techniques that play a crucial role in Exploratory Data Analysis (EDA). Visualizations help us gain insights into the data’s distribution, patterns, and relationships between variables. We will use the simulated dataset generated in the previous section to illustrate these techniques."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#choosing-the-right-plot",
    "href": "posts/010_Data_Visualization/index.html#choosing-the-right-plot",
    "title": "Data Visualization",
    "section": "",
    "text": "The choice of visualization depends on the nature of your data and the specific aspects you want to highlight. Generally, in EDA, we often need to:\n\nExamine Changes Over Time: Use time series plots when you want to assess changes in one or more variables over time.\nCheck for Data Distribution: Create distribution plots, such as histograms and density plots, to understand how data points are distributed.\nExplore Variable Relationships: Employ correlation plots and scatter plots to identify linear relationships between variables.\n\nLet’s start by examining these aspects one by one using our simulated dataset.\n\n\nTo explore changes over time, we’ll create a time series plot for the var_sin variable. This variable represents a sine wave and is well-suited for a time series representation. Here’s the R code to create a time series plot:\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\np &lt;- ggplot(dtm[variable == \"var_sin\"], aes(x = x, y = value, group = variable)) +\n     geom_line(aes(linetype = variable, color = variable))\np\n\n\n\n\n\n\n\n\nIn this code, we use ggplot2 to create a line plot for the var_sin variable.\n\n\n\nTo check the data distribution, we’ll create histogram plots for each of the variables: var_random, var_norm, and var_sin. Histograms provide a visual representation of the frequency distribution of data values. Here’s the R code:\n\np3 &lt;- ggplot(dtm[variable == \"var_sin\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np4 &lt;- ggplot(dtm[variable == \"var_norm\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20) \n\np5 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n      geom_histogram(bins = 20)\np3\n\n\n\n\n\n\n\np4\n\n\n\n\n\n\n\np5\n\n\n\n\n\n\n\n\nThese plots will help us understand the distribution characteristics of our variables.\n\n\n\nCorrelation plots allow us to examine relationships between variables. We’ll create scatter plots for pairs of variables to assess their linear correlation. Here’s an example for var_sin and var_sin2:\n\noptions(repr.plot.width = 7, repr.plot.height = 7)\n\nvar_random2  &lt;- runif(x,y_min,y_max)\nvar_norm2    &lt;- rnorm(x,y_mean,y_sd) \nvar_sin2     &lt;- sin(x) + rnorm(x,0,0.01) \n\ndf2&lt;- data.frame(df,var_sin2,var_norm2,var_random2)\ndt2 &lt;- data.table(df2)\n\np10 &lt;- ggplot(dt2) + geom_point(aes(x = var_sin, y = var_sin2)) \np10\n\n\n\n\n\n\n\n\nThese scatter plots help us identify whether variables exhibit linear correlation.\nIn the following sections, we’ll delve deeper into each of these plot types, interpret the results, and explore additional visualization techniques for EDA.\n\n\nBox plots, also known as box-and-whisker plots, provide a summary of the data’s distribution, including median, quartiles, and potential outliers. They are particularly useful for comparing the distributions of different variables or groups. Here’s an example of creating box plots for var_random, var_norm, and var_sin:\n\np6 &lt;- ggplot(dtm, aes(x = variable, y = value)) +\n      geom_boxplot()\np6\n\n\n\n\n\n\n\n\nBox plots can reveal variations and central tendencies of the variables.\n\n\n\nPair plots, or scatterplot matrices, allow us to visualize pairwise relationships between multiple variables in a dataset. They are helpful for identifying correlations and patterns among variables simultaneously. Here’s how to create a pair plot for our dataset:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\npair_plot &lt;- ggpairs(dt2, columns = c(\"var_random\", \"var_norm\", \"var_sin\", \"var_sin2\")) \n\npair_plot\n\nWarning in geom_point(): All aesthetics have length 1, but the data has 16 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nPair plots provide a comprehensive view of variable interactions.\n\n\n\nTime series data often contain underlying components such as trends and seasonality that can be crucial for understanding the data’s behavior. Time series decomposition is a technique used in Exploratory Data Analysis (EDA) to separate these components. In this section, we’ll demonstrate how to perform time series decomposition using our simulated var_sin data.\n\n# Install and load the forecast library if not already installed\nif (!requireNamespace(\"forecast\", quietly = TRUE)) {\n  install.packages(\"forecast\")\n}\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(forecast)\n\n# Decompose the time series\nsin_decomp &lt;- decompose(ts(dt2$var_sin, frequency = 365))\n\n# Plot the decomposed components\nplot(sin_decomp)\n\n\n\n\n\n\n\n\nThe code above performs the following:\n\nDecomposes the var_sin time series using the decompose function. We specify a frequency of 365 since the data represents daily observations.\nPlots the decomposed components, including the original time series, trend, seasonal component, and residual.\n\nThe resulting plot will show the individual components of the time series, allowing us to gain insights into its underlying patterns.\n\n\n\nInteractive plots, created using libraries like plotly or shiny, allow users to explore data interactively. You can create interactive scatter plots, line plots, or heatmaps, enhancing the user’s ability to dig deeper into the data.\n\n# Install and load the Plotly library if not already installed\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n\nCaricamento pacchetto: 'plotly'\n\n\nIl seguente oggetto è mascherato da 'package:ggplot2':\n\n    last_plot\n\n\nIl seguente oggetto è mascherato da 'package:stats':\n\n    filter\n\n\nIl seguente oggetto è mascherato da 'package:graphics':\n\n    layout\n\n# Create an interactive scatter plot\nscatter_plot &lt;- plot_ly(data = dt2, x = ~var_random, y = ~var_norm, text = ~paste(\"x:\", x, \"&lt;br&gt;var_random:\", var_random, \"&lt;br&gt;var_norm:\", var_norm),\n                        marker = list(size = 10, opacity = 0.7, color = var_sin)) %&gt;%\n  add_markers() %&gt;%\n  layout(title = \"Interactive Scatter Plot\",\n         xaxis = list(title = \"var_random\"),\n         yaxis = list(title = \"var_norm\"),\n         hovermode = \"closest\") \n\n# Display the interactive scatter plot\nscatter_plot\n\n\n\n\n\nIn this initial section, we’ve introduced the fundamental concepts of exploratory data analysis (EDA) and the importance of data visualization in gaining insights from complex datasets. We’ve explored various types of plots and their applications in EDA.\nNow, let’s dive deeper and enhance our understanding by demonstrating practical examples of EDA using real-world datasets. We’ll showcase how different types of plots and interactive visualizations can provide valuable insights and drive data-driven decisions.\nLet’s embark on this EDA journey and uncover the hidden stories within our data through hands-on examples."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#time-series-decomposition-for-insights",
    "href": "posts/010_Data_Visualization/index.html#time-series-decomposition-for-insights",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we’ll dive into a meaningful example of time series decomposition to demonstrate its practical utility in Exploratory Data Analysis (EDA). Time series decomposition allows us to extract valuable insights from time-dependent data. We’ll use our simulated var_sin time series to illustrate its significance.\n\n\n\nImagine we have daily temperature data for a city over several years. We want to understand the underlying patterns in temperature variations, including trends and seasonality, to make informed decisions related to weather forecasts, climate monitoring, or energy management.\nLet’s create the enhanced dataset with temperature data for multiple cities. We’ll use the data.table library to manage the dataset efficiently:\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"data.table\", quietly = TRUE)) {\n  install.packages(\"data.table\")\n}\n\nlibrary(data.table)\n\n# Set the seed for reproducibility\nset.seed(42)\n\n# Generate a dataset with temperature data for multiple cities\ncities &lt;- c(\"New York\", \"Los Angeles\", \"Chicago\", \"Miami\", \"Denver\")\nstart_date &lt;- as.Date(\"2010-01-01\")\nend_date &lt;- as.Date(\"2019-12-31\")\ndate_seq &lt;- seq(start_date, end_date, by = \"day\")\n\n# Create a data.table for the dataset\ntemperature_data &lt;- data.table(\n  Date = rep(date_seq, length(cities)),\n  City = rep(cities, each = length(date_seq)),\n  Temperature = rnorm(length(date_seq) * length(cities), mean = 60, sd = 20)\n)\n# Filter data for New York\nny_temperature &lt;- temperature_data[City == \"New York\"]\n\n# Decompose the daily temperature time series for New York\nny_decomp &lt;- decompose(ts(ny_temperature$Temperature, frequency = 365))\n\n# Plot the decomposed components for New York\nplot(ny_decomp)\n\n\n\n\n\n\n\n\nWe’ve generated temperature data for each city over the span of ten years, resulting in a diverse and complex dataset.\n\n\n\nNow that we have our multi-city temperature dataset, let’s apply time series decomposition to analyze temperature trends and seasonality for one of the cities, such as New York (see plot)\n\n\n\nThe plot will display the components of the time series for New York, including the original time series, trend, seasonal component, and residual. Similar analyses can be performed for other cities to identify regional temperature patterns.\n\n\n\nWith our enhanced multi-city temperature dataset and time series decomposition, we can:\n\nRegional Analysis: Compare temperature patterns across different cities to identify regional variations.\nSeasonal Insights: Understand how temperature seasonality differs between cities and regions.\nLong-Term Trends: Analyze temperature trends for each city over the ten-year period.\n\nThis advanced analysis helps us make informed decisions related to climate monitoring, urban planning, and resource management."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#leveraging-distribution-plots-for-in-depth-analysis",
    "href": "posts/010_Data_Visualization/index.html#leveraging-distribution-plots-for-in-depth-analysis",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we’ll illustrate the significance of distribution plots in Exploratory Data Analysis (EDA) by considering a practical scenario. Distribution plots help us understand how data points are distributed and can reveal insights about the underlying data characteristics. We’ll use our simulated dataset and focus on the var_random variable.\n\n\n\nImagine we have a dataset containing exam scores of students in a class. We want to gain insights into the distribution of exam scores to answer questions like:\n\nWhat is the typical exam score?\nAre the exam scores normally distributed?\nAre there any outliers or unusual patterns in the scores?\n\n\n\n\nLet’s create a histogram to visualize the distribution of exam scores using the var_random variable. This will help us answer the questions posed above.\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"ggplot2\", quietly = TRUE)) {\n  install.packages(\"ggplot2\")\n}\n\nlibrary(ggplot2)\n\n# Create a histogram to visualize the distribution of exam scores\np3 &lt;- ggplot(dtm[variable == \"var_random\"], aes(y = value, group = variable)) +\n     geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Distribution of Exam Scores\",\n          x = \"Scores\",\n          y = \"Frequency\")\n\n# Display the histogram\np3\n\n\n\n\n\n\n\n\n\n\n\nThe resulting histogram will display the distribution of exam scores. Here’s what we can interpret:\n\nTypical Exam Score: The histogram will show where the majority of exam scores lie, indicating the typical or central value.\nDistribution Shape: We can assess whether the scores follow a normal distribution, are skewed, or have other unique characteristics.\nOutliers: Outliers, if present, will appear as data points far from the central part of the distribution.\n\n\n\n\nBy analyzing the distribution of exam scores, we can:\n\nIdentify Central Tendency: Determine the typical exam score, which can be useful for setting benchmarks or evaluating student performance.\nUnderstand Data Characteristics: Gain insights into the shape of the distribution, which informs us about the data’s characteristics.\nDetect Outliers: Identify outliers or unusual scores that may require further investigation."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#correlation-analysis",
    "href": "posts/010_Data_Visualization/index.html#correlation-analysis",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we’ll explore advanced correlation analysis using more complex datasets. We’ll create two datasets: one representing students’ academic performance, and the other containing information about their study habits and extracurricular activities. We’ll investigate correlations between various factors to gain deeper insights.\n\n\nLet’s create the two complex datasets for our correlation analysis:\nAcademic Performance Dataset:\n\n# Create an academic performance dataset\nset.seed(123)\n\nnum_students &lt;- 500\n\nacademic_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Exam_Score = rnorm(num_students, mean = 75, sd = 10),\n  Assignment_Score = rnorm(num_students, mean = 85, sd = 5),\n  Final_Project_Score = rnorm(num_students, mean = 90, sd = 7)\n)\n\nStudy Habits and Activities Dataset:\n\n# Create a study habits and activities dataset\nset.seed(456)\n\nstudy_data &lt;- data.frame(\n  Student_ID = 1:num_students,\n  Study_Hours = rpois(num_students, lambda = 3) + 1,\n  Extracurricular_Hours = rpois(num_students, lambda = 2),\n  Stress_Level = rnorm(num_students, mean = 5, sd = 2)\n)\n\n\n\n\nNow that we have our complex datasets, let’s perform advanced correlation analysis to explore relationships between academic performance, study habits, and extracurricular activities. We’ll calculate correlations and visualize them using a heatmap:\n\n# Calculate correlations between variables\ncorrelation_matrix &lt;- cor(academic_data[, c(\"Exam_Score\", \"Assignment_Score\", \"Final_Project_Score\")], \n                          study_data[, c(\"Study_Hours\", \"Extracurricular_Hours\", \"Stress_Level\")])\n\n# Install and load the necessary libraries if not already installed\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) {\n  install.packages(\"corrplot\")\n}\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n# Create a heatmap to visualize correlations\ncorrplot(correlation_matrix, method = \"color\", type = \"lower\", tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nThe resulting heatmap visually represents the correlations between academic performance and study-related factors. Here’s what we can interpret:\n\nColor Intensity: The color intensity indicates the strength and direction of the correlation. Positive correlations are shown in blue, while negative correlations are in red. The darker the color, the stronger the correlation.\nCorrelation Coefficients: The heatmap displays the actual correlation coefficients as labels in the lower triangle. These values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n\n\n\n\nBy conducting advanced correlation analysis, we can:\n\nUnderstand Complex Relationships: Explore intricate correlations between academic performance, study hours, extracurricular activities, and stress levels.\nIdentify Key Factors: Determine which factors have the most significant impact on academic performance.\nOptimize Student Support: Use insights to provide targeted support and interventions for students.\n\nAdvanced correlation analysis helps us uncover nuanced relationships within complex datasets."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#exploring-diverse-distributions-with-box-plots",
    "href": "posts/010_Data_Visualization/index.html#exploring-diverse-distributions-with-box-plots",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, we’ll explore the versatility of box plots by working with diverse and complex datasets. We’ll create two datasets: one representing the distribution of monthly sales for multiple product categories, and the other containing information about customer demographics. These datasets will allow us to visualize various types of distributions and identify outliers.\n\n\nLet’s create the two complex datasets for our box plot analysis:\nSales Dataset and Customer Demographics Dataset:\n\n# Create a sales dataset\nset.seed(789)\n\nnum_months &lt;- 24\nproduct_categories &lt;- c(\"Electronics\", \"Clothing\", \"Home Decor\", \"Books\")\n\nsales_data &lt;- data.frame(\n  Month = rep(seq(1, num_months), each = length(product_categories)),\n  Product_Category = rep(product_categories, times = num_months),\n  Sales = rpois(length(product_categories) * num_months, lambda = 1000)\n)\n\n# Create a customer demographics dataset\nset.seed(101)\n\nnum_customers &lt;- 300\n\ndemographics_data &lt;- data.frame(\n  Customer_ID = 1:num_customers,\n  Age = rnorm(num_customers, mean = 30, sd = 5),\n  Income = rnorm(num_customers, mean = 50000, sd = 15000),\n  Education_Level = sample(c(\"High School\", \"Bachelor's\", \"Master's\", \"Ph.D.\"), \n                           size = num_customers, replace = TRUE)\n)\n\n# Create a box plot to visualize sales distributions by product category\np5 &lt;- ggplot(sales_data, aes(x = Product_Category, y = Sales, fill = Product_Category)) +\n     geom_boxplot() +\n     theme_minimal() +\n     labs(title = \"Sales Distribution by Product Category\",\n          x = \"Product Category\",\n          y = \"Sales\")\n\n# Display the box plot\np5\n\n\n\n\n\n\n\n# Create a box plot to visualize the distribution of customer ages\np6 &lt;- ggplot(demographics_data, aes(y = Age, x = \"Age\")) +\n     geom_boxplot(fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n     theme_minimal() +\n     labs(title = \"Customer Age Distribution (Box Plot)\",\n          x = \"\",\n          y = \"Age\")\n\n# Display the box plot\np6\n\n\n\n\n\n\n\n\n\n\n\nThese box plots help us gain insights into diverse distributions:\n\nSales Distribution: We can observe how sales are distributed across different product categories, identifying variations and potential outliers.\nCustomer Age Distribution: The box plot displays the spread of customer ages, highlighting the central tendency and any potential outliers.\n\n\n\n\nBy using box plots with complex datasets, we can:\n\nAnalyze Diverse Distributions: Visualize and compare distributions of sales for multiple product categories and customer age distributions.\nOutlier Detection: Identify potential outliers in both sales data and customer demographics.\nSegmentation Insights: Understand how sales vary across product categories and the age distribution of customers.\n\nBox plots are versatile tools for exploring various types of data distributions and making data-driven decisions."
  },
  {
    "objectID": "posts/010_Data_Visualization/index.html#interactive-data-visualization-with-plotly",
    "href": "posts/010_Data_Visualization/index.html#interactive-data-visualization-with-plotly",
    "title": "Data Visualization",
    "section": "",
    "text": "Suppose we have a dataset containing monthly stock prices for three companies: Company A, Company B, and Company C. We want to create an interactive time series plot that allows users to:\n\nSelect the company they want to visualize.\nZoom in and out to explore specific time periods.\nHover over data points to view detailed information.\n\n\nif (!requireNamespace(\"plotly\", quietly = TRUE)) {\n  install.packages(\"plotly\")\n}\n\nlibrary(plotly)\n\n# Create a sample time series dataset\nset.seed(789)\n\nnum_months &lt;- 24\n\ntime_series_data &lt;- data.frame(\n  Date = seq(as.Date(\"2022-01-01\"), by = \"months\", length.out = num_months),\n  Company_A = cumsum(rnorm(num_months, mean = 0.02, sd = 0.05)),\n  Company_B = cumsum(rnorm(num_months, mean = 0.03, sd = 0.04)),\n  Company_C = cumsum(rnorm(num_months, mean = 0.01, sd = 0.03))\n)\n\n# Create an interactive time series plot with Plotly\ninteractive_plot &lt;- plot_ly(data = time_series_data, x = ~Date) %&gt;%\n  add_trace(y = ~Company_A, name = \"Company A\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_B, name = \"Company B\", type = \"scatter\", mode = \"lines\") %&gt;%\n  add_trace(y = ~Company_C, name = \"Company C\", type = \"scatter\", mode = \"lines\") %&gt;%\n  layout(\n    title = \"Monthly Stock Prices\",\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Price\"),\n    showlegend = TRUE\n  )\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\nInteractive data visualizations are a powerful tool for EDA and data presentation. In the next section, we’ll explore another advanced visualization technique: time series decomposition.\n\n\n\nThe interactive time series plot created with Plotly offers the following interaction features:\n\nSelection: Users can click on the legend to select/deselect specific companies for visualization.\nZoom: Users can click and drag to zoom in on a specific time period.\nHover Information: Hovering the mouse pointer over data points displays detailed information about the selected data point.\n\n\n\n\nInteractive visualizations with Plotly are valuable for:\n\nExploration: Users can interactively explore complex datasets and focus on specific aspects of the data.\nData Communication: Presenting data in an interactive format enhances communication and engagement.\nDecision Support: Interactive plots can be used in decision-making processes where users need to explore data dynamics.\n\n[Wickham (2016)](Schloerke et al. 2021)"
  },
  {
    "objectID": "posts/021_confronto_xls_database/index.html",
    "href": "posts/021_confronto_xls_database/index.html",
    "title": "Confronto tra Excel e Database nella Gestione Clinica dei Dati: Errori E Sicurezza",
    "section": "",
    "text": "Confronto tra Excel e Database nella Gestione Clinica dei Dati: Errori e Sicurezza\n\nCaso Excel: Errore nel Valore di una Glicemia\nUn errore comune in Excel è l’inserimento di valori non corretti a causa di formati non standardizzati. Supponiamo di avere una tabella in cui si registrano i valori di glicemia dei pazienti. Il formato corretto richiede l’uso del punto come separatore decimale (ad esempio, 5.8 mmol/L), ma un utente inserisce erroneamente un valore con la virgola (5,8).\nEsempio di tabella:\n\n\n\nPaziente\nValore di glicemia (mmol/L)\n\n\n\n\nPaziente A\n5.6\n\n\nPaziente B\n6.2\n\n\nPaziente C\n5,8\n\n\nPaziente D\n7.1\n\n\n\nIn questo caso, Excel potrebbe trattare 5,8 come testo, non come numero, perché la virgola è interpretata come un separatore non numerico. Di conseguenza, il valore 5,8 verrebbe ignorato durante i calcoli. Per esempio, se si tenta di calcolare la media dei valori con la formula =MEDIA(B2:B5), Excel considererà solo i valori numerici (5.6, 6.2 e 7.1), restituendo una media non corretta.\n\nMedia calcolata da Excel (ignorando 5,8):\n(5.6+6.2+7.1)/3=6.3(5.6 + 6.2 + 7.1) / 3 = 6.3(5.6+6.2+7.1)/3=6.3\nMedia corretta (includendo 5.8):\n(5.6+6.2+5.8+7.1)/4=6.175(5.6 + 6.2 + 5.8 + 7.1) / 4 = 6.175(5.6+6.2+5.8+7.1)/4=6.175\n\nQuesto errore può facilmente passare inosservato, portando a conclusioni cliniche sbagliate.\n\n\nCaso Database: Prevenzione dell’Errore\nIn un database, l’inserimento di dati con formato errato viene automaticamente bloccato. Supponiamo che la colonna dei valori glicemici sia definita come DECIMAL o FLOAT, dove è richiesto l’uso del punto decimale. Se si tenta di inserire un valore con la virgola, il database restituirà un errore, impedendo l’inserimento errato.\nCREATE TABLE glicemia (\n      paziente VARCHAR(50), \n      valore_glicemia DECIMAL(4,1) );\n\nINSERT INTO glicemia (paziente, valore_glicemia) VALUES ('Paziente C', '5,8');\nQuesto comando genererebbe un errore simile a:\nERROR: invalid input syntax for type numeric: \"5,8\"\nIl database obbliga l’utente a correggere l’errore prima di continuare, prevenendo errori nei calcoli successivi.\n\n\nCaso Excel: Mancanza di Controllo sugli Accessi\nExcel non offre un sistema avanzato di controllo sugli accessi e tracciamento delle modifiche. Supponiamo che un file Excel venga utilizzato per gestire i dati clinici di pazienti, con il rischio che qualsiasi utente con accesso possa modificare i dati (intenzionalmente o accidentalmente) senza lasciare traccia.\nEsempio:\n\n\n\n\n\n\n\n\n\n\nID Paziente\nNome\nDiagnosi\nTerapia\nMedico\n\n\n\n\n001\nMario Rossi\nIpertensione\nACE Inibitori\nDott. Bianchi\n\n\n002\nAnna Verdi\nDiabete Tipo 2\nInsulina\nDott. Neri\n\n\n003\nLuca Blu\nInsufficienza cardiaca\nBetabloccanti\nDott. Verdi\n\n\n\nUn operatore sanitario potrebbe per errore modificare la terapia di un paziente, sostituendo “Insulina” con “Metformina” per Anna Verdi. In questo scenario, non ci sarebbe alcun modo di risalire a chi ha apportato la modifica, poiché Excel non tiene traccia delle modifiche, esponendo i dati clinici a gravi rischi.\n\n\nCaso Database: Robustezza nella Gestione della Sicurezza\nUn database offre robusti meccanismi di controllo degli accessi, con permessi specifici per ogni ruolo. Ad esempio, solo determinati utenti possono modificare i dati sensibili, mentre altri hanno accesso in sola lettura. Inoltre, ogni modifica viene tracciata automaticamente, registrando chi ha fatto la modifica e quando.\nEsempio di gestione degli accessi:\nGRANT SELECT ON cartella_clinica TO infermiera;\nGRANT UPDATE ON terapia TO medico;\nOgni modifica viene registrata, e se qualcuno cambia la terapia, è possibile risalire all’operatore e correggere l’errore.\nEsempio di log delle modifiche:\n\n\n\n\n\n\n\n\n\n\n\nID Paziente\nOperatore\nCampo Modificato\nValore Precedente\nValore Nuovo\nData Modifica\n\n\n\n\n002\ninfermiera1\nTerapia\nInsulina\nMetformina\n2024-09-24 10:30\n\n\n\n\n\nCaso Excel: Duplicazione dei Dati\nUn altro rischio comune in Excel è la duplicazione accidentale dei dati. Supponiamo che durante l’inserimento di nuovi pazienti, un utente copi e incolli una riga esistente invece di crearne una nuova, modificando solo alcune informazioni e lasciando altri campi inalterati.\nEsempio di tabella:\n\n\n\n\n\n\n\n\n\n\nID Paziente\nNome\nDiagnosi\nTerapia\nMedico\n\n\n\n\n001\nMario Rossi\nIpertensione\nACE Inibitori\nDott. Bianchi\n\n\n002\nAnna Verdi\nDiabete Tipo 2\nInsulina\nDott. Neri\n\n\n003\nMario Rossi\nInsufficienza Cardiaca\nBetabloccanti\nDott. Bianchi\n\n\n\nIn questo caso, abbiamo due pazienti con lo stesso nome, ma terapie e diagnosi diverse. Questa duplicazione potrebbe non essere immediatamente evidente e potrebbe causare confusione clinica o errori nella gestione del paziente. Excel non dispone di controlli per rilevare automaticamente questi errori.\n\n\nCaso Database: Eliminazione delle Duplicazioni\nUn database può evitare questo tipo di errore utilizzando vincoli di unicità. Se la colonna “ID Paziente” è definita come chiave primaria, nessun duplicato sarà consentito.\nSebbene Excel sia uno strumento potente e molto utilizzato per la gestione dei dati, può presentare alcune limitazioni, specialmente quando si tratta di gestire dati sensibili come quelli clinici. Un database strutturato offre un livello superiore di controllo e sicurezza, prevenendo errori comuni come quelli dovuti a formati non standardizzati o duplicazioni. La scelta dello strumento giusto dipende dalle esigenze specifiche e dalla complessità dei dati da gestire, con l’obiettivo di garantire sempre accuratezza e sicurezza.\nEsempio:\nCREATE TABLE cartella_clinica (\n    id_paziente INT PRIMARY KEY,\n    nome VARCHAR(100),\n    diagnosi VARCHAR(100),\n    terapia VARCHAR(100),\n    medico VARCHAR(100)\n);\nSe si tenta di inserire un nuovo paziente con un ID già esistente:\nINSERT INTO cartella_clinica (id_paziente, nome, diagnosi, terapia, medico)\nVALUES (003, 'Mario Rossi', 'Ipertensione', 'ACE Inibitori', 'Dott. Bianchi');\nIl database restituirebbe un errore simile a:\nERROR: duplicate key value violates unique constraint \"cartella_clinica_pkey\"\nQuesto impedisce l’inserimento accidentale di dati duplicati, preservando l’integrità dei dati clinici.\n\nL’utilizzo di Excel per la gestione di dati clinici comporta rischi significativi di errori dovuti a formati incoerenti, duplicazioni e mancanza di tracciabilità. Al contrario, un database offre strumenti avanzati per prevenire questi problemi, garantendo una maggiore sicurezza, precisione e affidabilità nella gestione dei dati clinici."
  },
  {
    "objectID": "posts/022_pitfall_curves/index.html",
    "href": "posts/022_pitfall_curves/index.html",
    "title": "The Pitfall of Arbitrary Curve Deconvolution: A Cautionary Tale for Scientists and Students",
    "section": "",
    "text": "As scientists and data analysts, we often encounter complex curves in our work. These could be spectroscopic data, chromatograms, or any other type of signal that appears as a single peak but might be composed of multiple underlying components. The process of breaking down such a curve into its constituent parts is known as curve deconvolution or peak fitting.\nIt’s a common scenario: a researcher or student successfully decomposes a curve into two or three components, and they’re thrilled with the result. It seems to fit well, and they’re ready to draw conclusions based on these components. But there’s a crucial point that’s often overlooked: the number of components used in the deconvolution is often arbitrary.\nIn this post, we’ll explore why this is problematic and demonstrate how we can fit a curve with a varying number of components using R. This exercise will highlight why we should be cautious about interpreting the results of curve deconvolution without careful consideration.\nLet’s start with some R code that generates a simple curve and then fits it with a varying number of Gaussian components:\n\n# Load necessary libraries\nlibrary(minpack.lm)\n\nWarning: il pacchetto 'minpack.lm' è stato creato con R versione 4.3.3\n\nlibrary(ggplot2)\n\nWarning: il pacchetto 'ggplot2' è stato creato con R versione 4.3.3\n\n# Function to generate a Gaussian curve\ngaussian &lt;- function(x, amp, cen, wid) {\n  amp * exp(-((x - cen)^2) / (2 * wid^2))\n}\n\n# Generate sample data (a curve with a single peak)\nset.seed(123)\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- gaussian(x, 1, 5, 1) + rnorm(100, sd = 0.05)\ndata &lt;- data.frame(x = x, y = y)\n\n# Function to create a model with n Gaussians\ncreate_n_gaussian_model &lt;- function(n) {\n  components &lt;- paste0(\"gaussian(x, amp\", 1:n, \", cen\", 1:n, \", wid\", 1:n, \")\", collapse = \" + \")\n  as.formula(paste(\"y ~\", components))\n}\n\n# Function to fit n Gaussians\nfit_n_gaussian &lt;- function(data, n) {\n  model &lt;- create_n_gaussian_model(n)\n  start_list &lt;- as.list(rep(c(0.5, 5, 1), n))\n  names(start_list) &lt;- c(rbind(paste0(\"amp\", 1:n), paste0(\"cen\", 1:n), paste0(\"wid\", 1:n)))\n  \n  fit &lt;- nlsLM(model, data = data, start = start_list)\n  return(fit)\n}\n\n# Fit with varying number of Gaussians\nfits &lt;- lapply(1:5, function(n) fit_n_gaussian(data, n))\n\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n\n\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n\nWarning in nls.lm(par = start, fn = FCT, jac = jac, control = control, lower = lower, : lmdif: info = -1. Number of iterations has reached `maxiter' == 50.\n\n# Calculate AIC for each fit\naics &lt;- sapply(fits, AIC)\n\n# Create predictions and components for each fit\nfor (i in 1:length(fits)) {\n  data[paste0(\"fit\", i)] &lt;- predict(fits[[i]], newdata = data)\n  params &lt;- coef(fits[[i]])\n  for (j in 1:i) {\n    data[paste0(\"comp\", j, \"_\", i)] &lt;- gaussian(data$x, params[paste0(\"amp\", j)], \n                                                params[paste0(\"cen\", j)], \n                                                params[paste0(\"wid\", j)])\n  }\n}\n\n# Plot the results\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit1, color = \"1 Gaussian\")) +\n  geom_line(aes(y = fit2, color = \"2 Gaussians\")) +\n  geom_line(aes(y = fit3, color = \"3 Gaussians\")) +\n  geom_line(aes(y = fit4, color = \"4 Gaussians\")) +\n  geom_line(aes(y = fit5, color = \"5 Gaussians\")) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\", \"orange\")) +\n  labs(title = \"Curve Deconvolution with Varying Number of Gaussians\",\n       subtitle = paste(\"AIC values:\", paste(round(aics, 2), collapse = \", \")),\n       x = \"X\", y = \"Y\", color = \"Fit\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot individual components for the 3-Gaussian fit\nggplot(data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = fit3, color = \"Total Fit\")) +\n  geom_line(aes(y = comp1_3, color = \"Component 1\")) +\n  geom_line(aes(y = comp2_3, color = \"Component 2\")) +\n  geom_line(aes(y = comp3_3, color = \"Component 3\")) +\n  scale_color_manual(values = c(\"black\", \"red\", \"blue\", \"green\")) +\n  labs(title = \"Deconvolution into 3 Gaussians\",\n       x = \"X\", y = \"Y\", color = \"Component\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis code generates a single peak and then fits it with 1 to 5 Gaussian components. It then plots the results and calculates the Akaike Information Criterion (AIC) for each fit.\nWhat we see from this exercise is striking:\n\nAll fits, from 1 to 5 components, appear to fit the data reasonably well visually.\nThe AIC values generally decrease as we add more components, suggesting that more complex models fit the data better.\n\nHowever, here’s the crucial point: our original data was generated from a single Gaussian curve with added noise. Despite this, we can “successfully” decompose it into 2, 3, 4, or even 5 components, each of which might seem meaningful if we didn’t know the true origin of the data.\nThis demonstrates a fundamental issue in curve deconvolution: without additional information or constraints, we can often fit a curve with an arbitrary number of components, and purely statistical measures might even suggest that more components provide a better fit.\nSo, what should we do? Here are some guidelines:\n\nAlways consider the physical or chemical meaning behind the components. Does it make sense in your specific context to have 2, 3, or more components?\nUse multiple criteria for model selection, not just visual fit or a single statistical measure. AIC, BIC, cross-validation, and other techniques can provide different perspectives.\nBe cautious about over-interpretation. Just because you can fit a curve with multiple components doesn’t necessarily mean those components represent real, distinct physical or chemical entities.\nWhen possible, use additional experimental techniques to validate your deconvolution. For example, in spectroscopy, you might use different types of spectroscopy or chemical separation techniques to confirm the presence of multiple components.\nAlways report the uncertainty in your fits and be transparent about the assumptions made in your analysis.\n\nIn conclusion, while curve deconvolution can be a powerful tool, it’s crucial to approach it with caution and skepticism. The ability to fit a curve with multiple components doesn’t always reflect the underlying reality of the system you’re studying. As scientists and data analysts, it’s our responsibility to critically evaluate our methods and results, always keeping in mind the limitations and potential pitfalls of our analytical techniques."
  }
]